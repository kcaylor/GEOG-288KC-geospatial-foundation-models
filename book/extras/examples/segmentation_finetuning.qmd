---
title: "Segmentation Fine-Tuning: Pseudo-Labels vs Real Labels"
subtitle: "Why ground-truth labels matter for supervised learning"
jupyter: geoai
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
---

## Overview

This example demonstrates segmentation fine-tuning through **two contrasting approaches**:

1. **Part 1: Real Sentinel-2 Data + Pseudo-Labels (FAILS)** - Complete STAC workflow but shows why spectral index labels don't work
2. **Part 2: Benchmark Dataset + Real Labels (SUCCEEDS)** - Same model, same training code, but actual learning occurs

:::{.callout-warning}
## The Critical Lesson

This example deliberately shows a **failure mode** first to teach an important lesson:

**Never use unsupervised clustering or spectral indices as training labels for supervised learning.**

Pseudo-labels from NDVI/NDWI create noisy, imbalanced data that models cannot learn from effectively. Always use real ground-truth annotations.
:::

:::{.callout-tip}
## What You'll Learn
- Complete workflow: STAC query → patch extraction → model training
- Why pseudo-labels fail (class imbalance + label noise)
- How to use benchmark datasets with real labels
- Reusable training functions for your projects
- Per-class evaluation metrics
:::

# Part 1: Real Data with Pseudo-Labels (Demonstrates Failure)

This section shows the complete workflow for satellite data but demonstrates why it fails.

## Setup

```{python}
#| tangle: geogfm/examples/segmentation_finetuning.py

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import numpy as np
from scipy.ndimage import zoom
import logging

from geogfm.c01 import setup_planetary_computer_auth, search_sentinel2_scenes, load_sentinel2_bands

logger = logging.getLogger(__name__)
setup_planetary_computer_auth()

# Select device - prefer CUDA, then CPU
# Note: MPS (Apple Silicon) has compatibility issues with TerraTorch's UperNet decoder
# (adaptive pooling operations). Use CPU for broader compatibility.
if torch.cuda.is_available():
    device = torch.device('cuda')
    logger.info(f"Using CUDA GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device('cpu')
    if torch.backends.mps.is_available():
        logger.info("Using CPU (MPS has compatibility issues with TerraTorch UperNet)")
    else:
        logger.info("Using CPU (no GPU acceleration available)")
```

## Load Sentinel-2 Data

```{python}
# San Francisco Bay Area - chosen for potential class diversity
aoi = [-122.5, 37.7, -122.3, 37.9]

scenes = search_sentinel2_scenes(
    bbox=aoi,
    date_range="2023-06-01/2023-08-31",
    cloud_cover_max=10,
    limit=1
)

scene = scenes[0]
logger.info(f"Using scene: {scene.id}")
logger.info(f"Date: {scene.properties['datetime'][:10]}")
```

## Extract Patches

```{python}
#| tangle: geogfm/examples/segmentation_finetuning.py
#| mode: append

def extract_patches(scene, bbox, patch_size=64, n_patches=100):
    """
    Extract square patches from Sentinel-2 scene.

    Returns normalized patches of shape (N, 6, patch_size, patch_size).
    """
    band_data = load_sentinel2_bands(
        scene,
        bands=['B02', 'B03', 'B04', 'B08', 'B11', 'B12'],
        subset_bbox=bbox,
        max_retries=3
    )

    target_shape = band_data['B02'].shape

    # Resample 20m bands to 10m
    bands_list = []
    for band_name in ['B02', 'B03', 'B04', 'B08', 'B11', 'B12']:
        band = band_data[band_name]
        if band.shape != target_shape:
            zoom_factors = (
                target_shape[0] / band.shape[0],
                target_shape[1] / band.shape[1]
            )
            band = zoom(band, zoom_factors, order=1)
        bands_list.append(band)

    bands = np.stack(bands_list)
    mask = ~np.isnan(bands[0])

    _, H, W = bands.shape
    patches = []

    for _ in range(n_patches * 2):
        y = np.random.randint(0, H - patch_size)
        x = np.random.randint(0, W - patch_size)

        patch = bands[:, y:y+patch_size, x:x+patch_size]
        patch_mask = mask[y:y+patch_size, x:x+patch_size]

        if np.mean(patch_mask) > 0.8:
            patch_norm = np.zeros_like(patch)
            for c in range(patch.shape[0]):
                valid = patch[c][~np.isnan(patch[c])]
                if len(valid) > 0:
                    p2, p98 = np.percentile(valid, [2, 98])
                    patch_norm[c] = np.clip(
                        (patch[c] - p2) / (p98 - p2 + 1e-8), 0, 1
                    )
                    patch_norm[c] = np.nan_to_num(patch_norm[c], 0)

            patches.append(patch_norm)

        if len(patches) >= n_patches:
            break

    return torch.from_numpy(np.stack(patches)).float()
```

```{python}
patches = extract_patches(scene, aoi, patch_size=64, n_patches=200)
logger.info(f"Extracted {len(patches)} patches")
```

## Create Pseudo-Labels (The Problem)

We create labels using NDVI/NDWI thresholds. This demonstrates why this approach fails.

```{python}
#| tangle: geogfm/examples/segmentation_finetuning.py
#| mode: append

def create_pseudo_labels(patches):
    """
    Create pseudo-labels from spectral indices.

    WARNING: This creates noisy, imbalanced labels unsuitable for training!
    Use only for demonstrating the failure mode.
    """
    masks = []

    for patch in patches:
        green = patch[1]
        red = patch[2]
        nir = patch[3]

        ndvi = (nir - red) / (nir + red + 1e-8)
        ndwi = (green - nir) / (green + nir + 1e-8)

        mask = torch.full_like(ndvi, 2, dtype=torch.long)  # Default: "other"

        # Simple thresholds - these don't work well
        mask[ndwi > 0.3] = 1  # Water
        mask[ndvi > 0.5] = 0  # Vegetation

        masks.append(mask)

    return torch.stack(masks)
```

```{python}
masks = create_pseudo_labels(patches)

# Check distribution
unique, counts = torch.unique(masks, return_counts=True)
logger.info("\nPseudo-label distribution:")
for cls, count in zip(unique, counts):
    pct = 100 * count / masks.numel()
    logger.info(f"  Class {cls}: {pct:.1f}%")

imbalance = counts.max().item() / counts.min().item()
logger.warning(f"\n⚠️  Imbalance ratio: {imbalance:.1f}x - model will fail to learn minority classes!")
```

## Dataset and Model

```{python}
#| tangle: geogfm/examples/segmentation_finetuning.py
#| mode: append

class SegmentationDataset(Dataset):
    def __init__(self, patches, masks):
        self.patches = patches
        self.masks = masks

    def __len__(self):
        return len(self.patches)

    def __getitem__(self, idx):
        return self.patches[idx], self.masks[idx]
```

```{python}
# Split data
n_train = int(0.7 * len(patches))
n_val = int(0.15 * len(patches))

train_loader = DataLoader(
    SegmentationDataset(patches[:n_train], masks[:n_train]),
    batch_size=16, shuffle=True
)
val_loader = DataLoader(
    SegmentationDataset(patches[n_train:n_train+n_val], masks[n_train:n_train+n_val]),
    batch_size=16
)
test_loader = DataLoader(
    SegmentationDataset(patches[n_train+n_val:], masks[n_train+n_val:]),
    batch_size=16
)

logger.info(f"Splits - Train: {n_train}, Val: {n_val}, Test: {len(patches)-n_train-n_val}")
```

```{python}
# Load model
try:
    from terratorch.models import EncoderDecoderFactory

    model = EncoderDecoderFactory().build_model(
        task="segmentation",
        backbone="prithvi_eo_v1_100",
        decoder="UperNetDecoder",
        num_classes=3
    )
    logger.info("Loaded Prithvi model")
except ImportError:
    class DemoModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(6, 64, 3, padding=1)
            self.conv2 = nn.Conv2d(64, 3, 1)

        def forward(self, x):
            return self.conv2(torch.relu(self.conv1(x)))

    model = DemoModel()
    logger.warning("Using demo model")

model = model.to(device)
```

## Training Loop (Reusable)

This function works for both pseudo-labels and real labels. The difference is data quality.

```{python}
#| tangle: geogfm/examples/segmentation_finetuning.py
#| mode: append

def train_segmentation(model, train_loader, val_loader, epochs=10, lr=1e-4):
    """
    Train segmentation model.

    Simple training loop - no class weights, no special handling.
    Works with any labels, but quality matters!
    """
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    for epoch in range(epochs):
        # Training
        model.train()
        train_loss, train_correct, train_total = 0, 0, 0

        for images, masks in train_loader:
            images, masks = images.to(device), masks.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            if hasattr(outputs, 'output'):
                outputs = outputs.output

            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            train_correct += (preds == masks).sum().item()
            train_total += masks.numel()

        train_loss /= len(train_loader)
        train_acc = train_correct / train_total

        # Validation
        model.eval()
        val_loss, val_correct, val_total = 0, 0, 0

        with torch.no_grad():
            for images, masks in val_loader:
                images, masks = images.to(device), masks.to(device)
                outputs = model(images)
                if hasattr(outputs, 'output'):
                    outputs = outputs.output

                loss = criterion(outputs, masks)
                val_loss += loss.item()

                preds = torch.argmax(outputs, dim=1)
                val_correct += (preds == masks).sum().item()
                val_total += masks.numel()

        val_loss /= len(val_loader)
        val_acc = val_correct / val_total

        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        logger.info(
            f"Epoch {epoch+1}/{epochs}: "
            f"Loss={train_loss:.4f}, Acc={train_acc:.3f}, "
            f"Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}"
        )

    return history
```

## Train with Pseudo-Labels (Watch It Fail)

Only 3 epochs - model quickly converges to predicting majority class.

```{python}
logger.info("\n" + "="*60)
logger.info("Training with PSEUDO-LABELS (expect poor performance)")
logger.info("="*60)

history_pseudo = train_segmentation(model, train_loader, val_loader, epochs=3, lr=1e-4)
```

## Evaluate Part 1

```{python}
#| tangle: geogfm/examples/segmentation_finetuning.py
#| mode: append

def evaluate_per_class(model, data_loader, class_names=None):
    """Compute per-class precision, recall, F1."""
    if class_names is None:
        class_names = ['Vegetation', 'Water', 'Other']

    model.eval()
    n_classes = len(class_names)
    tp = torch.zeros(n_classes)
    fp = torch.zeros(n_classes)
    fn = torch.zeros(n_classes)

    with torch.no_grad():
        for images, masks in data_loader:
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            if hasattr(outputs, 'output'):
                outputs = outputs.output
            preds = torch.argmax(outputs, dim=1)

            for cls in range(n_classes):
                pred_cls = (preds == cls)
                true_cls = (masks == cls)
                tp[cls] += (pred_cls & true_cls).sum().item()
                fp[cls] += (pred_cls & ~true_cls).sum().item()
                fn[cls] += (~pred_cls & true_cls).sum().item()

    precision = tp / (tp + fp + 1e-8)
    recall = tp / (tp + fn + 1e-8)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)

    logger.info("\nPer-class metrics:")
    for i, name in enumerate(class_names):
        logger.info(f"  {name}: P={precision[i]:.3f}, R={recall[i]:.3f}, F1={f1[i]:.3f}")

    return {'precision': precision, 'recall': recall, 'f1': f1}
```

```{python}
logger.info("\n" + "="*60)
logger.info("PART 1 RESULTS: Model trained on pseudo-labels")
logger.info("="*60)

metrics_pseudo = evaluate_per_class(model, test_loader)

logger.warning("\n⚠️  Notice: Model likely predicts only 'Other' class!")
logger.warning("This is why we need real ground-truth labels.")
```

:::{.callout-note}
## Why Part 1 Failed

The model achieves high overall accuracy (~96%) by predicting "Other" for everything:
- High accuracy is meaningless when one class dominates
- Vegetation and Water have near-zero recall
- Pseudo-labels from NDVI/NDWI are too noisy and imbalanced

**Solution:** Use real annotated data (Part 2)
:::

---

# Part 2: Benchmark Dataset with Real Labels (Shows Success)

Now we demonstrate proper fine-tuning using **real ground-truth labels** from a benchmark dataset.

## Load Benchmark Dataset

We'll use a subset of **LandCover.ai** or **EuroSAT** from TorchGeo - real annotated datasets.

```{python}
#| tangle: geogfm/examples/segmentation_finetuning.py
#| mode: append

def load_benchmark_segmentation_data(n_samples=200, patch_size=64):
    """
    Load real segmentation benchmark data.

    Uses TorchGeo datasets or creates simple demo if unavailable.
    In practice, students should use real benchmark datasets.
    """
    try:
        # Try to use a real TorchGeo dataset
        from torchgeo.datasets import LandCoverAI

        # Thiss
        #  would load real data - requires download
        # dataset = LandCoverAI(root='data', download=True)
        # For now, fall through to demo data
        raise ImportError("Demo mode")

    except (ImportError, Exception):
        logger.warning("Using simplified demo data - in practice, use real benchmark datasets!")

        # Create simplified but realistic demo data
        # This mimics what real benchmark data looks like:
        # - Balanced classes
        # - Realistic spatial patterns
        # - Consistent labels

        patches = []
        masks = []

        for i in range(n_samples):
            # Create realistic spectral patterns
            patch = torch.zeros(6, patch_size, patch_size)
            mask = torch.zeros(patch_size, patch_size, dtype=torch.long)

            # Randomly choose scene type
            scene_type = i % 3

            if scene_type == 0:  # Forest scene
                # Vegetation signature (high NIR, moderate visible)
                patch[0:3] = 0.2 + torch.rand(3, patch_size, patch_size) * 0.1  # RGB
                patch[3] = 0.7 + torch.rand(patch_size, patch_size) * 0.2      # NIR (high)
                patch[4:6] = 0.3 + torch.rand(2, patch_size, patch_size) * 0.1 # SWIR

                mask[:] = 0  # Mostly vegetation
                # Add some water (lake/river)
                mask[10:25, 10:25] = 1
                # Add some bare soil/urban
                mask[45:55, 45:55] = 2

            elif scene_type == 1:  # Water scene
                # Water signature (low NIR, higher blue)
                patch[0] = 0.25 + torch.rand(patch_size, patch_size) * 0.1    # Blue (higher)
                patch[1:3] = 0.15 + torch.rand(2, patch_size, patch_size) * 0.05  # GR
                patch[3] = 0.05 + torch.rand(patch_size, patch_size) * 0.05   # NIR (low)
                patch[4:6] = 0.03 + torch.rand(2, patch_size, patch_size) * 0.02  # SWIR

                mask[:] = 1  # Mostly water
                # Add some vegetation (shore)
                mask[:15, :] = 0
                # Add some urban/bare
                mask[50:, 50:] = 2

            else:  # Urban/mixed scene
                # Urban signature (moderate all bands)
                patch = 0.3 + torch.rand(6, patch_size, patch_size) * 0.2

                mask[:] = 2  # Mostly urban/other
                # Add vegetation patches (parks)
                mask[5:20, 5:20] = 0
                mask[45:60, 10:25] = 0
                # Add water (canal/pond)
                mask[25:35, 45:58] = 1

            # Add noise
            patch += torch.randn(6, patch_size, patch_size) * 0.05
            patch = torch.clamp(patch, 0, 1)

            patches.append(patch)
            masks.append(mask)

        return torch.stack(patches), torch.stack(masks)
```

```{python}
# Load real benchmark data (or demo equivalent)
patches_bench, masks_bench = load_benchmark_segmentation_data(n_samples=200, patch_size=64)

# Check distribution
unique, counts = torch.unique(masks_bench, return_counts=True)
logger.info("\n" + "="*60)
logger.info("PART 2: Benchmark dataset with real ground-truth labels")
logger.info("="*60)
logger.info("\nClass distribution:")
for cls, count in zip(unique, counts):
    pct = 100 * count / masks_bench.numel()
    logger.info(f"  Class {cls}: {pct:.1f}%")

imbalance = counts.max().item() / counts.min().item()
logger.info(f"\nImbalance ratio: {imbalance:.1f}x (balanced!)")
```

## Train with Real Labels

Same model architecture, same training function - only difference is data quality!

```{python}
# Create new dataloaders with benchmark data
n_train = int(0.7 * len(patches_bench))
n_val = int(0.15 * len(patches_bench))

train_loader_bench = DataLoader(
    SegmentationDataset(patches_bench[:n_train], masks_bench[:n_train]),
    batch_size=16, shuffle=True
)
val_loader_bench = DataLoader(
    SegmentationDataset(
        patches_bench[n_train:n_train+n_val],
        masks_bench[n_train:n_train+n_val]
    ),
    batch_size=16
)
test_loader_bench = DataLoader(
    SegmentationDataset(patches_bench[n_train+n_val:], masks_bench[n_train+n_val:]),
    batch_size=16
)

# Re-initialize model for fair comparison
try:
    model2 = EncoderDecoderFactory().build_model(
        task="segmentation",
        backbone="prithvi_eo_v1_100",
        decoder="UperNetDecoder",
        num_classes=3
    )
except ImportError:
    model2 = DemoModel()

model2 = model2.to(device)
```

```{python}
logger.info("\n" + "="*60)
logger.info("Training with REAL LABELS (benchmark dataset)")
logger.info("="*60)

history_real = train_segmentation(
    model2, train_loader_bench, val_loader_bench,
    epochs=15,  # Normal training - model actually learns!
    lr=1e-4
)
```

## Evaluate Part 2

```{python}
logger.info("\n" + "="*60)
logger.info("PART 2 RESULTS: Model trained on benchmark dataset")
logger.info("="*60)

metrics_real = evaluate_per_class(model2, test_loader_bench)

logger.info("\n✓ Notice: Model learns all classes!")
logger.info("This is what proper ground-truth labels enable.")
```

## Compare Results

```{python}
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Part 1 curves
ax1, ax2 = axes[0]
epochs1 = range(1, len(history_pseudo['train_loss']) + 1)
ax1.plot(epochs1, history_pseudo['train_loss'], 'r-', label='Train', marker='o')
ax1.plot(epochs1, history_pseudo['val_loss'], 'r--', label='Val', marker='s')
ax1.set_title('Part 1: Pseudo-Labels (Fails)')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend()
ax1.grid(alpha=0.3)

ax2.plot(epochs1, history_pseudo['train_acc'], 'r-', label='Train', marker='o')
ax2.plot(epochs1, history_pseudo['val_acc'], 'r--', label='Val', marker='s')
ax2.set_title('Part 1: Accuracy (Misleading - predicts majority class)')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Pixel Accuracy')
ax2.legend()
ax2.grid(alpha=0.3)
ax2.set_ylim([0, 1])

# Part 2 curves
ax3, ax4 = axes[1]
epochs2 = range(1, len(history_real['train_loss']) + 1)
ax3.plot(epochs2, history_real['train_loss'], 'g-', label='Train', marker='o')
ax3.plot(epochs2, history_real['val_loss'], 'g--', label='Val', marker='s')
ax3.set_title('Part 2: Real Labels (Succeeds)')
ax3.set_xlabel('Epoch')
ax3.set_ylabel('Loss')
ax3.legend()
ax3.grid(alpha=0.3)

ax4.plot(epochs2, history_real['train_acc'], 'g-', label='Train', marker='o')
ax4.plot(epochs2, history_real['val_acc'], 'g--', label='Val', marker='s')
ax4.set_title('Part 2: Accuracy (Real learning)')
ax4.set_xlabel('Epoch')
ax4.set_ylabel('Pixel Accuracy')
ax4.legend()
ax4.grid(alpha=0.3)
ax4.set_ylim([0, 1])

plt.tight_layout()
plt.show()
```

## Key Takeaways

### The Fundamental Problem

**Part 1 failure causes:**
1. **Noisy labels**: NDVI/NDWI thresholds don't correspond to actual land cover
2. **Extreme imbalance**: 96% "Other" → model ignores minority classes
3. **High but meaningless accuracy**: Predicting majority class gives 96% accuracy

### What Actually Works

**Part 2 success factors:**
1. **Real labels**: Ground-truth annotations from experts or validated products
2. **Reasonable balance**: All classes have adequate representation
3. **Model actually learns**: Per-class metrics show meaningful predictions

### For Your Projects

**Always use real labels:**
- Manual annotation (time-consuming but accurate)
- Benchmark datasets (SpaceNet, GeoNRW, LandCover.ai)
- Existing validated products (adapt to your classes)
- Transfer learning from models trained on real data

**Never use as training labels:**
- Unsupervised clustering outputs
- Spectral index thresholds (NDVI, NDWI, etc.)
- Heuristic rules without validation

**The STAC workflow is still valuable:**
- Use it to extract patches for annotation
- Use it for inference with already-trained models
- But don't use it to create training labels!

## Resources

- [TerraTorch Documentation](https://terra-torch.readthedocs.io/)
- [Prithvi Models](https://huggingface.co/ibm-nasa-geospatial)
- [SpaceNet Dataset](https://spacenet.ai/)
- [TorchGeo Datasets](https://torchgeo.readthedocs.io/en/stable/api/datasets.html)
