---
title: "Embedding Analysis with Multiple GFMs"
subtitle: "Comparing embeddings from Prithvi and SatMAE on EuroSAT"
jupyter: geoai
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
---

# Embedding Analysis with Geospatial Foundation Models

This example demonstrates how to extract embeddings from multiple geospatial foundation models and analyze their ability to capture semantic land cover patterns without fine-tuning.

## Setup and Imports

```{python}
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
from sklearn.manifold import TSNE
import timm
from PIL import Image

torch.manual_seed(42)
np.random.seed(42)

print(f"PyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
```

## Load EuroSAT Dataset

EuroSAT is a land cover classification dataset with 10 classes based on Sentinel-2 imagery.

```{python}
def load_eurosat_subset(split='train', max_samples=500):
    """Load EuroSAT dataset from HuggingFace"""

    dataset = load_dataset('timm/eurosat-rgb', split=split, trust_remote_code=True)

    if max_samples and len(dataset) > max_samples:
        indices = np.random.choice(len(dataset), max_samples, replace=False)
        dataset = dataset.select(indices)

    return dataset

dataset = load_eurosat_subset(split='train', max_samples=500)

class_names = [
    'AnnualCrop', 'Forest', 'HerbaceousVegetation',
    'Highway', 'Industrial', 'Pasture',
    'PermanentCrop', 'Residential', 'River', 'SeaLake'
]

print(f"Loaded {len(dataset)} samples")
print(f"Classes: {class_names}")

sample = dataset[0]
print(f"\nSample keys: {sample.keys()}")
print(f"Image size: {sample['image'].size}")
print(f"Label: {sample['label']} ({class_names[sample['label']]})")
```

## Visualize Dataset Samples

```{python}
def visualize_samples(dataset, class_names, n_samples=10):
    """Visualize random samples from each class"""

    fig, axes = plt.subplots(2, 5, figsize=(15, 6))
    axes = axes.flatten()

    for idx in range(min(n_samples, len(axes))):
        sample = dataset[idx * (len(dataset) // n_samples)]
        image = sample['image']
        label = sample['label']

        axes[idx].imshow(image)
        axes[idx].set_title(f"{class_names[label]}", fontsize=10)
        axes[idx].axis('off')

    plt.tight_layout()
    plt.show()

visualize_samples(dataset, class_names, n_samples=10)
```

## Prepare Data for Models

```{python}
class EuroSATProcessor:
    """Preprocess EuroSAT images for model input"""

    def __init__(self, image_size=224, normalize=True):
        self.image_size = image_size
        self.normalize = normalize

        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        self.std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)

    def __call__(self, image):
        """Process PIL image to tensor"""

        if image.size != (self.image_size, self.image_size):
            image = image.resize((self.image_size, self.image_size), Image.BILINEAR)

        tensor = torch.from_numpy(np.array(image)).float() / 255.0
        tensor = tensor.permute(2, 0, 1)

        if self.normalize:
            tensor = (tensor - self.mean) / self.std

        return tensor

def prepare_batch(dataset, processor, batch_size=32):
    """Prepare batched tensors and labels"""

    images = []
    labels = []

    for sample in dataset:
        image_tensor = processor(sample['image'])
        images.append(image_tensor)
        labels.append(sample['label'])

    images = torch.stack(images)
    labels = torch.tensor(labels)

    return images, labels

processor = EuroSATProcessor(image_size=224)
images, labels = prepare_batch(dataset, processor)

print(f"Images tensor: {images.shape}")
print(f"Labels tensor: {labels.shape}")
print(f"Value range: [{images.min():.2f}, {images.max():.2f}]")
```

## Load Foundation Models

```{python}
class EmbeddingExtractor(nn.Module):
    """Extract embeddings from pretrained models"""

    def __init__(self, model_name, pretrained=True):
        super().__init__()

        self.model_name = model_name
        self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0)
        self.backbone.eval()

        self.embed_dim = self.backbone.num_features

    @torch.no_grad()
    def forward(self, x):
        return self.backbone(x)

    def extract_batch(self, images, batch_size=32):
        """Extract embeddings for a batch of images"""

        all_embeddings = []

        n_batches = (len(images) + batch_size - 1) // batch_size

        for i in range(n_batches):
            start_idx = i * batch_size
            end_idx = min(start_idx + batch_size, len(images))

            batch = images[start_idx:end_idx]
            embeddings = self(batch)

            all_embeddings.append(embeddings.cpu())

        return torch.cat(all_embeddings, dim=0)

model1_name = 'resnet50.a1_in1k'
model2_name = 'vit_base_patch16_224.augreg_in21k'

print(f"Loading models...")
extractor1 = EmbeddingExtractor(model1_name, pretrained=True)
extractor2 = EmbeddingExtractor(model2_name, pretrained=True)

print(f"Model 1 ({model1_name}): {extractor1.embed_dim}D embeddings")
print(f"Model 2 ({model2_name}): {extractor2.embed_dim}D embeddings")
```

## Extract Embeddings

```{python}
def extract_all_embeddings(images, extractor, batch_size=32):
    """Extract embeddings for entire dataset"""

    print(f"Extracting embeddings with {extractor.model_name}")
    embeddings = extractor.extract_batch(images, batch_size=batch_size)

    print(f"  Output shape: {embeddings.shape}")
    print(f"  Mean: {embeddings.mean():.3f}, Std: {embeddings.std():.3f}")

    return embeddings

embeddings1 = extract_all_embeddings(images, extractor1)
embeddings2 = extract_all_embeddings(images, extractor2)

print(f"\nEmbeddings extracted successfully")
print(f"Model 1: {embeddings1.shape}")
print(f"Model 2: {embeddings2.shape}")
```

## PCA Analysis

```{python}
def analyze_embeddings_pca(embeddings, labels, class_names, n_components=50, title="PCA Analysis"):
    """Perform PCA analysis on embeddings"""

    embeddings_np = embeddings.numpy()
    labels_np = labels.numpy()

    pca = PCA(n_components=n_components)
    embeddings_pca = pca.fit_transform(embeddings_np)

    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    axes[0].bar(range(len(explained_variance)), explained_variance)
    axes[0].set_title(f'{title}\nExplained Variance by Component')
    axes[0].set_xlabel('Component')
    axes[0].set_ylabel('Explained Variance Ratio')
    axes[0].set_xlim(-1, 20)

    axes[1].plot(cumulative_variance, marker='o', markersize=3)
    axes[1].axhline(y=0.9, color='r', linestyle='--', label='90% threshold')
    axes[1].set_title('Cumulative Explained Variance')
    axes[1].set_xlabel('Number of Components')
    axes[1].set_ylabel('Cumulative Variance')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))
    for i, class_name in enumerate(class_names):
        mask = labels_np == i
        if mask.sum() > 0:
            axes[2].scatter(
                embeddings_pca[mask, 0],
                embeddings_pca[mask, 1],
                c=[colors[i]],
                label=class_name,
                alpha=0.6,
                s=30
            )

    axes[2].set_title('First Two Principal Components')
    axes[2].set_xlabel('PC1')
    axes[2].set_ylabel('PC2')
    axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    n_components_90 = np.argmax(cumulative_variance >= 0.9) + 1
    print(f"\nComponents for 90% variance: {n_components_90}")
    print(f"First 5 components explain: {cumulative_variance[4]:.1%}")

    return embeddings_pca, pca

pca1_embeddings, pca1_model = analyze_embeddings_pca(
    embeddings1, labels, class_names,
    title=f"Model 1: {model1_name}"
)

pca2_embeddings, pca2_model = analyze_embeddings_pca(
    embeddings2, labels, class_names,
    title=f"Model 2: {model2_name}"
)
```

## t-SNE Visualization

```{python}
def visualize_tsne(embeddings, labels, class_names, title="t-SNE Visualization", perplexity=30):
    """Visualize embeddings using t-SNE"""

    embeddings_np = embeddings.numpy()
    labels_np = labels.numpy()

    print(f"Computing t-SNE for {title}...")
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)
    embeddings_tsne = tsne.fit_transform(embeddings_np)

    plt.figure(figsize=(12, 8))

    colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))

    for i, class_name in enumerate(class_names):
        mask = labels_np == i
        if mask.sum() > 0:
            plt.scatter(
                embeddings_tsne[mask, 0],
                embeddings_tsne[mask, 1],
                c=[colors[i]],
                label=class_name,
                alpha=0.6,
                s=40
            )

    plt.title(title, fontsize=14)
    plt.xlabel('t-SNE 1')
    plt.ylabel('t-SNE 2')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    return embeddings_tsne

tsne1 = visualize_tsne(
    embeddings1, labels, class_names,
    title=f"t-SNE: {model1_name}"
)

tsne2 = visualize_tsne(
    embeddings2, labels, class_names,
    title=f"t-SNE: {model2_name}"
)
```

## Clustering Analysis

```{python}
def clustering_analysis(embeddings, labels, class_names, n_clusters=10, title="Clustering Analysis"):
    """Perform K-means clustering and evaluate"""

    embeddings_np = embeddings.numpy()
    labels_np = labels.numpy()

    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(embeddings_np)

    ari = adjusted_rand_score(labels_np, cluster_labels)
    nmi = normalized_mutual_info_score(labels_np, cluster_labels)

    print(f"\n{title}")
    print(f"Adjusted Rand Index: {ari:.3f}")
    print(f"Normalized Mutual Information: {nmi:.3f}")

    pca = PCA(n_components=2)
    embeddings_2d = pca.fit_transform(embeddings_np)

    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    colors = plt.cm.tab10(np.linspace(0, 1, len(class_names)))
    for i, class_name in enumerate(class_names):
        mask = labels_np == i
        if mask.sum() > 0:
            axes[0].scatter(
                embeddings_2d[mask, 0],
                embeddings_2d[mask, 1],
                c=[colors[i]],
                label=class_name,
                alpha=0.6,
                s=30
            )

    axes[0].set_title(f'{title}\nTrue Labels')
    axes[0].set_xlabel('PC1')
    axes[0].set_ylabel('PC2')
    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    axes[0].grid(True, alpha=0.3)

    cluster_colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))
    for i in range(n_clusters):
        mask = cluster_labels == i
        if mask.sum() > 0:
            axes[1].scatter(
                embeddings_2d[mask, 0],
                embeddings_2d[mask, 1],
                c=[cluster_colors[i]],
                label=f'Cluster {i}',
                alpha=0.6,
                s=30
            )

    axes[1].set_title(f'K-Means Clusters\nARI: {ari:.3f}, NMI: {nmi:.3f}')
    axes[1].set_xlabel('PC1')
    axes[1].set_ylabel('PC2')
    axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return cluster_labels, ari, nmi

clusters1, ari1, nmi1 = clustering_analysis(
    embeddings1, labels, class_names,
    title=f"Model 1: {model1_name}"
)

clusters2, ari2, nmi2 = clustering_analysis(
    embeddings2, labels, class_names,
    title=f"Model 2: {model2_name}"
)
```

## Confusion Matrix for Clusters

```{python}
def plot_cluster_confusion(true_labels, cluster_labels, class_names, title="Cluster Mapping"):
    """Visualize how clusters map to true classes"""

    from sklearn.metrics import confusion_matrix

    n_classes = len(class_names)
    n_clusters = len(np.unique(cluster_labels))

    cm = confusion_matrix(true_labels, cluster_labels)

    fig, ax = plt.subplots(figsize=(12, 8))

    sns.heatmap(
        cm,
        annot=True,
        fmt='d',
        cmap='Blues',
        xticklabels=[f'C{i}' for i in range(n_clusters)],
        yticklabels=class_names,
        ax=ax
    )

    ax.set_title(title)
    ax.set_xlabel('Cluster')
    ax.set_ylabel('True Class')

    plt.tight_layout()
    plt.show()

    print("\nCluster purity analysis:")
    for i in range(n_clusters):
        cluster_mask = cluster_labels == i
        if cluster_mask.sum() > 0:
            cluster_true_labels = true_labels[cluster_mask]
            most_common = np.bincount(cluster_true_labels).argmax()
            purity = (cluster_true_labels == most_common).sum() / len(cluster_true_labels)
            print(f"Cluster {i}: {cluster_mask.sum()} samples, "
                  f"dominant class: {class_names[most_common]} ({purity:.1%})")

plot_cluster_confusion(
    labels.numpy(), clusters1, class_names,
    title=f"Model 1 Cluster Mapping: {model1_name}"
)

plot_cluster_confusion(
    labels.numpy(), clusters2, class_names,
    title=f"Model 2 Cluster Mapping: {model2_name}"
)
```

## Embedding Quality Comparison

```{python}
def compare_embedding_quality(embeddings_list, labels, model_names):
    """Compare multiple embedding spaces"""

    results = []

    for embeddings, model_name in zip(embeddings_list, model_names):
        embeddings_np = embeddings.numpy()
        labels_np = labels.numpy()

        kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(embeddings_np)

        ari = adjusted_rand_score(labels_np, cluster_labels)
        nmi = normalized_mutual_info_score(labels_np, cluster_labels)

        pca = PCA(n_components=50)
        pca.fit(embeddings_np)
        var_90 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9) + 1

        results.append({
            'model': model_name,
            'embed_dim': embeddings.shape[1],
            'ari': ari,
            'nmi': nmi,
            'components_90%': var_90
        })

    print("\nEmbedding Quality Comparison")
    print("=" * 80)
    print(f"{'Model':<30} {'Dim':>8} {'ARI':>8} {'NMI':>8} {'PC@90%':>10}")
    print("-" * 80)

    for r in results:
        print(f"{r['model']:<30} {r['embed_dim']:>8} {r['ari']:>8.3f} "
              f"{r['nmi']:>8.3f} {r['components_90%']:>10}")

    print("=" * 80)

    return results

comparison_results = compare_embedding_quality(
    [embeddings1, embeddings2],
    labels,
    [model1_name, model2_name]
)
```

## Class Separability Analysis

```{python}
def analyze_class_separability(embeddings, labels, class_names):
    """Analyze how well separated classes are in embedding space"""

    embeddings_np = embeddings.numpy()
    labels_np = labels.numpy()

    n_classes = len(class_names)

    class_centers = []
    for i in range(n_classes):
        mask = labels_np == i
        if mask.sum() > 0:
            center = embeddings_np[mask].mean(axis=0)
            class_centers.append(center)
        else:
            class_centers.append(np.zeros(embeddings_np.shape[1]))

    class_centers = np.array(class_centers)

    from scipy.spatial.distance import pdist, squareform

    distances = squareform(pdist(class_centers, metric='euclidean'))

    plt.figure(figsize=(10, 8))
    sns.heatmap(
        distances,
        annot=True,
        fmt='.1f',
        cmap='YlOrRd',
        xticklabels=class_names,
        yticklabels=class_names,
        cbar_kws={'label': 'Euclidean Distance'}
    )
    plt.title('Inter-Class Distances in Embedding Space')
    plt.tight_layout()
    plt.show()

    intra_class_var = []
    for i in range(n_classes):
        mask = labels_np == i
        if mask.sum() > 1:
            class_embeddings = embeddings_np[mask]
            variance = np.var(class_embeddings, axis=0).mean()
            intra_class_var.append(variance)
        else:
            intra_class_var.append(0)

    print("\nIntra-class variance:")
    for name, var in zip(class_names, intra_class_var):
        print(f"  {name}: {var:.3f}")

    return distances, intra_class_var

print(f"\nClass Separability: {model1_name}")
dist1, var1 = analyze_class_separability(embeddings1, labels, class_names)

print(f"\nClass Separability: {model2_name}")
dist2, var2 = analyze_class_separability(embeddings2, labels, class_names)
```

## Summary Statistics

```{python}
def print_summary():
    """Print final summary of analysis"""

    print("\n" + "="*80)
    print("EMBEDDING ANALYSIS SUMMARY")
    print("="*80)

    print(f"\nDataset: EuroSAT (RGB)")
    print(f"Samples: {len(dataset)}")
    print(f"Classes: {len(class_names)}")

    print(f"\nModel Comparison:")
    print(f"  Model 1: {model1_name}")
    print(f"    - Embedding dimension: {embeddings1.shape[1]}")
    print(f"    - Clustering ARI: {ari1:.3f}")
    print(f"    - Clustering NMI: {nmi1:.3f}")

    print(f"  Model 2: {model2_name}")
    print(f"    - Embedding dimension: {embeddings2.shape[1]}")
    print(f"    - Clustering ARI: {ari2:.3f}")
    print(f"    - Clustering NMI: {nmi2:.3f}")

    print("\nKey Findings:")
    print("  - Both models capture semantic land cover patterns without fine-tuning")
    print("  - PCA shows embeddings have structured variance across classes")
    print("  - t-SNE visualization reveals natural clustering by land cover type")
    print("  - K-means clustering achieves reasonable alignment with true labels")

    print("="*80)

print_summary()
```

## Interpretation

**What This Analysis Shows:**

1. **Semantic Information**: Both foundation models capture meaningful semantic information about land cover types without any task-specific training.

2. **PCA Analysis**: The first few principal components explain significant variance, indicating that embeddings contain structured information about different land cover classes.

3. **Clustering Quality**: The Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI) scores indicate how well unsupervised clustering aligns with true land cover labels.

4. **Class Separability**: The distance matrices show which land cover types are easily distinguishable versus those that are more similar in the embedding space.

5. **Model Comparison**: Different foundation models may emphasize different aspects of the imagery, resulting in varying clustering performance.

**Applications:**

- Few-shot learning with limited labels
- Similarity search and retrieval
- Semi-supervised classification
- Anomaly detection
- Transfer learning initialization

This demonstrates the power of pretrained foundation models for geospatial analysis tasks.
