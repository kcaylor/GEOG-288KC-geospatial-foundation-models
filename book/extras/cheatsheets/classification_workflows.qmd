---
title: "Classification Workflows"
subtitle: "Binary and multi-class classification for geospatial data"
jupyter: geoai
format:
  html:
    code-fold: false
---

# Classification Workflows

This cheatsheet demonstrates practical classification workflows for geospatial data, covering binary and multi-class scenarios with comprehensive evaluation strategies.

## Setup and Dependencies

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    precision_recall_fscore_support
)
import seaborn as sns

torch.manual_seed(42)
np.random.seed(42)

print(f"PyTorch version: {torch.__version__}")
```

## Geospatial Classification Dataset

```{python}
class GeospatialClassificationDataset(Dataset):
    """Dataset for land cover classification"""

    def __init__(self, num_samples=200, size=32, num_bands=6, num_classes=5):
        self.num_samples = num_samples
        self.size = size
        self.num_bands = num_bands
        self.num_classes = num_classes

        self.images = []
        self.labels = []

        rng = np.random.RandomState(42)

        for idx in range(num_samples):
            class_label = idx % num_classes

            if class_label == 0:  # Water
                base = rng.normal(0.15, 0.05, (num_bands, size, size))
                base[0] *= 1.5  # High blue
            elif class_label == 1:  # Forest
                base = rng.normal(0.35, 0.08, (num_bands, size, size))
                base[1] *= 1.8  # High green
                base[3] *= 1.5  # High NIR
            elif class_label == 2:  # Urban
                base = rng.normal(0.55, 0.12, (num_bands, size, size))
            elif class_label == 3:  # Agriculture
                base = rng.normal(0.45, 0.09, (num_bands, size, size))
                base[1] *= 1.3
                base[3] *= 1.2
            else:  # Bare soil
                base = rng.normal(0.65, 0.10, (num_bands, size, size))
                base[2] *= 1.2  # High red

            image = np.clip(base, 0, 1).astype(np.float32)

            self.images.append(image)
            self.labels.append(class_label)

        self.images = np.array(self.images)
        self.labels = np.array(self.labels)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return torch.FloatTensor(self.images[idx]), self.labels[idx]

train_dataset = GeospatialClassificationDataset(num_samples=200, size=32, num_classes=5)
val_dataset = GeospatialClassificationDataset(num_samples=50, size=32, num_classes=5)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
```

## Classification Model Architecture

```{python}
class LandCoverClassifier(nn.Module):
    """CNN classifier for land cover types"""

    def __init__(self, num_bands=6, num_classes=5):
        super().__init__()

        self.features = nn.Sequential(
            nn.Conv2d(num_bands, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(4)
        )

        self.classifier = nn.Sequential(
            nn.Linear(128 * 4 * 4, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        features = self.features(x)
        features = features.view(features.size(0), -1)
        return self.classifier(features)

model = LandCoverClassifier(num_bands=6, num_classes=5)
print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
```

## Training Loop with Metrics

```{python}
def train_epoch(model, dataloader, optimizer, criterion):
    """Train one epoch and return metrics"""
    model.train()

    total_loss = 0
    all_preds = []
    all_targets = []

    for images, labels in dataloader:
        optimizer.zero_grad()

        outputs = model(images)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        _, predicted = outputs.max(1)
        all_preds.extend(predicted.cpu().numpy())
        all_targets.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_targets))

    return avg_loss, accuracy, all_preds, all_targets

def validate(model, dataloader, criterion):
    """Validate model and return detailed metrics"""
    model.eval()

    total_loss = 0
    all_preds = []
    all_targets = []
    all_probs = []

    with torch.no_grad():
        for images, labels in dataloader:
            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item()

            probs = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)

            all_probs.extend(probs.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = 100 * np.mean(np.array(all_preds) == np.array(all_targets))

    return avg_loss, accuracy, all_preds, all_targets, all_probs

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

print("Training multi-class classifier")
for epoch in range(5):
    train_loss, train_acc, _, _ = train_epoch(model, train_loader, optimizer, criterion)
    val_loss, val_acc, val_preds, val_targets, val_probs = validate(model, val_loader, criterion)

    print(f"Epoch {epoch+1}: Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.1f}%, "
          f"Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.1f}%")
```

## Confusion Matrix Analysis

```{python}
def plot_confusion_matrix(y_true, y_pred, class_names=None):
    """Plot confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)

    if class_names is None:
        class_names = [f"Class {i}" for i in range(len(cm))]

    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names, ax=ax)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('True')
    ax.set_title('Confusion Matrix')
    plt.tight_layout()
    plt.show()

    return cm

class_names = ['Water', 'Forest', 'Urban', 'Agriculture', 'Bare Soil']
cm = plot_confusion_matrix(val_targets, val_preds, class_names)
print("\nConfusion Matrix:")
print(cm)
```

## Per-Class Metrics

```{python}
def compute_per_class_metrics(y_true, y_pred, class_names=None):
    """Compute precision, recall, F1 for each class"""
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, average=None
    )

    if class_names is None:
        class_names = [f"Class {i}" for i in range(len(precision))]

    print("\nPer-Class Metrics:")
    print(f"{'Class':<15} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Support':>10}")
    print("-" * 60)

    for i, name in enumerate(class_names):
        print(f"{name:<15} {precision[i]:>10.3f} {recall[i]:>10.3f} "
              f"{f1[i]:>10.3f} {support[i]:>10}")

    macro_p = np.mean(precision)
    macro_r = np.mean(recall)
    macro_f1 = np.mean(f1)

    print("-" * 60)
    print(f"{'Macro Avg':<15} {macro_p:>10.3f} {macro_r:>10.3f} {macro_f1:>10.3f}")

    return precision, recall, f1, support

precision, recall, f1, support = compute_per_class_metrics(
    val_targets, val_preds, class_names
)
```

## Binary Classification Example

```{python}
class BinaryClassificationDataset(Dataset):
    """Binary classification dataset"""

    def __init__(self, num_samples=200, size=32, num_bands=6):
        self.num_samples = num_samples
        self.size = size
        self.num_bands = num_bands

        self.images = []
        self.labels = []

        rng = np.random.RandomState(42)

        for idx in range(num_samples):
            label = idx % 2

            if label == 0:  # Negative class
                base = rng.normal(0.3, 0.1, (num_bands, size, size))
            else:  # Positive class
                base = rng.normal(0.6, 0.12, (num_bands, size, size))

            image = np.clip(base, 0, 1).astype(np.float32)

            self.images.append(image)
            self.labels.append(label)

        self.images = np.array(self.images)
        self.labels = np.array(self.labels)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return torch.FloatTensor(self.images[idx]), self.labels[idx]

class BinaryClassifier(nn.Module):
    """Binary classifier with sigmoid output"""

    def __init__(self, num_bands=6):
        super().__init__()

        self.features = nn.Sequential(
            nn.Conv2d(num_bands, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(4)
        )

        self.classifier = nn.Sequential(
            nn.Linear(64 * 4 * 4, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        features = self.features(x)
        features = features.view(features.size(0), -1)
        return self.classifier(features)

binary_train = BinaryClassificationDataset(num_samples=200)
binary_val = BinaryClassificationDataset(num_samples=50)

binary_train_loader = DataLoader(binary_train, batch_size=32, shuffle=True)
binary_val_loader = DataLoader(binary_val, batch_size=32, shuffle=False)

binary_model = BinaryClassifier(num_bands=6)
binary_criterion = nn.BCEWithLogitsLoss()
binary_optimizer = optim.Adam(binary_model.parameters(), lr=0.001)

print("Training binary classifier")
for epoch in range(5):
    binary_model.train()
    total_loss = 0

    for images, labels in binary_train_loader:
        binary_optimizer.zero_grad()

        outputs = binary_model(images).squeeze()
        loss = binary_criterion(outputs, labels.float())

        loss.backward()
        binary_optimizer.step()

        total_loss += loss.item()

    binary_model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in binary_val_loader:
            outputs = binary_model(images).squeeze()
            predicted = (torch.sigmoid(outputs) > 0.5).long()

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    avg_loss = total_loss / len(binary_train_loader)

    print(f"Epoch {epoch+1}: Loss: {avg_loss:.3f}, Val Acc: {accuracy:.1f}%")
```

## Handling Class Imbalance

```{python}
class ImbalancedDataset(Dataset):
    """Dataset with class imbalance"""

    def __init__(self, num_samples=200, size=32, num_bands=6, num_classes=3):
        self.num_samples = num_samples
        self.size = size
        self.num_bands = num_bands
        self.num_classes = num_classes

        self.images = []
        self.labels = []

        rng = np.random.RandomState(42)

        class_distribution = [0.7, 0.2, 0.1]  # Imbalanced

        for idx in range(num_samples):
            rand_val = rng.random()
            if rand_val < class_distribution[0]:
                class_label = 0
            elif rand_val < class_distribution[0] + class_distribution[1]:
                class_label = 1
            else:
                class_label = 2

            base = rng.normal(0.3 + class_label * 0.2, 0.1, (num_bands, size, size))
            image = np.clip(base, 0, 1).astype(np.float32)

            self.images.append(image)
            self.labels.append(class_label)

        self.images = np.array(self.images)
        self.labels = np.array(self.labels)

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        return torch.FloatTensor(self.images[idx]), self.labels[idx]

def compute_class_weights(labels):
    """Compute class weights for imbalanced data"""
    unique, counts = np.unique(labels, return_counts=True)
    total = len(labels)

    weights = total / (len(unique) * counts)

    print("Class distribution and weights:")
    for cls, count, weight in zip(unique, counts, weights):
        print(f"Class {cls}: {count} samples ({100*count/total:.1f}%), weight: {weight:.3f}")

    return torch.FloatTensor(weights)

imbalanced_dataset = ImbalancedDataset(num_samples=200, num_classes=3)
class_weights = compute_class_weights(imbalanced_dataset.labels)

weighted_criterion = nn.CrossEntropyLoss(weight=class_weights)

imbalanced_loader = DataLoader(imbalanced_dataset, batch_size=32, shuffle=True)

imbalanced_model = LandCoverClassifier(num_bands=6, num_classes=3)
weighted_optimizer = optim.Adam(imbalanced_model.parameters(), lr=0.001)

print("\nTraining with class weights")
for epoch in range(3):
    imbalanced_model.train()
    total_loss = 0

    for images, labels in imbalanced_loader:
        weighted_optimizer.zero_grad()

        outputs = imbalanced_model(images)
        loss = weighted_criterion(outputs, labels)

        loss.backward()
        weighted_optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(imbalanced_loader)
    print(f"Epoch {epoch+1}: Loss: {avg_loss:.3f}")
```

## Top-K Accuracy

```{python}
def compute_topk_accuracy(outputs, targets, k=3):
    """Compute top-k accuracy"""
    batch_size = targets.size(0)

    _, pred = outputs.topk(k, dim=1, largest=True, sorted=True)
    pred = pred.t()
    correct = pred.eq(targets.view(1, -1).expand_as(pred))

    topk_correct = correct[:k].reshape(-1).float().sum(0)
    topk_accuracy = topk_correct / batch_size

    return topk_accuracy.item()

model.eval()
top1_correct = 0
top3_correct = 0
total = 0

with torch.no_grad():
    for images, labels in val_loader:
        outputs = model(images)

        top1_correct += compute_topk_accuracy(outputs, labels, k=1) * labels.size(0)
        top3_correct += compute_topk_accuracy(outputs, labels, k=3) * labels.size(0)
        total += labels.size(0)

print(f"\nTop-1 Accuracy: {100 * top1_correct / total:.2f}%")
print(f"Top-3 Accuracy: {100 * top3_correct / total:.2f}%")
```

## Probability Calibration Check

```{python}
def analyze_prediction_confidence(probs, targets):
    """Analyze prediction confidence distribution"""
    probs_array = np.array(probs)
    targets_array = np.array(targets)

    max_probs = probs_array.max(axis=1)
    predicted_classes = probs_array.argmax(axis=1)

    correct_mask = (predicted_classes == targets_array)

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

    ax1.hist(max_probs[correct_mask], bins=20, alpha=0.7, label='Correct', color='green')
    ax1.hist(max_probs[~correct_mask], bins=20, alpha=0.7, label='Incorrect', color='red')
    ax1.set_xlabel('Confidence')
    ax1.set_ylabel('Count')
    ax1.set_title('Prediction Confidence Distribution')
    ax1.legend()

    confidence_bins = np.linspace(0, 1, 11)
    bin_indices = np.digitize(max_probs, confidence_bins) - 1

    bin_accuracy = []
    bin_confidence = []

    for i in range(len(confidence_bins) - 1):
        mask = bin_indices == i
        if mask.sum() > 0:
            bin_accuracy.append(correct_mask[mask].mean())
            bin_confidence.append(max_probs[mask].mean())

    ax2.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')
    ax2.plot(bin_confidence, bin_accuracy, 'o-', label='Model')
    ax2.set_xlabel('Confidence')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Calibration Plot')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    print(f"\nAverage confidence (correct): {max_probs[correct_mask].mean():.3f}")
    print(f"Average confidence (incorrect): {max_probs[~correct_mask].mean():.3f}")

analyze_prediction_confidence(val_probs, val_targets)
```

## One-vs-Rest Classification

```{python}
def train_one_vs_rest(num_classes, target_class):
    """Train one-vs-rest binary classifier for a specific class"""

    def convert_to_binary(labels, target):
        return (labels == target).long()

    binary_model = BinaryClassifier(num_bands=6)
    binary_criterion = nn.BCEWithLogitsLoss()
    binary_optimizer = optim.Adam(binary_model.parameters(), lr=0.001)

    print(f"\nTraining One-vs-Rest for class {target_class}")

    for epoch in range(3):
        binary_model.train()
        total_loss = 0

        for images, labels in train_loader:
            binary_labels = convert_to_binary(labels, target_class)

            binary_optimizer.zero_grad()
            outputs = binary_model(images).squeeze()
            loss = binary_criterion(outputs, binary_labels.float())

            loss.backward()
            binary_optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}: Loss: {avg_loss:.3f}")

    return binary_model

ovr_model_0 = train_one_vs_rest(num_classes=5, target_class=0)
```

## Model Predictions Analysis

```{python}
def analyze_misclassifications(model, dataloader, class_names):
    """Analyze common misclassification patterns"""
    model.eval()

    all_preds = []
    all_targets = []

    with torch.no_grad():
        for images, labels in dataloader:
            outputs = model(images)
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())

    all_preds = np.array(all_preds)
    all_targets = np.array(all_targets)

    cm = confusion_matrix(all_targets, all_preds)

    print("\nMost Common Misclassifications:")
    misclassifications = []

    for i in range(len(cm)):
        for j in range(len(cm)):
            if i != j and cm[i, j] > 0:
                misclassifications.append((cm[i, j], i, j))

    misclassifications.sort(reverse=True)

    for count, true_class, pred_class in misclassifications[:5]:
        true_name = class_names[true_class] if class_names else f"Class {true_class}"
        pred_name = class_names[pred_class] if class_names else f"Class {pred_class}"
        print(f"{true_name} misclassified as {pred_name}: {count} times")

analyze_misclassifications(model, val_loader, class_names)
```

## Classification Report

```{python}
def print_classification_summary(y_true, y_pred, class_names=None):
    """Print comprehensive classification report"""

    report = classification_report(
        y_true, y_pred,
        target_names=class_names,
        digits=3
    )

    print("\n" + "="*60)
    print("Classification Report")
    print("="*60)
    print(report)

    accuracy = np.mean(np.array(y_true) == np.array(y_pred))
    print(f"\nOverall Accuracy: {100 * accuracy:.2f}%")

print_classification_summary(val_targets, val_preds, class_names)
```

## Best Practices Summary

```{python}
def print_best_practices():
    """Print classification best practices"""

    practices = {
        "Multi-class Strategy": [
            "Use CrossEntropyLoss for multi-class problems",
            "Apply softmax for probability outputs",
            "Monitor per-class metrics, not just overall accuracy"
        ],
        "Binary Classification": [
            "Use BCEWithLogitsLoss for numerical stability",
            "Threshold at 0.5 may not be optimal for imbalanced data",
            "Consider ROC-AUC for threshold selection"
        ],
        "Class Imbalance": [
            "Use class weights in loss function",
            "Consider oversampling minority classes",
            "Evaluate with precision, recall, F1 not just accuracy"
        ],
        "Evaluation": [
            "Always use confusion matrix for error analysis",
            "Report per-class metrics separately",
            "Check prediction confidence calibration",
            "Analyze misclassification patterns"
        ],
        "Model Training": [
            "Use batch normalization for stable training",
            "Apply dropout to prevent overfitting",
            "Monitor validation metrics during training",
            "Save best model based on validation performance"
        ]
    }

    print("\n" + "="*60)
    print("Classification Best Practices")
    print("="*60)

    for category, tips in practices.items():
        print(f"\n{category}:")
        for tip in tips:
            print(f"  - {tip}")

print_best_practices()
```

## Summary

**Key Takeaways:**

- **Multi-class classification** uses softmax activation and CrossEntropyLoss
- **Binary classification** uses sigmoid activation and BCEWithLogitsLoss
- **Class imbalance** requires weighted losses or sampling strategies
- **Evaluation** should include confusion matrix, per-class metrics, and confidence analysis
- **Top-k accuracy** is useful for multi-class problems with many classes
- **One-vs-rest** can provide interpretable binary classifiers per class
- **Calibration analysis** helps understand prediction confidence reliability

These workflows provide production-ready components for geospatial classification tasks that can be directly integrated into your projects.
