<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Understanding Neural Networks: From Neurons to Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">GEOG 288KC</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">🏠 home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../Syllabus.html"> 
<span class="menu-text">📋 syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-weekly-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">💻 weekly sessions</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-weekly-sessions">    
        <li>
    <a class="dropdown-item" href="../chapters/c01-geospatial-data-foundations.html">
 <span class="dropdown-text">Week 1 - 🚀 Core Tools and Data Access</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c02-spatial-temporal-attention-mechanisms.html">
 <span class="dropdown-text">Week 2 - ⚡ Rapid Remote Sensing Preprocessing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c03a-terratorch-foundations.html">
 <span class="dropdown-text">Week 3a - 🌍 TerraTorch Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c03-complete-gfm-architecture.html">
 <span class="dropdown-text">Week 3b - 🤖 Machine Learning on Remote Sensing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c04-pretraining-implementation.html">
 <span class="dropdown-text">Week 4 - 🏗️ Foundation Models in Practice</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c05-training-loop-optimization.html">
 <span class="dropdown-text">Week 5 - 🔧 Fine-Tuning &amp; Transfer Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c06-model-evaluation-analysis.html">
 <span class="dropdown-text">Week 6 - ⏰ Spatiotemporal Modeling &amp; Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-cheatsheets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">👀 cheatsheets</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-cheatsheets">    
        <li>
    <a class="dropdown-item" href="../cheatsheets.html">
 <span class="dropdown-text">📋 All Cheatsheets</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">⚡ Quick Starts</li>
        <li>
    <a class="dropdown-item" href="../extras/cheatsheets/week01_imports.html">
 <span class="dropdown-text">Week 01: Import Guide</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-explainers" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">🧩 explainers</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-explainers">    
        <li class="dropdown-header">1️⃣ Week 1</li>
        <li>
    <a class="dropdown-item" href="../extras/ai-ml-dl-fm-hierarchy.html">
 <span class="dropdown-text">🤖 AI/ML/DL/FM Hierarchy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/geospatial-foundation-model-predictions-standalone.html">
 <span class="dropdown-text">🎯 GFM Predictions (Standalone)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/geospatial-prediction-hierarchy.html">
 <span class="dropdown-text">✅ Geospatial Task/Prediction Types</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/neural_networks_explainer.html">
 <span class="dropdown-text">🧠 Neural Networks: Neurons to Transformers</span></a>
  </li>  
        <li class="dropdown-header">2️⃣ Week 2</li>
        <li>
    <a class="dropdown-item" href="../chapters/c00a-foundation_model_architectures.html">
 <span class="dropdown-text">🏗️ Foundation Model Architectures</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../chapters/c00b-introduction-to-deeplearning-architecture.html">
 <span class="dropdown-text">🎓 Introduction to Deep Learning Architecture</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-extras" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">📖 extras</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-extras">    
        <li class="dropdown-header">🎯 Practical Examples</li>
        <li>
    <a class="dropdown-item" href="../extras/examples/normalization_comparison.html">
 <span class="dropdown-text">Normalization Comparison</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/resnet.html">
 <span class="dropdown-text">ResNet Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/segmentation_finetuning.html">
 <span class="dropdown-text">Segmentation Fine-Tuning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/text_encoder.html">
 <span class="dropdown-text">Text Encoder</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/tiling-and-patches.html">
 <span class="dropdown-text">Tiling and Patches</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/examples/terratorch_workflows.html">
 <span class="dropdown-text">TerraTorch Workflows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/resources/course_resources.html">
 <span class="dropdown-text">📚 Reference Materials</span></a>
  </li>  
        <li class="dropdown-header">📁 Project Templates</li>
        <li>
    <a class="dropdown-item" href="../extras/projects/project-proposal-template.html">
 <span class="dropdown-text">Project Proposal Template</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../extras/projects/mvp-template.html">
 <span class="dropdown-text">Project Results Template</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gfms-from-scratch/gfms-from-scratch.github.io" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Understanding Neural Networks: From Neurons to Transformers</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">A Progressive Guide to Deep Learning Architecture</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#part-1-the-individual-neuron" id="toc-part-1-the-individual-neuron" class="nav-link" data-scroll-target="#part-1-the-individual-neuron">Part 1: The Individual Neuron</a>
  <ul class="collapse">
  <li><a href="#what-does-a-neuron-do" id="toc-what-does-a-neuron-do" class="nav-link" data-scroll-target="#what-does-a-neuron-do">What Does a Neuron Do?</a></li>
  <li><a href="#activation-functions-adding-non-linearity" id="toc-activation-functions-adding-non-linearity" class="nav-link" data-scroll-target="#activation-functions-adding-non-linearity">Activation Functions: Adding Non-Linearity</a></li>
  <li><a href="#seeing-the-effect-of-activation-functions" id="toc-seeing-the-effect-of-activation-functions" class="nav-link" data-scroll-target="#seeing-the-effect-of-activation-functions">Seeing the Effect of Activation Functions</a></li>
  </ul></li>
  <li><a href="#part-2-from-single-neurons-to-layers" id="toc-part-2-from-single-neurons-to-layers" class="nav-link" data-scroll-target="#part-2-from-single-neurons-to-layers">Part 2: From Single Neurons to Layers</a>
  <ul class="collapse">
  <li><a href="#the-power-of-vectorization" id="toc-the-power-of-vectorization" class="nav-link" data-scroll-target="#the-power-of-vectorization">The Power of Vectorization</a></li>
  <li><a href="#building-a-multi-layer-network" id="toc-building-a-multi-layer-network" class="nav-link" data-scroll-target="#building-a-multi-layer-network">Building a Multi-Layer Network</a></li>
  <li><a href="#visualizing-the-transformation" id="toc-visualizing-the-transformation" class="nav-link" data-scroll-target="#visualizing-the-transformation">Visualizing the Transformation</a></li>
  </ul></li>
  <li><a href="#part-3-attention-mechanisms-and-transformers" id="toc-part-3-attention-mechanisms-and-transformers" class="nav-link" data-scroll-target="#part-3-attention-mechanisms-and-transformers">Part 3: Attention Mechanisms and Transformers</a>
  <ul class="collapse">
  <li><a href="#the-limitation-of-basic-neural-networks" id="toc-the-limitation-of-basic-neural-networks" class="nav-link" data-scroll-target="#the-limitation-of-basic-neural-networks">The Limitation of Basic Neural Networks</a></li>
  <li><a href="#what-is-attention" id="toc-what-is-attention" class="nav-link" data-scroll-target="#what-is-attention">What is Attention?</a></li>
  <li><a href="#visualizing-attention-patterns" id="toc-visualizing-attention-patterns" class="nav-link" data-scroll-target="#visualizing-attention-patterns">Visualizing Attention Patterns</a></li>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention">Multi-Head Attention</a></li>
  <li><a href="#how-attention-changes-encodings" id="toc-how-attention-changes-encodings" class="nav-link" data-scroll-target="#how-attention-changes-encodings">How Attention Changes Encodings</a></li>
  </ul></li>
  <li><a href="#part-4-putting-it-all-together---the-transformer-block" id="toc-part-4-putting-it-all-together---the-transformer-block" class="nav-link" data-scroll-target="#part-4-putting-it-all-together---the-transformer-block">Part 4: Putting It All Together - The Transformer Block</a></li>
  <li><a href="#summary-the-neural-network-hierarchy" id="toc-summary-the-neural-network-hierarchy" class="nav-link" data-scroll-target="#summary-the-neural-network-hierarchy">Summary: The Neural Network Hierarchy</a>
  <ul class="collapse">
  <li><a href="#level-1-single-neuron" id="toc-level-1-single-neuron" class="nav-link" data-scroll-target="#level-1-single-neuron">Level 1: Single Neuron</a></li>
  <li><a href="#level-2-layer-of-neurons" id="toc-level-2-layer-of-neurons" class="nav-link" data-scroll-target="#level-2-layer-of-neurons">Level 2: Layer of Neurons</a></li>
  <li><a href="#level-3-multi-layer-network" id="toc-level-3-multi-layer-network" class="nav-link" data-scroll-target="#level-3-multi-layer-network">Level 3: Multi-Layer Network</a></li>
  <li><a href="#level-4-attention-transformers" id="toc-level-4-attention-transformers" class="nav-link" data-scroll-target="#level-4-attention-transformers">Level 4: Attention &amp; Transformers</a></li>
  </ul></li>
  <li><a href="#interactive-exploration" id="toc-interactive-exploration" class="nav-link" data-scroll-target="#interactive-exploration">Interactive Exploration</a></li>
  <li><a href="#further-resources" id="toc-further-resources" class="nav-link" data-scroll-target="#further-resources">Further Resources</a>
  <ul class="collapse">
  <li><a href="#video-explanations-3blue1brown" id="toc-video-explanations-3blue1brown" class="nav-link" data-scroll-target="#video-explanations-3blue1brown">Video Explanations (3Blue1Brown)</a></li>
  <li><a href="#key-papers" id="toc-key-papers" class="nav-link" data-scroll-target="#key-papers">Key Papers</a></li>
  <li><a href="#next-steps-in-this-course" id="toc-next-steps-in-this-course" class="nav-link" data-scroll-target="#next-steps-in-this-course">Next Steps in This Course</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This guide builds your understanding of neural networks from the ground up, starting with how individual neurons process data, scaling to vectorized layer operations, and culminating in transformer architectures with attention mechanisms.</p>
<div class="cell" data-fig-width="10" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    start["🎯 Start Here"] --&gt; part1["Part 1&lt;br/&gt;Single Neuron&lt;br/&gt;────&lt;br/&gt;Activation&lt;br/&gt;Functions"]
    part1 --&gt; part2["Part 2&lt;br/&gt;Layers&lt;br/&gt;────&lt;br/&gt;Vectorization&lt;br/&gt;&amp; Depth"]
    part2 --&gt; part3["Part 3&lt;br/&gt;Attention&lt;br/&gt;────&lt;br/&gt;Context-Aware&lt;br/&gt;Processing"]
    part3 --&gt; part4["Part 4&lt;br/&gt;Transformers&lt;br/&gt;────&lt;br/&gt;Complete&lt;br/&gt;Architecture"]
    part4 --&gt; end_goal["🎓 Goal Achieved&lt;br/&gt;────&lt;br/&gt;Understand GPT,&lt;br/&gt;BERT, GFMs"]
    
    style start fill:#e1f5ff
    style part1 fill:#e8f0ff
    style part2 fill:#f0e1ff
    style part3 fill:#ffe8e1
    style part4 fill:#ffe1e1
    style end_goal fill:#e1ffe1
</pre>
</div>
<p></p><figcaption> Learning Roadmap: Our Journey Through Neural Networks</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>What you’ll learn</strong>: - How neurons transform data with activation functions - How vectorization makes processing efficient - Why attention is revolutionary for sequence modeling - How transformers combine these ideas into powerful models</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video Resources
</div>
</div>
<div class="callout-body-container callout-body">
<p>For excellent visual explanations of neural networks, check out 3Blue1Brown’s Neural Networks series:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=aircAruvnKk">But what is a neural network?</a> - Foundation concepts</li>
<li><a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">Gradient descent, how neural networks learn</a> - Training process</li>
</ul>
</div>
</div>
<hr>
</section>
<section id="part-1-the-individual-neuron" class="level2">
<h2 class="anchored" data-anchor-id="part-1-the-individual-neuron">Part 1: The Individual Neuron</h2>
<section id="what-does-a-neuron-do" class="level3">
<h3 class="anchored" data-anchor-id="what-does-a-neuron-do">What Does a Neuron Do?</h3>
<p>At its core, a neuron is a simple computational unit that:</p>
<ol type="1">
<li>Takes multiple inputs</li>
<li>Combines them with learned weights</li>
<li>Adds a bias term</li>
<li>Passes the result through an activation function</li>
</ol>
<div class="cell" data-fig-width="8" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph inputs["Inputs"]
        x1["x₁ = 1.5"]
        x2["x₂ = 2.0"]
        x3["x₃ = -0.5"]
    end
    
    subgraph weights["× Weights"]
        w1["w₁ = 0.5"]
        w2["w₂ = -1.0"]
        w3["w₃ = 0.3"]
    end
    
    subgraph computation["Weighted Sum"]
        sum["Σ(xᵢ × wᵢ) + b"]
        bias["+ bias (0.5)"]
    end
    
    subgraph activation["Activation"]
        z["z = -0.35"]
        act["f(z)"]
        output["Output"]
    end
    
    x1 --&gt; w1
    x2 --&gt; w2
    x3 --&gt; w3
    w1 --&gt; sum
    w2 --&gt; sum
    w3 --&gt; sum
    bias --&gt; sum
    sum --&gt; z
    z --&gt; act
    act --&gt; output
    
    style inputs fill:#e1f5ff
    style weights fill:#fff3e1
    style computation fill:#f0e1ff
    style activation fill:#e1ffe1
</pre>
</div>
<p></p><figcaption> Single Neuron Computation Flow</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Key insight</strong>: A neuron is just a weighted sum followed by a non-linear function. That’s it!</p>
<p>Let’s see this in action with code:</p>
<div id="4ec2df1b" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># A single neuron processes inputs</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> single_neuron(inputs, weights, bias, activation_fn):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate a single neuron's computation.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        inputs: array of input values</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">        weights: array of weights (same length as inputs)</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">        bias: single bias value</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">        activation_fn: function to apply to weighted sum</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">        The neuron's output after activation</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Weighted sum</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.dot(inputs, weights) <span class="op">+</span> bias</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Apply activation function</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> activation_fn(z)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output, z</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Example inputs and parameters</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.array([<span class="fl">1.5</span>, <span class="fl">2.0</span>, <span class="op">-</span><span class="fl">0.5</span>])</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.3</span>])</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Without activation (just linear combination)</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>output_linear, z <span class="op">=</span> single_neuron(inputs, weights, bias, <span class="kw">lambda</span> x: x)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input values: </span><span class="sc">{</span>inputs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weights: </span><span class="sc">{</span>weights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bias: </span><span class="sc">{</span>bias<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Weighted sum (z): </span><span class="sc">{</span>z<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Linear output: </span><span class="sc">{</span>output_linear<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>What to notice:</strong> The neuron computes a weighted sum of its inputs plus a bias. Without an activation function, this is just a linear transformation—not very powerful for learning complex patterns.</p>
</section>
<section id="activation-functions-adding-non-linearity" class="level3">
<h3 class="anchored" data-anchor-id="activation-functions-adding-non-linearity">Activation Functions: Adding Non-Linearity</h3>
<p>Activation functions introduce <strong>non-linearity</strong>, which is crucial for neural networks to learn complex patterns. Let’s explore the most common ones:</p>
<div id="f6cb2c6c" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define common activation functions</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""ReLU: Rectified Linear Unit"""</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, x)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Sigmoid: Squashes values to (0, 1)"""</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Tanh: Squashes values to (-1, 1)"""</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.tanh(x)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> leaky_relu(x, alpha<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Leaky ReLU: Like ReLU but allows small negative values"""</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.where(x <span class="op">&gt;</span> <span class="dv">0</span>, x, alpha <span class="op">*</span> x)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize these functions</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"># ReLU</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].plot(x, relu(x), <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'ReLU: max(0, x)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'Input (x)'</span>)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'Output'</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmoid</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].plot(x, sigmoid(x), <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Sigmoid: 1/(1+e^(-x))'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Input (x)'</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Output'</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].axhline(y<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'r'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Tanh</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].plot(x, tanh(x), <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'Tanh: (e^x - e^(-x))/(e^x + e^(-x))'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'Input (x)'</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'Output'</span>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Leaky ReLU</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].plot(x, leaky_relu(x), <span class="st">'m-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Leaky ReLU: max(0.01x, x)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Input (x)'</span>)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Output'</span>)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>What to notice:</strong></p>
<ul>
<li><strong>ReLU</strong> is the most popular: simple, fast, and effective. It “turns off” negative values completely.</li>
<li><strong>Sigmoid</strong> squashes values between 0 and 1, useful for probabilities.</li>
<li><strong>Tanh</strong> centers outputs around 0, often better than sigmoid for hidden layers.</li>
<li><strong>Leaky ReLU</strong> prevents “dead neurons” by allowing small negative values.</li>
</ul>
</section>
<section id="seeing-the-effect-of-activation-functions" class="level3">
<h3 class="anchored" data-anchor-id="seeing-the-effect-of-activation-functions">Seeing the Effect of Activation Functions</h3>
<p>Let’s see how different activation functions transform our neuron’s output:</p>
<div id="5608df66" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the same inputs from before</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.array([<span class="fl">1.5</span>, <span class="fl">2.0</span>, <span class="op">-</span><span class="fl">0.5</span>])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.3</span>])</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute weighted sum</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.dot(inputs, weights) <span class="op">+</span> bias</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted sum (z): </span><span class="sc">{</span>z<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply different activations</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>activations <span class="op">=</span> {</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Linear (no activation)'</span>: <span class="kw">lambda</span> x: x,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ReLU'</span>: relu,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sigmoid'</span>: sigmoid,</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Tanh'</span>: tanh,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Leaky ReLU'</span>: leaky_relu</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, fn <span class="kw">in</span> activations.items():</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> fn(z)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">:25s}</span><span class="ss">: </span><span class="sc">{</span>output<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Why this matters:</strong> The activation function dramatically changes the neuron’s output. This non-linear transformation is what allows neural networks to learn complex, non-linear patterns in data.</p>
<hr>
</section>
</section>
<section id="part-2-from-single-neurons-to-layers" class="level2">
<h2 class="anchored" data-anchor-id="part-2-from-single-neurons-to-layers">Part 2: From Single Neurons to Layers</h2>
<section id="the-power-of-vectorization" class="level3">
<h3 class="anchored" data-anchor-id="the-power-of-vectorization">The Power of Vectorization</h3>
<p>Rather than computing neurons one at a time, we can process an entire layer simultaneously using matrix operations. This is both computationally efficient and conceptually elegant.</p>
<div class="cell" data-fig-width="10" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TB
    subgraph single["Single Neuron (Sequential)"]
        direction LR
        i1["Input&lt;br/&gt;Vector&lt;br/&gt;(3 dims)"] --&gt; n1["Neuron 1"]
        i2["Input&lt;br/&gt;Vector&lt;br/&gt;(3 dims)"] --&gt; n2["Neuron 2"]
        i3["Input&lt;br/&gt;Vector&lt;br/&gt;(3 dims)"] --&gt; n3["Neuron 3"]
        n1 --&gt; o1["Output 1"]
        n2 --&gt; o2["Output 2"]
        n3 --&gt; o3["Output 3"]
    end
    
    subgraph vectorized["Vectorized Layer (Parallel)"]
        direction LR
        iv["Input&lt;br/&gt;Vector&lt;br/&gt;(3 dims)"] --&gt; matrix["Weight Matrix&lt;br/&gt;(3 × 5)&lt;br/&gt;+ Bias&lt;br/&gt;+ Activation"]
        matrix --&gt; ov["Output&lt;br/&gt;Vector&lt;br/&gt;(5 dims)"]
    end
    
    single -.-&gt;|"Matrix Operation"| vectorized
    
    style single fill:#ffe1e1
    style vectorized fill:#e1ffe1
    style matrix fill:#fff3e1
</pre>
</div>
<p></p><figcaption> From Single Neuron to Layer of Neurons</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Key insight</strong>: Instead of looping through neurons, one matrix multiplication processes all neurons simultaneously!</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video Resource
</div>
</div>
<div class="callout-body-container callout-body">
<p>For understanding how layers work together, see 3Blue1Brown’s <a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;t=420s">Neural network intuitions</a> (timestamp 7:00 onwards).</p>
</div>
</div>
<div id="b2a586c7" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A layer is just multiple neurons working in parallel</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralLayer:</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A single layer of neurons with vectorized operations."""</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_inputs, n_neurons):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialize a layer.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">            n_inputs: number of input features</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">            n_neurons: number of neurons in this layer</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each neuron has n_inputs weights</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape: (n_inputs, n_neurons)</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(n_inputs, n_neurons) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each neuron has one bias</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape: (n_neurons,)</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> np.zeros(n_neurons)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs, activation_fn<span class="op">=</span>relu):</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co">        Forward pass through the layer.</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co">            inputs: input array of shape (n_samples, n_inputs)</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co">            activation_fn: activation function to apply</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co">            outputs after activation, shape (n_samples, n_neurons)</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Matrix multiplication: (n_samples, n_inputs) @ (n_inputs, n_neurons)</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Result: (n_samples, n_neurons)</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(inputs, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.biases</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply activation function element-wise</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> activation_fn(z)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a layer with 3 inputs and 5 neurons</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> NeuralLayer(n_inputs<span class="op">=</span><span class="dv">3</span>, n_neurons<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Process a batch of 4 samples</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>batch_inputs <span class="op">=</span> np.random.randn(<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> layer.forward(batch_inputs, activation_fn<span class="op">=</span>relu)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>batch_inputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weight matrix shape: </span><span class="sc">{</span>layer<span class="sc">.</span>weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>outputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample input:"</span>)</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_inputs[<span class="dv">0</span>])</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Corresponding output:"</span>)</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outputs[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>What to notice:</strong></p>
<ul>
<li>The weight matrix has shape <code>(n_inputs, n_neurons)</code> - each column represents one neuron’s weights.</li>
<li>One matrix multiplication processes all neurons and all samples simultaneously.</li>
<li>Output shape is <code>(n_samples, n_neurons)</code> - each sample gets transformed into a vector of neuron activations.</li>
</ul>
</section>
<section id="building-a-multi-layer-network" class="level3">
<h3 class="anchored" data-anchor-id="building-a-multi-layer-network">Building a Multi-Layer Network</h3>
<p>Now let’s stack multiple layers to create a deep neural network:</p>
<div class="cell" data-fig-width="10" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    subgraph input["Input Layer"]
        i1["x₁"]
        i2["x₂"]
        i3["x₃"]
    end
    
    subgraph hidden1["Hidden Layer 1&lt;br/&gt;(8 neurons)"]
        h11["🔵"]
        h12["🔵"]
        h13["🔵"]
        h14["🔵"]
        h15["🔵"]
        h16["🔵"]
        h17["🔵"]
        h18["🔵"]
    end
    
    subgraph hidden2["Hidden Layer 2&lt;br/&gt;(5 neurons)"]
        h21["🟢"]
        h22["🟢"]
        h23["🟢"]
        h24["🟢"]
        h25["🟢"]
    end
    
    subgraph output["Output Layer&lt;br/&gt;(2 neurons)"]
        o1["🔴"]
        o2["🔴"]
    end
    
    i1 --&gt; h11 &amp; h12 &amp; h13 &amp; h14 &amp; h15 &amp; h16 &amp; h17 &amp; h18
    i2 --&gt; h11 &amp; h12 &amp; h13 &amp; h14 &amp; h15 &amp; h16 &amp; h17 &amp; h18
    i3 --&gt; h11 &amp; h12 &amp; h13 &amp; h14 &amp; h15 &amp; h16 &amp; h17 &amp; h18
    
    h11 &amp; h12 &amp; h13 &amp; h14 &amp; h15 &amp; h16 &amp; h17 &amp; h18 --&gt; h21 &amp; h22 &amp; h23 &amp; h24 &amp; h25
    
    h21 &amp; h22 &amp; h23 &amp; h24 &amp; h25 --&gt; o1 &amp; o2
    
    style input fill:#e1f5ff
    style hidden1 fill:#e8e1ff
    style hidden2 fill:#e1ffe8
    style output fill:#ffe1e1
</pre>
</div>
<p></p><figcaption> Multi-Layer Neural Network Architecture</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Key insight</strong>: Data flows forward through the network, with each layer creating progressively more abstract representations.</p>
<p>Now let’s implement this:</p>
<div id="579889b5" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNeuralNetwork:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A simple feedforward neural network."""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layer_sizes):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">            layer_sizes: list of layer sizes, e.g., [3, 8, 5, 2]</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">                        means 3 inputs, two hidden layers (8 and 5 neurons),</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">                        and 2 output neurons</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> []</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(layer_sizes) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> NeuralLayer(layer_sizes[i], layer_sizes[i <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.layers.append(layer)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass through all layers."""</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        activation <span class="op">=</span> inputs</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Forward pass through network:"</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>activation<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.layers):</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use ReLU for hidden layers, linear for output</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(<span class="va">self</span>.layers) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>                activation <span class="op">=</span> layer.forward(activation, activation_fn<span class="op">=</span>relu)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>                activation <span class="op">=</span> layer.forward(activation, activation_fn<span class="op">=</span><span class="kw">lambda</span> x: x)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"After layer </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>activation<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> activation</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a network: 3 inputs -&gt; 8 hidden -&gt; 5 hidden -&gt; 2 outputs</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> SimpleNeuralNetwork([<span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">2</span>])</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Process a batch of data</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> np.random.randn(<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> network.forward(batch)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Final output:</span><span class="ch">\n</span><span class="sc">{</span>output<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>What to notice:</strong> - Each layer transforms the data: <code>(batch, n_in) -&gt; (batch, n_out)</code> - The output of one layer becomes the input to the next - This creates a series of increasingly abstract representations</p>
</section>
<section id="visualizing-the-transformation" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-transformation">Visualizing the Transformation</h3>
<p>Let’s see how data is transformed as it flows through the network:</p>
<div id="41ea08a0" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simpler network for visualization</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>viz_network <span class="op">=</span> SimpleNeuralNetwork([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create some structured input data</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, <span class="dv">100</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.column_stack([np.cos(theta), np.sin(theta)])</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Track activations at each layer</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>activations <span class="op">=</span> [inputs]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>current <span class="op">=</span> inputs</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(viz_network.layers):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(viz_network.layers) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        current <span class="op">=</span> layer.forward(current, activation_fn<span class="op">=</span>relu)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        current <span class="op">=</span> layer.forward(current, activation_fn<span class="op">=</span><span class="kw">lambda</span> x: x)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    activations.append(current)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the transformations</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (ax, act) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(axes, activations)):</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> <span class="st">"Input Space"</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="bu">len</span>(activations) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> <span class="st">"Output Space"</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> <span class="ss">f"Hidden Layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot first two dimensions</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    ax.scatter(act[:, <span class="dv">0</span>], act[:, <span class="dv">1</span>], c<span class="op">=</span>theta, cmap<span class="op">=</span><span class="st">'viridis'</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Dimension 1'</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Dimension 2'</span>)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>What to notice:</strong> Each layer progressively transforms the data, creating new representations. The network learns to map inputs to outputs through these transformations.</p>
<hr>
</section>
</section>
<section id="part-3-attention-mechanisms-and-transformers" class="level2">
<h2 class="anchored" data-anchor-id="part-3-attention-mechanisms-and-transformers">Part 3: Attention Mechanisms and Transformers</h2>
<section id="the-limitation-of-basic-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="the-limitation-of-basic-neural-networks">The Limitation of Basic Neural Networks</h3>
<p>Traditional feedforward networks process each input independently. But what if relationships <em>between</em> inputs matter? This is where <strong>attention mechanisms</strong> come in.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video Resource
</div>
</div>
<div class="callout-body-container callout-body">
<p>For an excellent visual explanation of attention and transformers, watch: - <a href="https://www.youtube.com/watch?v=eMlx5fFNoYc">Attention in transformers, visually explained</a> by 3Blue1Brown - <a href="https://www.youtube.com/watch?v=eMlx5fFNoYc&amp;t=0s">Visualizing Attention, a Transformer’s Heart</a> (full explanation)</p>
</div>
</div>
</section>
<section id="what-is-attention" class="level3">
<h3 class="anchored" data-anchor-id="what-is-attention">What is Attention?</h3>
<p>Attention allows the network to <strong>focus on relevant parts of the input</strong> when processing each element. Think of reading a sentence: to understand “it,” you need to look back and find what “it” refers to.</p>
<div class="cell" data-fig-width="10" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TB
    subgraph input["Input Sequence"]
        t1["Token 1"]
        t2["Token 2"]
        t3["Token 3"]
        t4["Token 4"]
    end
    
    subgraph qkv["Linear Projections"]
        direction LR
        q["Query (Q)&lt;br/&gt;What I'm looking for"]
        k["Key (K)&lt;br/&gt;What I offer"]
        v["Value (V)&lt;br/&gt;What I return"]
    end
    
    subgraph attention["Attention Computation"]
        similarity["Compute Similarity&lt;br/&gt;Q · Kᵀ"]
        weights["Softmax&lt;br/&gt;(Attention Weights)"]
        output["Weighted Sum&lt;br/&gt;Σ(weights × V)"]
    end
    
    subgraph result["Context-Aware Output"]
        o1["Output 1&lt;br/&gt;(informed by all tokens)"]
        o2["Output 2&lt;br/&gt;(informed by all tokens)"]
        o3["Output 3&lt;br/&gt;(informed by all tokens)"]
        o4["Output 4&lt;br/&gt;(informed by all tokens)"]
    end
    
    t1 &amp; t2 &amp; t3 &amp; t4 --&gt; qkv
    qkv --&gt; similarity
    similarity --&gt; weights
    weights --&gt; output
    output --&gt; o1 &amp; o2 &amp; o3 &amp; o4
    
    style input fill:#e1f5ff
    style qkv fill:#fff3e1
    style attention fill:#f0e1ff
    style result fill:#e1ffe1
</pre>
</div>
<p></p><figcaption> Attention Mechanism: Query-Key-Value</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Key insight</strong>: Each token can “attend to” (look at) all other tokens, deciding which are most relevant for its own representation.</p>
<div id="1736b9e0" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_attention(query, keys, values):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simplified attention mechanism.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">        query: what we're looking for (n_queries, d_k)</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">        keys: what we're comparing against (n_keys, d_k)</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">        values: what we return (n_keys, d_v)</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">        weighted combination of values</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Compute similarity scores</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># How much does each key match the query?</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.dot(query, keys.T)  <span class="co"># (n_queries, n_keys)</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Convert to attention weights (softmax)</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scale by sqrt of dimension for stability</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    d_k <span class="op">=</span> keys.shape[<span class="dv">1</span>]</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores <span class="op">/</span> np.sqrt(d_k)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> np.exp(scores) <span class="op">/</span> np.exp(scores).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Weighted sum of values</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> np.dot(attention_weights, values)  <span class="co"># (n_queries, d_v)</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output, attention_weights</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: A simple sequence</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's say we have 4 tokens (words), each represented by a 3D vector</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>sequence <span class="op">=</span> np.array([</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.5</span>],  <span class="co"># Token 0</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">0.2</span>],  <span class="co"># Token 1</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">1.0</span>],  <span class="co"># Token 2</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span>],  <span class="co"># Token 3</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's see what Token 2 attends to</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> sequence[<span class="dv">2</span>:<span class="dv">3</span>]  <span class="co"># Query is Token 2</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> sequence        <span class="co"># Keys are all tokens</span></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> sequence      <span class="co"># Values are all tokens (simplified)</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>output, attention_weights <span class="op">=</span> simple_attention(query, keys, values)</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights for Token 2:"</span>)</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, weight <span class="kw">in</span> <span class="bu">enumerate</span>(attention_weights[<span class="dv">0</span>]):</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Token </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>weight<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Original Token 2: </span><span class="sc">{</span>sequence[<span class="dv">2</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"After attention:  </span><span class="sc">{</span>output[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>What to notice:</strong> - The attention weights sum to 1.0 - Token 2 attends most to itself, but also considers other tokens - The output is a weighted combination - it’s been “informed” by the context</p>
</section>
<section id="visualizing-attention-patterns" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-attention-patterns">Visualizing Attention Patterns</h3>
<div id="8c8b3343" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute full attention matrix (all tokens attending to all tokens)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>all_queries <span class="op">=</span> sequence</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>all_keys <span class="op">=</span> sequence</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>all_values <span class="op">=</span> sequence</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>outputs, full_attention <span class="op">=</span> simple_attention(all_queries, all_keys, all_values)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the attention matrix</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Attention matrix</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax1.imshow(full_attention, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Attention Matrix</span><span class="ch">\n</span><span class="st">(Which tokens attend to which)'</span>, </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>              fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Key Token'</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Query Token'</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>ax1.set_xticks(<span class="bu">range</span>(<span class="dv">4</span>))</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>ax1.set_yticks(<span class="bu">range</span>(<span class="dv">4</span>))</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>ax1, label<span class="op">=</span><span class="st">'Attention Weight'</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Add values to cells</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> ax1.text(j, i, <span class="ss">f'</span><span class="sc">{</span>full_attention[i, j]<span class="sc">:.2f}</span><span class="ss">'</span>,</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>                       ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>, color<span class="op">=</span><span class="st">"white"</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Show transformation</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>ax2.bar(<span class="bu">range</span>(<span class="dv">4</span>), sequence[:, <span class="dv">0</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Original (dim 0)'</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>ax2.bar(<span class="bu">range</span>(<span class="dv">4</span>), outputs[:, <span class="dv">0</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'After attention (dim 0)'</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Token Representations</span><span class="ch">\n</span><span class="st">(First dimension)'</span>, </span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>              fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Token'</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Value'</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>What to notice:</strong> The attention matrix shows which tokens influence each other. Diagonal values are often high (tokens attend to themselves), but off-diagonal values capture contextual relationships.</p>
</section>
<section id="multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention">Multi-Head Attention</h3>
<p>Transformers use <strong>multiple attention heads</strong> to capture different types of relationships simultaneously.</p>
<div class="cell" data-fig-width="10" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TB
    subgraph input_seq["Input Sequence (d_model = 512)"]
        seq["Token 1, Token 2, ..., Token n"]
    end
    
    subgraph linear_proj["Linear Projections"]
        wq["W_Q"]
        wk["W_K"]
        wv["W_V"]
    end
    
    subgraph heads["Split into Multiple Heads (e.g., 8 heads × 64 dims)"]
        direction LR
        head1["Head 1&lt;br/&gt;🔵&lt;br/&gt;Attends to&lt;br/&gt;Syntax"]
        head2["Head 2&lt;br/&gt;🟢&lt;br/&gt;Attends to&lt;br/&gt;Semantics"]
        head3["Head 3&lt;br/&gt;🟡&lt;br/&gt;Attends to&lt;br/&gt;Position"]
        dots["..."]
        head8["Head 8&lt;br/&gt;🔴&lt;br/&gt;Attends to&lt;br/&gt;Context"]
    end
    
    subgraph attention_ops["Parallel Attention Operations"]
        att1["Attention&lt;br/&gt;Computation"]
        att2["Attention&lt;br/&gt;Computation"]
        att3["Attention&lt;br/&gt;Computation"]
        att4["Attention&lt;br/&gt;Computation"]
    end
    
    subgraph concat["Concatenate Heads"]
        combined["Combined Output&lt;br/&gt;(8 heads × 64 = 512 dims)"]
    end
    
    subgraph final["Final Projection"]
        wo["W_O&lt;br/&gt;(512 × 512)"]
        output["Context-Aware&lt;br/&gt;Representations"]
    end
    
    seq --&gt; linear_proj
    linear_proj --&gt; heads
    head1 --&gt; att1
    head2 --&gt; att2
    head3 --&gt; att3
    head8 --&gt; att4
    att1 &amp; att2 &amp; att3 &amp; att4 --&gt; combined
    combined --&gt; wo
    wo --&gt; output
    
    style input_seq fill:#e1f5ff
    style linear_proj fill:#fff3e1
    style heads fill:#f0e1ff
    style attention_ops fill:#ffe1f0
    style concat fill:#e1ffe8
    style final fill:#e1ffe1
</pre>
</div>
<p></p><figcaption> Multi-Head Attention Architecture</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Key insight</strong>: Multiple heads can learn different attention patterns - some might focus on nearby words, others on distant relationships, enabling richer representations.</p>
<div id="a72fab1b" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention:</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Multi-head attention mechanism."""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, n_heads):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">            d_model: dimension of the model (e.g., 512)</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">            n_heads: number of attention heads (e.g., 8)</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_heads <span class="op">=</span> n_heads</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> n_heads  <span class="co"># dimension per head</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear projections for Q, K, V</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_q <span class="op">=</span> np.random.randn(d_model, d_model) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_k <span class="op">=</span> np.random.randn(d_model, d_model) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_v <span class="op">=</span> np.random.randn(d_model, d_model) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_o <span class="op">=</span> np.random.randn(d_model, d_model) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> split_heads(<span class="va">self</span>, x):</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Split the last dimension into (n_heads, d_k)."""</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_len, d_model <span class="op">=</span> x.shape</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape to (batch_size, seq_len, n_heads, d_k)</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(batch_size, seq_len, <span class="va">self</span>.n_heads, <span class="va">self</span>.d_k)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transpose to (batch_size, n_heads, seq_len, d_k)</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> attention(<span class="va">self</span>, q, k, v):</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Scaled dot-product attention."""</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        d_k <span class="op">=</span> q.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> np.matmul(q, k.transpose(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)) <span class="op">/</span> np.sqrt(d_k)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> np.exp(scores) <span class="op">/</span> np.exp(scores).<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> np.matmul(attention_weights, v)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention_weights</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="co">        Forward pass.</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="co">            x: input tensor of shape (batch_size, seq_len, d_model)</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="co">            output of shape (batch_size, seq_len, d_model)</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_len, _ <span class="op">=</span> x.shape</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear projections</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> np.dot(x, <span class="va">self</span>.W_q)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> np.dot(x, <span class="va">self</span>.W_k)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> np.dot(x, <span class="va">self</span>.W_v)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split into multiple heads</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.split_heads(q.reshape(batch_size, seq_len, <span class="va">self</span>.d_model))</span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">self</span>.split_heads(k.reshape(batch_size, seq_len, <span class="va">self</span>.d_model))</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.split_heads(v.reshape(batch_size, seq_len, <span class="va">self</span>.d_model))</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply attention</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>        attended, attention_weights <span class="op">=</span> <span class="va">self</span>.attention(q, k, v)</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concatenate heads</span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>        attended <span class="op">=</span> attended.transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>        concatenated <span class="op">=</span> attended.reshape(batch_size, seq_len, <span class="va">self</span>.d_model)</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Final linear projection</span></span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> np.dot(concatenated, <span class="va">self</span>.W_o)</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention_weights</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Create multi-head attention</span></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>n_heads <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadAttention(d_model, n_heads)</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Process a sequence</span></span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>output, attention_weights <span class="op">=</span> mha.forward(x)</span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention weights shape: </span><span class="sc">{</span>attention_weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Number of heads: </span><span class="sc">{</span>n_heads<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Each head attends to sequence of length: </span><span class="sc">{</span>seq_len<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>What to notice:</strong></p>
<ul>
<li>Each head learns different attention patterns</li>
<li>Multiple heads capture multiple types of relationships simultaneously</li>
<li>This is much more powerful than single-head attention</li>
</ul>
</section>
<section id="how-attention-changes-encodings" class="level3">
<h3 class="anchored" data-anchor-id="how-attention-changes-encodings">How Attention Changes Encodings</h3>
<p>Let’s see how attention modifies representations compared to a simple feedforward layer:</p>
<div class="cell" data-fig-width="12" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TB
    subgraph comparison["Processing Paradigms"]
        direction LR
        
        subgraph ff["Feedforward Network"]
            direction TB
            ff_input["Token 1 | Token 2 | Token 3 | Token 4"]
            ff_process["↓ Independent Processing ↓"]
            ff_layer["Layer applies SAME transformation&lt;br/&gt;to each position separately"]
            ff_output["Output 1 | Output 2 | Output 3 | Output 4"]
            ff_note["❌ No communication between positions"]
            
            ff_input --&gt; ff_process
            ff_process --&gt; ff_layer
            ff_layer --&gt; ff_output
            ff_output --&gt; ff_note
        end
        
        subgraph attn["Attention Network"]
            direction TB
            attn_input["Token 1 | Token 2 | Token 3 | Token 4"]
            attn_process["↓ Context-Aware Processing ↓"]
            attn_layer["Each position looks at ALL positions&lt;br/&gt;Weighted combination based on relevance"]
            attn_output["Output 1 | Output 2 | Output 3 | Output 4"]
            attn_note["✅ Each output informed by full context"]
            
            attn_input --&gt; attn_process
            attn_process --&gt; attn_layer
            attn_layer --&gt; attn_output
            attn_output --&gt; attn_note
        end
    end
    
    subgraph examples["Real-World Example"]
        direction LR
        sentence["'The animal didn't cross the street because it was too tired'"]
        ff_ex["Feedforward: 'it' processed in isolation&lt;br/&gt;❌ Can't determine if 'it' = animal or street"]
        attn_ex["Attention: 'it' attends to all words&lt;br/&gt;✅ Learns 'it' → 'animal' (via context)"]
        
        sentence --&gt; ff_ex
        sentence --&gt; attn_ex
    end
    
    style ff fill:#ffe1e1
    style attn fill:#e1ffe1
    style ff_note fill:#ffcccc
    style attn_note fill:#ccffcc
    style examples fill:#fff9e1
</pre>
</div>
<p></p><figcaption> Feedforward vs Attention: The Key Difference</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Why this is revolutionary</strong>: Attention allows the network to dynamically route information based on context, rather than applying fixed transformations. This is essential for understanding language, time series, and sequential data where relationships between elements matter.</p>
<div id="f60ab7fc" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample sequence data</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Input sequence</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>input_seq <span class="op">=</span> np.random.randn(<span class="dv">1</span>, seq_length, d_model)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Option 1: Process with feedforward layer (no attention)</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>ff_layer <span class="op">=</span> NeuralLayer(d_model, d_model)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>ff_output <span class="op">=</span> ff_layer.forward(input_seq.reshape(<span class="op">-</span><span class="dv">1</span>, d_model), activation_fn<span class="op">=</span>relu)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>ff_output <span class="op">=</span> ff_output.reshape(<span class="dv">1</span>, seq_length, d_model)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Option 2: Process with attention</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadAttention(d_model, n_heads<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>attn_output, attn_weights <span class="op">=</span> mha.forward(input_seq)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the difference</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Input</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> axes[<span class="dv">0</span>].imshow(input_seq[<span class="dv">0</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Input Sequence'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Position in Sequence'</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Feature Dimension'</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im0, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Feedforward output</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> axes[<span class="dv">1</span>].imshow(ff_output[<span class="dv">0</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'After Feedforward Layer</span><span class="ch">\n</span><span class="st">(No attention)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Position in Sequence'</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Feature Dimension'</span>)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im1, ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Attention output</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>im2 <span class="op">=</span> axes[<span class="dv">2</span>].imshow(attn_output[<span class="dv">0</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'After Multi-Head Attention</span><span class="ch">\n</span><span class="st">(Context-aware)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">'Position in Sequence'</span>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'Feature Dimension'</span>)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im2, ax<span class="op">=</span>axes[<span class="dv">2</span>])</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Show how representations relate to each other</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Cosine similarity between position encodings:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Feedforward (independent processing):"</span>)</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>ff_norm <span class="op">=</span> ff_output[<span class="dv">0</span>] <span class="op">/</span> np.linalg.norm(ff_output[<span class="dv">0</span>], axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>ff_similarity <span class="op">=</span> np.dot(ff_norm, ff_norm.T)</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average off-diagonal similarity: </span><span class="sc">{</span>(ff_similarity.<span class="bu">sum</span>() <span class="op">-</span> seq_length) <span class="op">/</span> (seq_length <span class="op">*</span> (seq_length <span class="op">-</span> <span class="dv">1</span>))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Attention (context-aware processing):"</span>)</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>attn_norm <span class="op">=</span> attn_output[<span class="dv">0</span>] <span class="op">/</span> np.linalg.norm(attn_output[<span class="dv">0</span>], axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>attn_similarity <span class="op">=</span> np.dot(attn_norm, attn_norm.T)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average off-diagonal similarity: </span><span class="sc">{</span>(attn_similarity.<span class="bu">sum</span>() <span class="op">-</span> seq_length) <span class="op">/</span> (seq_length <span class="op">*</span> (seq_length <span class="op">-</span> <span class="dv">1</span>))<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Why this matters:</strong></p>
<ul>
<li><strong>Feedforward layers</strong> process each position independently - no position “knows” about others</li>
<li><strong>Attention layers</strong> mix information across positions - each position is informed by context</li>
<li>This context-awareness is crucial for sequential data like language, time series, or video</li>
</ul>
<hr>
</section>
</section>
<section id="part-4-putting-it-all-together---the-transformer-block" class="level2">
<h2 class="anchored" data-anchor-id="part-4-putting-it-all-together---the-transformer-block">Part 4: Putting It All Together - The Transformer Block</h2>
<p>A complete transformer block combines attention with feedforward layers:</p>
<div class="cell" data-fig-width="8" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TB
    input["Input&lt;br/&gt;(Sequence of Embeddings)"]
    
    subgraph mha_block["Multi-Head Attention Block"]
        mha["Multi-Head&lt;br/&gt;Attention"]
        add1["Add"]
        norm1["Layer Norm"]
    end
    
    subgraph ff_block["Feedforward Block"]
        ff1["Linear&lt;br/&gt;(expand)"]
        relu["ReLU"]
        ff2["Linear&lt;br/&gt;(project)"]
        add2["Add"]
        norm2["Layer Norm"]
    end
    
    output["Output&lt;br/&gt;(Enriched Representations)"]
    
    input --&gt; mha
    input -.-&gt;|"Residual&lt;br/&gt;Connection"| add1
    mha --&gt; add1
    add1 --&gt; norm1
    
    norm1 --&gt; ff1
    ff1 --&gt; relu
    relu --&gt; ff2
    norm1 -.-&gt;|"Residual&lt;br/&gt;Connection"| add2
    ff2 --&gt; add2
    add2 --&gt; norm2
    
    norm2 --&gt; output
    
    style input fill:#e1f5ff
    style mha_block fill:#f0e1ff
    style ff_block fill:#ffe8e1
    style output fill:#e1ffe1
    style mha fill:#d8b3ff
    style add1 fill:#ffd8b3
    style add2 fill:#ffd8b3
</pre>
</div>
<p></p><figcaption> Complete Transformer Block Architecture</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Key components</strong>:</p>
<ol type="1">
<li><strong>Multi-Head Attention</strong>: Mix information across positions (context)</li>
<li><strong>Residual Connection</strong>: Add input back to help gradient flow</li>
<li><strong>Layer Normalization</strong>: Stabilize training</li>
<li><strong>Feedforward Network</strong>: Transform features independently</li>
<li><strong>Another Residual Connection</strong>: More gradient flow</li>
</ol>
<p>This pattern repeats for each transformer layer in a model!</p>
<div id="a3778da9" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock:</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A single transformer block with attention and feedforward."""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, n_heads, d_ff):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">            d_model: model dimension</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">            n_heads: number of attention heads</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">            d_ff: feedforward network dimension</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(d_model, n_heads)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff1 <span class="op">=</span> NeuralLayer(d_model, d_ff)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff2 <span class="op">=</span> NeuralLayer(d_ff, d_model)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co">        Forward pass through transformer block.</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co">            x: input of shape (batch_size, seq_len, d_model)</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="co">            output of same shape as input</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_len, d_model <span class="op">=</span> x.shape</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: Multi-head attention</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>        attn_out, attn_weights <span class="op">=</span> <span class="va">self</span>.attention.forward(x)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Add &amp; Norm (simplified - just add)</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> attn_out</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: Feedforward network</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape for layer processing</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>        x_flat <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>, d_model)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>        ff_out <span class="op">=</span> <span class="va">self</span>.ff1.forward(x_flat, activation_fn<span class="op">=</span>relu)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        ff_out <span class="op">=</span> <span class="va">self</span>.ff2.forward(ff_out, activation_fn<span class="op">=</span><span class="kw">lambda</span> x: x)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        ff_out <span class="op">=</span> ff_out.reshape(batch_size, seq_len, d_model)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Add &amp; Norm (simplified - just add)</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> x <span class="op">+</span> ff_out</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attn_weights</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and test a transformer block</span></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>transformer <span class="op">=</span> TransformerBlock(d_model<span class="op">=</span><span class="dv">8</span>, n_heads<span class="op">=</span><span class="dv">2</span>, d_ff<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Process a sequence</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>input_seq <span class="op">=</span> np.random.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">8</span>)</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>output, attention <span class="op">=</span> transformer.forward(input_seq)</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>input_seq<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Transformer block completed:"</span>)</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  1. Multi-head attention (context mixing)"</span>)</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  2. Residual connection"</span>)</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  3. Feedforward network (feature transformation)"</span>)</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  4. Residual connection"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>What to notice:</strong> The transformer combines two key ideas:</p>
<ol type="1">
<li><strong>Attention</strong>: Mix information across sequence positions (capture context)</li>
<li><strong>Feedforward</strong>: Transform features independently at each position (extract patterns)</li>
<li><strong>Residual connections</strong>: Add the input back to help gradients flow</li>
</ol>
<hr>
</section>
<section id="summary-the-neural-network-hierarchy" class="level2">
<h2 class="anchored" data-anchor-id="summary-the-neural-network-hierarchy">Summary: The Neural Network Hierarchy</h2>
<p>Let’s recap the progressive complexity:</p>
<div class="cell" data-fig-width="12" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    subgraph level1["Level 1: Single Neuron"]
        n1["Weighted Sum&lt;br/&gt;+ Activation&lt;br/&gt;────&lt;br/&gt;z = Σ(wᵢxᵢ) + b&lt;br/&gt;output = f(z)"]
    end
    
    subgraph level2["Level 2: Neural Layer"]
        layer["Vectorized Operations&lt;br/&gt;────&lt;br/&gt;Multiple neurons in parallel&lt;br/&gt;Matrix: (n_in, n_out)"]
    end
    
    subgraph level3["Level 3: Deep Network"]
        deep["Stacked Layers&lt;br/&gt;────&lt;br/&gt;Input → Hidden₁ → Hidden₂ → Output&lt;br/&gt;Hierarchical features"]
    end
    
    subgraph level4a["Level 4a: Attention Mechanism"]
        attention["Query-Key-Value&lt;br/&gt;────&lt;br/&gt;Context-aware weighting&lt;br/&gt;Tokens attend to each other"]
    end
    
    subgraph level4b["Level 4b: Multi-Head Attention"]
        multihead["Parallel Attention Heads&lt;br/&gt;────&lt;br/&gt;Multiple relationship types&lt;br/&gt;8 heads × 64 dims = 512 dims"]
    end
    
    subgraph level5["Level 5: Transformer Block"]
        transformer["Complete Architecture&lt;br/&gt;────&lt;br/&gt;Multi-Head Attention&lt;br/&gt;+ Residual Connection&lt;br/&gt;+ Feedforward Network&lt;br/&gt;+ Residual Connection"]
    end
    
    subgraph level6["Level 6: Full Transformer"]
        full["Stack of N Blocks&lt;br/&gt;────&lt;br/&gt;Block₁ → Block₂ → ... → Blockₙ&lt;br/&gt;+ Positional Encoding&lt;br/&gt;+ Output Head"]
    end
    
    level1 -.-&gt;|"Parallelize"| level2
    level2 -.-&gt;|"Stack"| level3
    level3 -.-&gt;|"Add Context&lt;br/&gt;Awareness"| level4a
    level4a -.-&gt;|"Multiple&lt;br/&gt;Heads"| level4b
    level4b -.-&gt;|"+ Feedforward&lt;br/&gt;+ Residuals"| level5
    level5 -.-&gt;|"Stack&lt;br/&gt;Layers"| level6
    
    capability1["Independent&lt;br/&gt;Processing"] -.-&gt; level1
    capability2["Batch&lt;br/&gt;Processing"] -.-&gt; level2
    capability3["Hierarchical&lt;br/&gt;Features"] -.-&gt; level3
    capability4["Sequential&lt;br/&gt;Dependencies"] -.-&gt; level4a
    capability5["Rich&lt;br/&gt;Relationships"] -.-&gt; level4b
    capability6["Stable Deep&lt;br/&gt;Learning"] -.-&gt; level5
    capability7["Complex&lt;br/&gt;Understanding"] -.-&gt; level6
    
    style level1 fill:#e1f5ff
    style level2 fill:#e8f0ff
    style level3 fill:#e8e5ff
    style level4a fill:#f0e1ff
    style level4b fill:#f8e1ff
    style level5 fill:#ffe1f0
    style level6 fill:#ffe1e1
    
    style capability1 fill:#fff9e1
    style capability2 fill:#fff9e1
    style capability3 fill:#fff9e1
    style capability4 fill:#fff9e1
    style capability5 fill:#fff9e1
    style capability6 fill:#fff9e1
    style capability7 fill:#fff9e1
</pre>
</div>
<p></p><figcaption> The Complete Neural Network Hierarchy: From Neurons to Transformers</figcaption> </figure><p></p>
</div>
</div>
</div>
<section id="level-1-single-neuron" class="level3">
<h3 class="anchored" data-anchor-id="level-1-single-neuron">Level 1: Single Neuron</h3>
<ul>
<li>Computes weighted sum of inputs</li>
<li>Applies activation function (e.g., ReLU)</li>
<li>Transforms: scalar inputs → scalar output</li>
<li><strong>Key insight</strong>: Non-linearity enables learning complex patterns</li>
</ul>
</section>
<section id="level-2-layer-of-neurons" class="level3">
<h3 class="anchored" data-anchor-id="level-2-layer-of-neurons">Level 2: Layer of Neurons</h3>
<ul>
<li>Multiple neurons computed in parallel (vectorized)</li>
<li>Matrix multiplication: efficient batch processing</li>
<li>Transforms: vector input → vector output</li>
<li><strong>Key insight</strong>: Multiple features extracted simultaneously</li>
</ul>
</section>
<section id="level-3-multi-layer-network" class="level3">
<h3 class="anchored" data-anchor-id="level-3-multi-layer-network">Level 3: Multi-Layer Network</h3>
<ul>
<li>Stack layers to create deep representations</li>
<li>Each layer builds on previous abstractions</li>
<li>Transforms: input space → hidden spaces → output space</li>
<li><strong>Key insight</strong>: Depth creates hierarchical features</li>
</ul>
</section>
<section id="level-4-attention-transformers" class="level3">
<h3 class="anchored" data-anchor-id="level-4-attention-transformers">Level 4: Attention &amp; Transformers</h3>
<ul>
<li>Attention: dynamically weight inputs based on context</li>
<li>Multi-head: capture multiple relationship types</li>
<li>Transformer: attention + feedforward with residual connections</li>
<li><strong>Key insight</strong>: Context-aware processing for sequential data</li>
</ul>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Key Difference
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Traditional Neural Networks</strong>: Process each input independently</p>
<ul>
<li>Good for: images, tabular data where position doesn’t matter</li>
</ul>
<p><strong>Transformers with Attention</strong>: Process inputs in context of each other</p>
<ul>
<li>Good for: language, time series, any sequential data</li>
<li>Revolution: Enables models like GPT, BERT, and modern GFMs</li>
</ul>
</div>
</div>
<hr>
</section>
</section>
<section id="interactive-exploration" class="level2">
<h2 class="anchored" data-anchor-id="interactive-exploration">Interactive Exploration</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Try This Yourself
</div>
</div>
<div class="callout-body-container callout-body">
<p>Experiment with the code above:</p>
<ol type="1">
<li><strong>Activation functions</strong>: Change the activation in <code>single_neuron()</code> and observe output changes</li>
<li><strong>Layer sizes</strong>: Modify <code>layer_sizes</code> in <code>SimpleNeuralNetwork</code> - what happens with very wide or very deep networks?</li>
<li><strong>Attention heads</strong>: Increase <code>n_heads</code> in <code>MultiHeadAttention</code> - do patterns change?</li>
<li><strong>Sequence length</strong>: Use longer sequences in attention examples - observe attention patterns</li>
</ol>
<p>These experiments will deepen your intuition about how neural networks transform data.</p>
</div>
</div>
<hr>
</section>
<section id="further-resources" class="level2">
<h2 class="anchored" data-anchor-id="further-resources">Further Resources</h2>
<section id="video-explanations-3blue1brown" class="level3">
<h3 class="anchored" data-anchor-id="video-explanations-3blue1brown">Video Explanations (3Blue1Brown)</h3>
<ol type="1">
<li><a href="https://www.youtube.com/watch?v=aircAruvnKk">Neural Networks</a> - Core concepts</li>
<li><a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">Gradient Descent</a> - How networks learn</li>
<li><a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U">Backpropagation</a> - How gradients flow</li>
<li><a href="https://www.youtube.com/watch?v=eMlx5fFNoYc">Attention &amp; Transformers</a> - Modern architecture</li>
</ol>
</section>
<section id="key-papers" class="level3">
<h3 class="anchored" data-anchor-id="key-papers">Key Papers</h3>
<ul>
<li>“Attention Is All You Need” (Vaswani et al., 2017) - The original transformer paper</li>
<li>“Deep Residual Learning” (He et al., 2015) - Residual connections</li>
<li>“Understanding Deep Learning Requires Rethinking Generalization” (Zhang et al., 2017)</li>
</ul>
</section>
<section id="next-steps-in-this-course" class="level3">
<h3 class="anchored" data-anchor-id="next-steps-in-this-course">Next Steps in This Course</h3>
<ul>
<li><strong>Week 2</strong>: Spatial-temporal attention for geospatial data</li>
<li><strong>Week 3</strong>: Vision Transformers adapted for satellite imagery</li>
<li><strong>Week 4</strong>: Pretraining strategies (masked autoencoders)</li>
</ul>
<hr>
<p><em>This explainer is designed to build progressive understanding. Each section assumes you’ve understood the previous ones. Take time to run the code, observe the outputs, and experiment!</em></p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kcaylor\.github\.io\/GEOG-288KC-geospatial-foundation-models");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Understanding Neural Networks: From Neurons to Transformers"</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "A Progressive Guide to Deep Learning Architecture"</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>This guide builds your understanding of neural networks from the ground up, starting with how individual neurons process data, scaling to vectorized layer operations, and culminating in transformer architectures with attention mechanisms.</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-width: 10</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-cap: "Learning Roadmap: Our Journey Through Neural Networks"</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart LR</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="in">    start["🎯 Start Here"] --&gt; part1["Part 1&lt;br/&gt;Single Neuron&lt;br/&gt;────&lt;br/&gt;Activation&lt;br/&gt;Functions"]</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="in">    part1 --&gt; part2["Part 2&lt;br/&gt;Layers&lt;br/&gt;────&lt;br/&gt;Vectorization&lt;br/&gt;&amp; Depth"]</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="in">    part2 --&gt; part3["Part 3&lt;br/&gt;Attention&lt;br/&gt;────&lt;br/&gt;Context-Aware&lt;br/&gt;Processing"]</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="in">    part3 --&gt; part4["Part 4&lt;br/&gt;Transformers&lt;br/&gt;────&lt;br/&gt;Complete&lt;br/&gt;Architecture"]</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="in">    part4 --&gt; end_goal["🎓 Goal Achieved&lt;br/&gt;────&lt;br/&gt;Understand GPT,&lt;br/&gt;BERT, GFMs"]</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="in">    style start fill:#e1f5ff</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="in">    style part1 fill:#e8f0ff</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="in">    style part2 fill:#f0e1ff</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="in">    style part3 fill:#ffe8e1</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="in">    style part4 fill:#ffe1e1</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="in">    style end_goal fill:#e1ffe1</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>**What you'll learn**:</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How neurons transform data with activation functions</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How vectorization makes processing efficient</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Why attention is revolutionary for sequence modeling</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How transformers combine these ideas into powerful models</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Resources</span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>For excellent visual explanations of neural networks, check out 3Blue1Brown's Neural Networks series:</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">But what is a neural network?</span><span class="co">](https://www.youtube.com/watch?v=aircAruvnKk)</span> - Foundation concepts</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Gradient descent, how neural networks learn</span><span class="co">](https://www.youtube.com/watch?v=IHZwWFHWa-w)</span> - Training process</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 1: The Individual Neuron</span></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a><span class="fu">### What Does a Neuron Do?</span></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>At its core, a neuron is a simple computational unit that:</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Takes multiple inputs</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Combines them with learned weights</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Adds a bias term</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Passes the result through an activation function</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-width: 8</span></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-cap: "Single Neuron Computation Flow"</span></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart LR</span></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph inputs["Inputs"]</span></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a><span class="in">        x1["x₁ = 1.5"]</span></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a><span class="in">        x2["x₂ = 2.0"]</span></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a><span class="in">        x3["x₃ = -0.5"]</span></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph weights["× Weights"]</span></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a><span class="in">        w1["w₁ = 0.5"]</span></span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a><span class="in">        w2["w₂ = -1.0"]</span></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a><span class="in">        w3["w₃ = 0.3"]</span></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph computation["Weighted Sum"]</span></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a><span class="in">        sum["Σ(xᵢ × wᵢ) + b"]</span></span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a><span class="in">        bias["+ bias (0.5)"]</span></span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph activation["Activation"]</span></span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a><span class="in">        z["z = -0.35"]</span></span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a><span class="in">        act["f(z)"]</span></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a><span class="in">        output["Output"]</span></span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a><span class="in">    x1 --&gt; w1</span></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a><span class="in">    x2 --&gt; w2</span></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a><span class="in">    x3 --&gt; w3</span></span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a><span class="in">    w1 --&gt; sum</span></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a><span class="in">    w2 --&gt; sum</span></span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a><span class="in">    w3 --&gt; sum</span></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a><span class="in">    bias --&gt; sum</span></span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a><span class="in">    sum --&gt; z</span></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a><span class="in">    z --&gt; act</span></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a><span class="in">    act --&gt; output</span></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a><span class="in">    style inputs fill:#e1f5ff</span></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a><span class="in">    style weights fill:#fff3e1</span></span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a><span class="in">    style computation fill:#f0e1ff</span></span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a><span class="in">    style activation fill:#e1ffe1</span></span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a>**Key insight**: A neuron is just a weighted sum followed by a non-linear function. That's it!</span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>Let's see this in action with code:</span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a><span class="co"># A single neuron processes inputs</span></span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> single_neuron(inputs, weights, bias, activation_fn):</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a><span class="co">    Simulate a single neuron's computation.</span></span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a><span class="co">        inputs: array of input values</span></span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a><span class="co">        weights: array of weights (same length as inputs)</span></span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a><span class="co">        bias: single bias value</span></span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a><span class="co">        activation_fn: function to apply to weighted sum</span></span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a><span class="co">        The neuron's output after activation</span></span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Weighted sum</span></span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.dot(inputs, weights) <span class="op">+</span> bias</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Apply activation function</span></span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> activation_fn(z)</span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output, z</span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a><span class="co"># Example inputs and parameters</span></span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.array([<span class="fl">1.5</span>, <span class="fl">2.0</span>, <span class="op">-</span><span class="fl">0.5</span>])</span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.3</span>])</span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a><span class="co"># Without activation (just linear combination)</span></span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a>output_linear, z <span class="op">=</span> single_neuron(inputs, weights, bias, <span class="kw">lambda</span> x: x)</span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input values: </span><span class="sc">{</span>inputs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weights: </span><span class="sc">{</span>weights<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bias: </span><span class="sc">{</span>bias<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Weighted sum (z): </span><span class="sc">{</span>z<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Linear output: </span><span class="sc">{</span>output_linear<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a>**What to notice:** The neuron computes a weighted sum of its inputs plus a bias. Without an activation function, this is just a linear transformation—not very powerful for learning complex patterns.</span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a><span class="fu">### Activation Functions: Adding Non-Linearity</span></span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>Activation functions introduce **non-linearity**, which is crucial for neural networks to learn complex patterns. Let's explore the most common ones:</span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a><span class="co"># Define common activation functions</span></span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""ReLU: Rectified Linear Unit"""</span></span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, x)</span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Sigmoid: Squashes values to (0, 1)"""</span></span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x):</span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Tanh: Squashes values to (-1, 1)"""</span></span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.tanh(x)</span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> leaky_relu(x, alpha<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Leaky ReLU: Like ReLU but allows small negative values"""</span></span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.where(x <span class="op">&gt;</span> <span class="dv">0</span>, x, alpha <span class="op">*</span> x)</span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize these functions</span></span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">10</span>))</span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a><span class="co"># ReLU</span></span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].plot(x, relu(x), <span class="st">'b-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'ReLU: max(0, x)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'Input (x)'</span>)</span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'Output'</span>)</span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a><span class="co"># Sigmoid</span></span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].plot(x, sigmoid(x), <span class="st">'g-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Sigmoid: 1/(1+e^(-x))'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Input (x)'</span>)</span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Output'</span>)</span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].axhline(y<span class="op">=</span><span class="fl">0.5</span>, color<span class="op">=</span><span class="st">'r'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a><span class="co"># Tanh</span></span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].plot(x, tanh(x), <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'Tanh: (e^x - e^(-x))/(e^x + e^(-x))'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_xlabel(<span class="st">'Input (x)'</span>)</span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_ylabel(<span class="st">'Output'</span>)</span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a><span class="co"># Leaky ReLU</span></span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].plot(x, leaky_relu(x), <span class="st">'m-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Leaky ReLU: max(0.01x, x)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_xlabel(<span class="st">'Input (x)'</span>)</span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_ylabel(<span class="st">'Output'</span>)</span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linewidth<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a>**What to notice:** </span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ReLU** is the most popular: simple, fast, and effective. It "turns off" negative values completely.</span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sigmoid** squashes values between 0 and 1, useful for probabilities.</span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Tanh** centers outputs around 0, often better than sigmoid for hidden layers.</span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Leaky ReLU** prevents "dead neurons" by allowing small negative values.</span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a><span class="fu">### Seeing the Effect of Activation Functions</span></span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a>Let's see how different activation functions transform our neuron's output:</span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the same inputs from before</span></span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.array([<span class="fl">1.5</span>, <span class="fl">2.0</span>, <span class="op">-</span><span class="fl">0.5</span>])</span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.3</span>])</span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute weighted sum</span></span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.dot(inputs, weights) <span class="op">+</span> bias</span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weighted sum (z): </span><span class="sc">{</span>z<span class="sc">:.3f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply different activations</span></span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a>activations <span class="op">=</span> {</span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Linear (no activation)'</span>: <span class="kw">lambda</span> x: x,</span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a>    <span class="st">'ReLU'</span>: relu,</span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Sigmoid'</span>: sigmoid,</span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Tanh'</span>: tanh,</span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Leaky ReLU'</span>: leaky_relu</span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, fn <span class="kw">in</span> activations.items():</span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> fn(z)</span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">:25s}</span><span class="ss">: </span><span class="sc">{</span>output<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a>**Why this matters:** The activation function dramatically changes the neuron's output. This non-linear transformation is what allows neural networks to learn complex, non-linear patterns in data.</span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 2: From Single Neurons to Layers</span></span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Power of Vectorization</span></span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a>Rather than computing neurons one at a time, we can process an entire layer simultaneously using matrix operations. This is both computationally efficient and conceptually elegant.</span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-width: 10</span></span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-cap: "From Single Neuron to Layer of Neurons"</span></span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart TB</span></span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph single["Single Neuron (Sequential)"]</span></span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a><span class="in">        direction LR</span></span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a><span class="in">        i1["Input&lt;br/&gt;Vector&lt;br/&gt;(3 dims)"] --&gt; n1["Neuron 1"]</span></span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a><span class="in">        i2["Input&lt;br/&gt;Vector&lt;br/&gt;(3 dims)"] --&gt; n2["Neuron 2"]</span></span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a><span class="in">        i3["Input&lt;br/&gt;Vector&lt;br/&gt;(3 dims)"] --&gt; n3["Neuron 3"]</span></span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a><span class="in">        n1 --&gt; o1["Output 1"]</span></span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a><span class="in">        n2 --&gt; o2["Output 2"]</span></span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a><span class="in">        n3 --&gt; o3["Output 3"]</span></span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph vectorized["Vectorized Layer (Parallel)"]</span></span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a><span class="in">        direction LR</span></span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a><span class="in">        iv["Input&lt;br/&gt;Vector&lt;br/&gt;(3 dims)"] --&gt; matrix["Weight Matrix&lt;br/&gt;(3 × 5)&lt;br/&gt;+ Bias&lt;br/&gt;+ Activation"]</span></span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a><span class="in">        matrix --&gt; ov["Output&lt;br/&gt;Vector&lt;br/&gt;(5 dims)"]</span></span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a><span class="in">    single -.-&gt;|"Matrix Operation"| vectorized</span></span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a><span class="in">    style single fill:#ffe1e1</span></span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a><span class="in">    style vectorized fill:#e1ffe1</span></span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a><span class="in">    style matrix fill:#fff3e1</span></span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a>**Key insight**: Instead of looping through neurons, one matrix multiplication processes all neurons simultaneously!</span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Resource</span></span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a>For understanding how layers work together, see 3Blue1Brown's <span class="co">[</span><span class="ot">Neural network intuitions</span><span class="co">](https://www.youtube.com/watch?v=aircAruvnKk&amp;t=420s)</span> (timestamp 7:00 onwards).</span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a><span class="co"># A layer is just multiple neurons working in parallel</span></span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralLayer:</span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A single layer of neurons with vectorized operations."""</span></span>
<span id="cb12-314"><a href="#cb12-314" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_inputs, n_neurons):</span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a><span class="co">        Initialize a layer.</span></span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a><span class="co">            n_inputs: number of input features</span></span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a><span class="co">            n_neurons: number of neurons in this layer</span></span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each neuron has n_inputs weights</span></span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape: (n_inputs, n_neurons)</span></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(n_inputs, n_neurons) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each neuron has one bias</span></span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape: (n_neurons,)</span></span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.biases <span class="op">=</span> np.zeros(n_neurons)</span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs, activation_fn<span class="op">=</span>relu):</span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a><span class="co">        Forward pass through the layer.</span></span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a><span class="co">            inputs: input array of shape (n_samples, n_inputs)</span></span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a><span class="co">            activation_fn: activation function to apply</span></span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a><span class="co">            outputs after activation, shape (n_samples, n_neurons)</span></span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Matrix multiplication: (n_samples, n_inputs) @ (n_inputs, n_neurons)</span></span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Result: (n_samples, n_neurons)</span></span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> np.dot(inputs, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.biases</span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply activation function element-wise</span></span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> activation_fn(z)</span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a layer with 3 inputs and 5 neurons</span></span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a>layer <span class="op">=</span> NeuralLayer(n_inputs<span class="op">=</span><span class="dv">3</span>, n_neurons<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a><span class="co"># Process a batch of 4 samples</span></span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a>batch_inputs <span class="op">=</span> np.random.randn(<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> layer.forward(batch_inputs, activation_fn<span class="op">=</span>relu)</span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>batch_inputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Weight matrix shape: </span><span class="sc">{</span>layer<span class="sc">.</span>weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>outputs<span class="sc">.</span>shape<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Sample input:"</span>)</span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(batch_inputs[<span class="dv">0</span>])</span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Corresponding output:"</span>)</span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outputs[<span class="dv">0</span>])</span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a>**What to notice:** </span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The weight matrix has shape <span class="in">`(n_inputs, n_neurons)`</span> - each column represents one neuron's weights.</span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>One matrix multiplication processes all neurons and all samples simultaneously.</span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output shape is <span class="in">`(n_samples, n_neurons)`</span> - each sample gets transformed into a vector of neuron activations.</span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a><span class="fu">### Building a Multi-Layer Network</span></span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a>Now let's stack multiple layers to create a deep neural network:</span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-width: 10</span></span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-cap: "Multi-Layer Neural Network Architecture"</span></span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart LR</span></span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph input["Input Layer"]</span></span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a><span class="in">        i1["x₁"]</span></span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a><span class="in">        i2["x₂"]</span></span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a><span class="in">        i3["x₃"]</span></span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph hidden1["Hidden Layer 1&lt;br/&gt;(8 neurons)"]</span></span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a><span class="in">        h11["🔵"]</span></span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a><span class="in">        h12["🔵"]</span></span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a><span class="in">        h13["🔵"]</span></span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a><span class="in">        h14["🔵"]</span></span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a><span class="in">        h15["🔵"]</span></span>
<span id="cb12-394"><a href="#cb12-394" aria-hidden="true" tabindex="-1"></a><span class="in">        h16["🔵"]</span></span>
<span id="cb12-395"><a href="#cb12-395" aria-hidden="true" tabindex="-1"></a><span class="in">        h17["🔵"]</span></span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a><span class="in">        h18["🔵"]</span></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph hidden2["Hidden Layer 2&lt;br/&gt;(5 neurons)"]</span></span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a><span class="in">        h21["🟢"]</span></span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a><span class="in">        h22["🟢"]</span></span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a><span class="in">        h23["🟢"]</span></span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a><span class="in">        h24["🟢"]</span></span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a><span class="in">        h25["🟢"]</span></span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph output["Output Layer&lt;br/&gt;(2 neurons)"]</span></span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a><span class="in">        o1["🔴"]</span></span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a><span class="in">        o2["🔴"]</span></span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a><span class="in">    i1 --&gt; h11 &amp; h12 &amp; h13 &amp; h14 &amp; h15 &amp; h16 &amp; h17 &amp; h18</span></span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a><span class="in">    i2 --&gt; h11 &amp; h12 &amp; h13 &amp; h14 &amp; h15 &amp; h16 &amp; h17 &amp; h18</span></span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a><span class="in">    i3 --&gt; h11 &amp; h12 &amp; h13 &amp; h14 &amp; h15 &amp; h16 &amp; h17 &amp; h18</span></span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a><span class="in">    h11 &amp; h12 &amp; h13 &amp; h14 &amp; h15 &amp; h16 &amp; h17 &amp; h18 --&gt; h21 &amp; h22 &amp; h23 &amp; h24 &amp; h25</span></span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a><span class="in">    h21 &amp; h22 &amp; h23 &amp; h24 &amp; h25 --&gt; o1 &amp; o2</span></span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a><span class="in">    style input fill:#e1f5ff</span></span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a><span class="in">    style hidden1 fill:#e8e1ff</span></span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a><span class="in">    style hidden2 fill:#e1ffe8</span></span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a><span class="in">    style output fill:#ffe1e1</span></span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a>**Key insight**: Data flows forward through the network, with each layer creating progressively more abstract representations.</span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a>Now let's implement this:</span>
<span id="cb12-429"><a href="#cb12-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNeuralNetwork:</span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A simple feedforward neural network."""</span></span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, layer_sizes):</span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a><span class="co">            layer_sizes: list of layer sizes, e.g., [3, 8, 5, 2]</span></span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a><span class="co">                        means 3 inputs, two hidden layers (8 and 5 neurons),</span></span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a><span class="co">                        and 2 output neurons</span></span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> []</span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(layer_sizes) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> NeuralLayer(layer_sizes[i], layer_sizes[i <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.layers.append(layer)</span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass through all layers."""</span></span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a>        activation <span class="op">=</span> inputs</span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Forward pass through network:"</span>)</span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>activation<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.layers):</span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use ReLU for hidden layers, linear for output</span></span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(<span class="va">self</span>.layers) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a>                activation <span class="op">=</span> layer.forward(activation, activation_fn<span class="op">=</span>relu)</span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a>                activation <span class="op">=</span> layer.forward(activation, activation_fn<span class="op">=</span><span class="kw">lambda</span> x: x)</span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"After layer </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>activation<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> activation</span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a network: 3 inputs -&gt; 8 hidden -&gt; 5 hidden -&gt; 2 outputs</span></span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> SimpleNeuralNetwork([<span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">5</span>, <span class="dv">2</span>])</span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a><span class="co"># Process a batch of data</span></span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> np.random.randn(<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> network.forward(batch)</span>
<span id="cb12-472"><a href="#cb12-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-473"><a href="#cb12-473" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Final output:</span><span class="ch">\n</span><span class="sc">{</span>output<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-476"><a href="#cb12-476" aria-hidden="true" tabindex="-1"></a>**What to notice:** </span>
<span id="cb12-477"><a href="#cb12-477" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each layer transforms the data: <span class="in">`(batch, n_in) -&gt; (batch, n_out)`</span></span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The output of one layer becomes the input to the next</span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This creates a series of increasingly abstract representations</span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualizing the Transformation</span></span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a>Let's see how data is transformed as it flows through the network:</span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simpler network for visualization</span></span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a>viz_network <span class="op">=</span> SimpleNeuralNetwork([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>])</span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a><span class="co"># Create some structured input data</span></span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, <span class="dv">100</span>)</span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.column_stack([np.cos(theta), np.sin(theta)])</span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a><span class="co"># Track activations at each layer</span></span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a>activations <span class="op">=</span> [inputs]</span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a>current <span class="op">=</span> inputs</span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, layer <span class="kw">in</span> <span class="bu">enumerate</span>(viz_network.layers):</span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(viz_network.layers) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a>        current <span class="op">=</span> layer.forward(current, activation_fn<span class="op">=</span>relu)</span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a>        current <span class="op">=</span> layer.forward(current, activation_fn<span class="op">=</span><span class="kw">lambda</span> x: x)</span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a>    activations.append(current)</span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the transformations</span></span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (ax, act) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(axes, activations)):</span>
<span id="cb12-510"><a href="#cb12-510" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb12-511"><a href="#cb12-511" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> <span class="st">"Input Space"</span></span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> i <span class="op">==</span> <span class="bu">len</span>(activations) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> <span class="st">"Output Space"</span></span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-515"><a href="#cb12-515" aria-hidden="true" tabindex="-1"></a>        title <span class="op">=</span> <span class="ss">f"Hidden Layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb12-516"><a href="#cb12-516" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot first two dimensions</span></span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a>    ax.scatter(act[:, <span class="dv">0</span>], act[:, <span class="dv">1</span>], c<span class="op">=</span>theta, cmap<span class="op">=</span><span class="st">'viridis'</span>, s<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a>    ax.set_title(title, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'Dimension 1'</span>)</span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Dimension 2'</span>)</span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'equal'</span>)</span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a>**What to notice:** Each layer progressively transforms the data, creating new representations. The network learns to map inputs to outputs through these transformations.</span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-531"><a href="#cb12-531" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-532"><a href="#cb12-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 3: Attention Mechanisms and Transformers</span></span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Limitation of Basic Neural Networks</span></span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a>Traditional feedforward networks process each input independently. But what if relationships *between* inputs matter? This is where **attention mechanisms** come in.</span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Resource</span></span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a>For an excellent visual explanation of attention and transformers, watch:</span>
<span id="cb12-542"><a href="#cb12-542" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Attention in transformers, visually explained</span><span class="co">](https://www.youtube.com/watch?v=eMlx5fFNoYc)</span> by 3Blue1Brown</span>
<span id="cb12-543"><a href="#cb12-543" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Visualizing Attention, a Transformer's Heart</span><span class="co">](https://www.youtube.com/watch?v=eMlx5fFNoYc&amp;t=0s)</span> (full explanation)</span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a><span class="fu">### What is Attention?</span></span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-548"><a href="#cb12-548" aria-hidden="true" tabindex="-1"></a>Attention allows the network to **focus on relevant parts of the input** when processing each element. Think of reading a sentence: to understand "it," you need to look back and find what "it" refers to.</span>
<span id="cb12-549"><a href="#cb12-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-width: 10</span></span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-cap: "Attention Mechanism: Query-Key-Value"</span></span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart TB</span></span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph input["Input Sequence"]</span></span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a><span class="in">        t1["Token 1"]</span></span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a><span class="in">        t2["Token 2"]</span></span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a><span class="in">        t3["Token 3"]</span></span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a><span class="in">        t4["Token 4"]</span></span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-563"><a href="#cb12-563" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph qkv["Linear Projections"]</span></span>
<span id="cb12-564"><a href="#cb12-564" aria-hidden="true" tabindex="-1"></a><span class="in">        direction LR</span></span>
<span id="cb12-565"><a href="#cb12-565" aria-hidden="true" tabindex="-1"></a><span class="in">        q["Query (Q)&lt;br/&gt;What I'm looking for"]</span></span>
<span id="cb12-566"><a href="#cb12-566" aria-hidden="true" tabindex="-1"></a><span class="in">        k["Key (K)&lt;br/&gt;What I offer"]</span></span>
<span id="cb12-567"><a href="#cb12-567" aria-hidden="true" tabindex="-1"></a><span class="in">        v["Value (V)&lt;br/&gt;What I return"]</span></span>
<span id="cb12-568"><a href="#cb12-568" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-569"><a href="#cb12-569" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-570"><a href="#cb12-570" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph attention["Attention Computation"]</span></span>
<span id="cb12-571"><a href="#cb12-571" aria-hidden="true" tabindex="-1"></a><span class="in">        similarity["Compute Similarity&lt;br/&gt;Q · Kᵀ"]</span></span>
<span id="cb12-572"><a href="#cb12-572" aria-hidden="true" tabindex="-1"></a><span class="in">        weights["Softmax&lt;br/&gt;(Attention Weights)"]</span></span>
<span id="cb12-573"><a href="#cb12-573" aria-hidden="true" tabindex="-1"></a><span class="in">        output["Weighted Sum&lt;br/&gt;Σ(weights × V)"]</span></span>
<span id="cb12-574"><a href="#cb12-574" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-575"><a href="#cb12-575" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-576"><a href="#cb12-576" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph result["Context-Aware Output"]</span></span>
<span id="cb12-577"><a href="#cb12-577" aria-hidden="true" tabindex="-1"></a><span class="in">        o1["Output 1&lt;br/&gt;(informed by all tokens)"]</span></span>
<span id="cb12-578"><a href="#cb12-578" aria-hidden="true" tabindex="-1"></a><span class="in">        o2["Output 2&lt;br/&gt;(informed by all tokens)"]</span></span>
<span id="cb12-579"><a href="#cb12-579" aria-hidden="true" tabindex="-1"></a><span class="in">        o3["Output 3&lt;br/&gt;(informed by all tokens)"]</span></span>
<span id="cb12-580"><a href="#cb12-580" aria-hidden="true" tabindex="-1"></a><span class="in">        o4["Output 4&lt;br/&gt;(informed by all tokens)"]</span></span>
<span id="cb12-581"><a href="#cb12-581" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-582"><a href="#cb12-582" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-583"><a href="#cb12-583" aria-hidden="true" tabindex="-1"></a><span class="in">    t1 &amp; t2 &amp; t3 &amp; t4 --&gt; qkv</span></span>
<span id="cb12-584"><a href="#cb12-584" aria-hidden="true" tabindex="-1"></a><span class="in">    qkv --&gt; similarity</span></span>
<span id="cb12-585"><a href="#cb12-585" aria-hidden="true" tabindex="-1"></a><span class="in">    similarity --&gt; weights</span></span>
<span id="cb12-586"><a href="#cb12-586" aria-hidden="true" tabindex="-1"></a><span class="in">    weights --&gt; output</span></span>
<span id="cb12-587"><a href="#cb12-587" aria-hidden="true" tabindex="-1"></a><span class="in">    output --&gt; o1 &amp; o2 &amp; o3 &amp; o4</span></span>
<span id="cb12-588"><a href="#cb12-588" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-589"><a href="#cb12-589" aria-hidden="true" tabindex="-1"></a><span class="in">    style input fill:#e1f5ff</span></span>
<span id="cb12-590"><a href="#cb12-590" aria-hidden="true" tabindex="-1"></a><span class="in">    style qkv fill:#fff3e1</span></span>
<span id="cb12-591"><a href="#cb12-591" aria-hidden="true" tabindex="-1"></a><span class="in">    style attention fill:#f0e1ff</span></span>
<span id="cb12-592"><a href="#cb12-592" aria-hidden="true" tabindex="-1"></a><span class="in">    style result fill:#e1ffe1</span></span>
<span id="cb12-593"><a href="#cb12-593" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-594"><a href="#cb12-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-595"><a href="#cb12-595" aria-hidden="true" tabindex="-1"></a>**Key insight**: Each token can "attend to" (look at) all other tokens, deciding which are most relevant for its own representation.</span>
<span id="cb12-596"><a href="#cb12-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-599"><a href="#cb12-599" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-600"><a href="#cb12-600" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_attention(query, keys, values):</span>
<span id="cb12-601"><a href="#cb12-601" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-602"><a href="#cb12-602" aria-hidden="true" tabindex="-1"></a><span class="co">    Simplified attention mechanism.</span></span>
<span id="cb12-603"><a href="#cb12-603" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb12-604"><a href="#cb12-604" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb12-605"><a href="#cb12-605" aria-hidden="true" tabindex="-1"></a><span class="co">        query: what we're looking for (n_queries, d_k)</span></span>
<span id="cb12-606"><a href="#cb12-606" aria-hidden="true" tabindex="-1"></a><span class="co">        keys: what we're comparing against (n_keys, d_k)</span></span>
<span id="cb12-607"><a href="#cb12-607" aria-hidden="true" tabindex="-1"></a><span class="co">        values: what we return (n_keys, d_v)</span></span>
<span id="cb12-608"><a href="#cb12-608" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb12-609"><a href="#cb12-609" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb12-610"><a href="#cb12-610" aria-hidden="true" tabindex="-1"></a><span class="co">        weighted combination of values</span></span>
<span id="cb12-611"><a href="#cb12-611" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-612"><a href="#cb12-612" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Compute similarity scores</span></span>
<span id="cb12-613"><a href="#cb12-613" aria-hidden="true" tabindex="-1"></a>    <span class="co"># How much does each key match the query?</span></span>
<span id="cb12-614"><a href="#cb12-614" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.dot(query, keys.T)  <span class="co"># (n_queries, n_keys)</span></span>
<span id="cb12-615"><a href="#cb12-615" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-616"><a href="#cb12-616" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Convert to attention weights (softmax)</span></span>
<span id="cb12-617"><a href="#cb12-617" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scale by sqrt of dimension for stability</span></span>
<span id="cb12-618"><a href="#cb12-618" aria-hidden="true" tabindex="-1"></a>    d_k <span class="op">=</span> keys.shape[<span class="dv">1</span>]</span>
<span id="cb12-619"><a href="#cb12-619" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> scores <span class="op">/</span> np.sqrt(d_k)</span>
<span id="cb12-620"><a href="#cb12-620" aria-hidden="true" tabindex="-1"></a>    attention_weights <span class="op">=</span> np.exp(scores) <span class="op">/</span> np.exp(scores).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-621"><a href="#cb12-621" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-622"><a href="#cb12-622" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Weighted sum of values</span></span>
<span id="cb12-623"><a href="#cb12-623" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> np.dot(attention_weights, values)  <span class="co"># (n_queries, d_v)</span></span>
<span id="cb12-624"><a href="#cb12-624" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-625"><a href="#cb12-625" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output, attention_weights</span>
<span id="cb12-626"><a href="#cb12-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-627"><a href="#cb12-627" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: A simple sequence</span></span>
<span id="cb12-628"><a href="#cb12-628" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's say we have 4 tokens (words), each represented by a 3D vector</span></span>
<span id="cb12-629"><a href="#cb12-629" aria-hidden="true" tabindex="-1"></a>sequence <span class="op">=</span> np.array([</span>
<span id="cb12-630"><a href="#cb12-630" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.5</span>],  <span class="co"># Token 0</span></span>
<span id="cb12-631"><a href="#cb12-631" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">0.2</span>],  <span class="co"># Token 1</span></span>
<span id="cb12-632"><a href="#cb12-632" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">1.0</span>],  <span class="co"># Token 2</span></span>
<span id="cb12-633"><a href="#cb12-633" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span>],  <span class="co"># Token 3</span></span>
<span id="cb12-634"><a href="#cb12-634" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb12-635"><a href="#cb12-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-636"><a href="#cb12-636" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's see what Token 2 attends to</span></span>
<span id="cb12-637"><a href="#cb12-637" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> sequence[<span class="dv">2</span>:<span class="dv">3</span>]  <span class="co"># Query is Token 2</span></span>
<span id="cb12-638"><a href="#cb12-638" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> sequence        <span class="co"># Keys are all tokens</span></span>
<span id="cb12-639"><a href="#cb12-639" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> sequence      <span class="co"># Values are all tokens (simplified)</span></span>
<span id="cb12-640"><a href="#cb12-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-641"><a href="#cb12-641" aria-hidden="true" tabindex="-1"></a>output, attention_weights <span class="op">=</span> simple_attention(query, keys, values)</span>
<span id="cb12-642"><a href="#cb12-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-643"><a href="#cb12-643" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention weights for Token 2:"</span>)</span>
<span id="cb12-644"><a href="#cb12-644" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, weight <span class="kw">in</span> <span class="bu">enumerate</span>(attention_weights[<span class="dv">0</span>]):</span>
<span id="cb12-645"><a href="#cb12-645" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Token </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>weight<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-646"><a href="#cb12-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-647"><a href="#cb12-647" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Original Token 2: </span><span class="sc">{</span>sequence[<span class="dv">2</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-648"><a href="#cb12-648" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"After attention:  </span><span class="sc">{</span>output[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-649"><a href="#cb12-649" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-650"><a href="#cb12-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-651"><a href="#cb12-651" aria-hidden="true" tabindex="-1"></a>**What to notice:** </span>
<span id="cb12-652"><a href="#cb12-652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The attention weights sum to 1.0</span>
<span id="cb12-653"><a href="#cb12-653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Token 2 attends most to itself, but also considers other tokens</span>
<span id="cb12-654"><a href="#cb12-654" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The output is a weighted combination - it's been "informed" by the context</span>
<span id="cb12-655"><a href="#cb12-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-656"><a href="#cb12-656" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualizing Attention Patterns</span></span>
<span id="cb12-657"><a href="#cb12-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-660"><a href="#cb12-660" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-661"><a href="#cb12-661" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute full attention matrix (all tokens attending to all tokens)</span></span>
<span id="cb12-662"><a href="#cb12-662" aria-hidden="true" tabindex="-1"></a>all_queries <span class="op">=</span> sequence</span>
<span id="cb12-663"><a href="#cb12-663" aria-hidden="true" tabindex="-1"></a>all_keys <span class="op">=</span> sequence</span>
<span id="cb12-664"><a href="#cb12-664" aria-hidden="true" tabindex="-1"></a>all_values <span class="op">=</span> sequence</span>
<span id="cb12-665"><a href="#cb12-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-666"><a href="#cb12-666" aria-hidden="true" tabindex="-1"></a>outputs, full_attention <span class="op">=</span> simple_attention(all_queries, all_keys, all_values)</span>
<span id="cb12-667"><a href="#cb12-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-668"><a href="#cb12-668" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the attention matrix</span></span>
<span id="cb12-669"><a href="#cb12-669" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb12-670"><a href="#cb12-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-671"><a href="#cb12-671" aria-hidden="true" tabindex="-1"></a><span class="co"># Attention matrix</span></span>
<span id="cb12-672"><a href="#cb12-672" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> ax1.imshow(full_attention, cmap<span class="op">=</span><span class="st">'viridis'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>)</span>
<span id="cb12-673"><a href="#cb12-673" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">'Attention Matrix</span><span class="ch">\n</span><span class="st">(Which tokens attend to which)'</span>, </span>
<span id="cb12-674"><a href="#cb12-674" aria-hidden="true" tabindex="-1"></a>              fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-675"><a href="#cb12-675" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">'Key Token'</span>)</span>
<span id="cb12-676"><a href="#cb12-676" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'Query Token'</span>)</span>
<span id="cb12-677"><a href="#cb12-677" aria-hidden="true" tabindex="-1"></a>ax1.set_xticks(<span class="bu">range</span>(<span class="dv">4</span>))</span>
<span id="cb12-678"><a href="#cb12-678" aria-hidden="true" tabindex="-1"></a>ax1.set_yticks(<span class="bu">range</span>(<span class="dv">4</span>))</span>
<span id="cb12-679"><a href="#cb12-679" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im, ax<span class="op">=</span>ax1, label<span class="op">=</span><span class="st">'Attention Weight'</span>)</span>
<span id="cb12-680"><a href="#cb12-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-681"><a href="#cb12-681" aria-hidden="true" tabindex="-1"></a><span class="co"># Add values to cells</span></span>
<span id="cb12-682"><a href="#cb12-682" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb12-683"><a href="#cb12-683" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb12-684"><a href="#cb12-684" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> ax1.text(j, i, <span class="ss">f'</span><span class="sc">{</span>full_attention[i, j]<span class="sc">:.2f}</span><span class="ss">'</span>,</span>
<span id="cb12-685"><a href="#cb12-685" aria-hidden="true" tabindex="-1"></a>                       ha<span class="op">=</span><span class="st">"center"</span>, va<span class="op">=</span><span class="st">"center"</span>, color<span class="op">=</span><span class="st">"white"</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-686"><a href="#cb12-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-687"><a href="#cb12-687" aria-hidden="true" tabindex="-1"></a><span class="co"># Show transformation</span></span>
<span id="cb12-688"><a href="#cb12-688" aria-hidden="true" tabindex="-1"></a>ax2.bar(<span class="bu">range</span>(<span class="dv">4</span>), sequence[:, <span class="dv">0</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'Original (dim 0)'</span>)</span>
<span id="cb12-689"><a href="#cb12-689" aria-hidden="true" tabindex="-1"></a>ax2.bar(<span class="bu">range</span>(<span class="dv">4</span>), outputs[:, <span class="dv">0</span>], alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'After attention (dim 0)'</span>)</span>
<span id="cb12-690"><a href="#cb12-690" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">'Token Representations</span><span class="ch">\n</span><span class="st">(First dimension)'</span>, </span>
<span id="cb12-691"><a href="#cb12-691" aria-hidden="true" tabindex="-1"></a>              fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-692"><a href="#cb12-692" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Token'</span>)</span>
<span id="cb12-693"><a href="#cb12-693" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'Value'</span>)</span>
<span id="cb12-694"><a href="#cb12-694" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb12-695"><a href="#cb12-695" aria-hidden="true" tabindex="-1"></a>ax2.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-696"><a href="#cb12-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-697"><a href="#cb12-697" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-698"><a href="#cb12-698" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-699"><a href="#cb12-699" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-700"><a href="#cb12-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-701"><a href="#cb12-701" aria-hidden="true" tabindex="-1"></a>**What to notice:** The attention matrix shows which tokens influence each other. Diagonal values are often high (tokens attend to themselves), but off-diagonal values capture contextual relationships.</span>
<span id="cb12-702"><a href="#cb12-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-703"><a href="#cb12-703" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-Head Attention</span></span>
<span id="cb12-704"><a href="#cb12-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-705"><a href="#cb12-705" aria-hidden="true" tabindex="-1"></a>Transformers use **multiple attention heads** to capture different types of relationships simultaneously.</span>
<span id="cb12-706"><a href="#cb12-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-709"><a href="#cb12-709" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb12-710"><a href="#cb12-710" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-width: 10</span></span>
<span id="cb12-711"><a href="#cb12-711" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-cap: "Multi-Head Attention Architecture"</span></span>
<span id="cb12-712"><a href="#cb12-712" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart TB</span></span>
<span id="cb12-713"><a href="#cb12-713" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph input_seq["Input Sequence (d_model = 512)"]</span></span>
<span id="cb12-714"><a href="#cb12-714" aria-hidden="true" tabindex="-1"></a><span class="in">        seq["Token 1, Token 2, ..., Token n"]</span></span>
<span id="cb12-715"><a href="#cb12-715" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-716"><a href="#cb12-716" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-717"><a href="#cb12-717" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph linear_proj["Linear Projections"]</span></span>
<span id="cb12-718"><a href="#cb12-718" aria-hidden="true" tabindex="-1"></a><span class="in">        wq["W_Q"]</span></span>
<span id="cb12-719"><a href="#cb12-719" aria-hidden="true" tabindex="-1"></a><span class="in">        wk["W_K"]</span></span>
<span id="cb12-720"><a href="#cb12-720" aria-hidden="true" tabindex="-1"></a><span class="in">        wv["W_V"]</span></span>
<span id="cb12-721"><a href="#cb12-721" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-722"><a href="#cb12-722" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-723"><a href="#cb12-723" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph heads["Split into Multiple Heads (e.g., 8 heads × 64 dims)"]</span></span>
<span id="cb12-724"><a href="#cb12-724" aria-hidden="true" tabindex="-1"></a><span class="in">        direction LR</span></span>
<span id="cb12-725"><a href="#cb12-725" aria-hidden="true" tabindex="-1"></a><span class="in">        head1["Head 1&lt;br/&gt;🔵&lt;br/&gt;Attends to&lt;br/&gt;Syntax"]</span></span>
<span id="cb12-726"><a href="#cb12-726" aria-hidden="true" tabindex="-1"></a><span class="in">        head2["Head 2&lt;br/&gt;🟢&lt;br/&gt;Attends to&lt;br/&gt;Semantics"]</span></span>
<span id="cb12-727"><a href="#cb12-727" aria-hidden="true" tabindex="-1"></a><span class="in">        head3["Head 3&lt;br/&gt;🟡&lt;br/&gt;Attends to&lt;br/&gt;Position"]</span></span>
<span id="cb12-728"><a href="#cb12-728" aria-hidden="true" tabindex="-1"></a><span class="in">        dots["..."]</span></span>
<span id="cb12-729"><a href="#cb12-729" aria-hidden="true" tabindex="-1"></a><span class="in">        head8["Head 8&lt;br/&gt;🔴&lt;br/&gt;Attends to&lt;br/&gt;Context"]</span></span>
<span id="cb12-730"><a href="#cb12-730" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-731"><a href="#cb12-731" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-732"><a href="#cb12-732" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph attention_ops["Parallel Attention Operations"]</span></span>
<span id="cb12-733"><a href="#cb12-733" aria-hidden="true" tabindex="-1"></a><span class="in">        att1["Attention&lt;br/&gt;Computation"]</span></span>
<span id="cb12-734"><a href="#cb12-734" aria-hidden="true" tabindex="-1"></a><span class="in">        att2["Attention&lt;br/&gt;Computation"]</span></span>
<span id="cb12-735"><a href="#cb12-735" aria-hidden="true" tabindex="-1"></a><span class="in">        att3["Attention&lt;br/&gt;Computation"]</span></span>
<span id="cb12-736"><a href="#cb12-736" aria-hidden="true" tabindex="-1"></a><span class="in">        att4["Attention&lt;br/&gt;Computation"]</span></span>
<span id="cb12-737"><a href="#cb12-737" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-738"><a href="#cb12-738" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-739"><a href="#cb12-739" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph concat["Concatenate Heads"]</span></span>
<span id="cb12-740"><a href="#cb12-740" aria-hidden="true" tabindex="-1"></a><span class="in">        combined["Combined Output&lt;br/&gt;(8 heads × 64 = 512 dims)"]</span></span>
<span id="cb12-741"><a href="#cb12-741" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-742"><a href="#cb12-742" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-743"><a href="#cb12-743" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph final["Final Projection"]</span></span>
<span id="cb12-744"><a href="#cb12-744" aria-hidden="true" tabindex="-1"></a><span class="in">        wo["W_O&lt;br/&gt;(512 × 512)"]</span></span>
<span id="cb12-745"><a href="#cb12-745" aria-hidden="true" tabindex="-1"></a><span class="in">        output["Context-Aware&lt;br/&gt;Representations"]</span></span>
<span id="cb12-746"><a href="#cb12-746" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-747"><a href="#cb12-747" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-748"><a href="#cb12-748" aria-hidden="true" tabindex="-1"></a><span class="in">    seq --&gt; linear_proj</span></span>
<span id="cb12-749"><a href="#cb12-749" aria-hidden="true" tabindex="-1"></a><span class="in">    linear_proj --&gt; heads</span></span>
<span id="cb12-750"><a href="#cb12-750" aria-hidden="true" tabindex="-1"></a><span class="in">    head1 --&gt; att1</span></span>
<span id="cb12-751"><a href="#cb12-751" aria-hidden="true" tabindex="-1"></a><span class="in">    head2 --&gt; att2</span></span>
<span id="cb12-752"><a href="#cb12-752" aria-hidden="true" tabindex="-1"></a><span class="in">    head3 --&gt; att3</span></span>
<span id="cb12-753"><a href="#cb12-753" aria-hidden="true" tabindex="-1"></a><span class="in">    head8 --&gt; att4</span></span>
<span id="cb12-754"><a href="#cb12-754" aria-hidden="true" tabindex="-1"></a><span class="in">    att1 &amp; att2 &amp; att3 &amp; att4 --&gt; combined</span></span>
<span id="cb12-755"><a href="#cb12-755" aria-hidden="true" tabindex="-1"></a><span class="in">    combined --&gt; wo</span></span>
<span id="cb12-756"><a href="#cb12-756" aria-hidden="true" tabindex="-1"></a><span class="in">    wo --&gt; output</span></span>
<span id="cb12-757"><a href="#cb12-757" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-758"><a href="#cb12-758" aria-hidden="true" tabindex="-1"></a><span class="in">    style input_seq fill:#e1f5ff</span></span>
<span id="cb12-759"><a href="#cb12-759" aria-hidden="true" tabindex="-1"></a><span class="in">    style linear_proj fill:#fff3e1</span></span>
<span id="cb12-760"><a href="#cb12-760" aria-hidden="true" tabindex="-1"></a><span class="in">    style heads fill:#f0e1ff</span></span>
<span id="cb12-761"><a href="#cb12-761" aria-hidden="true" tabindex="-1"></a><span class="in">    style attention_ops fill:#ffe1f0</span></span>
<span id="cb12-762"><a href="#cb12-762" aria-hidden="true" tabindex="-1"></a><span class="in">    style concat fill:#e1ffe8</span></span>
<span id="cb12-763"><a href="#cb12-763" aria-hidden="true" tabindex="-1"></a><span class="in">    style final fill:#e1ffe1</span></span>
<span id="cb12-764"><a href="#cb12-764" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-765"><a href="#cb12-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-766"><a href="#cb12-766" aria-hidden="true" tabindex="-1"></a>**Key insight**: Multiple heads can learn different attention patterns - some might focus on nearby words, others on distant relationships, enabling richer representations.</span>
<span id="cb12-767"><a href="#cb12-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-770"><a href="#cb12-770" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-771"><a href="#cb12-771" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention:</span>
<span id="cb12-772"><a href="#cb12-772" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Multi-head attention mechanism."""</span></span>
<span id="cb12-773"><a href="#cb12-773" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-774"><a href="#cb12-774" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, n_heads):</span>
<span id="cb12-775"><a href="#cb12-775" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-776"><a href="#cb12-776" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-777"><a href="#cb12-777" aria-hidden="true" tabindex="-1"></a><span class="co">            d_model: dimension of the model (e.g., 512)</span></span>
<span id="cb12-778"><a href="#cb12-778" aria-hidden="true" tabindex="-1"></a><span class="co">            n_heads: number of attention heads (e.g., 8)</span></span>
<span id="cb12-779"><a href="#cb12-779" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-780"><a href="#cb12-780" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_heads <span class="op">=</span> n_heads</span>
<span id="cb12-781"><a href="#cb12-781" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_model <span class="op">=</span> d_model</span>
<span id="cb12-782"><a href="#cb12-782" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.d_k <span class="op">=</span> d_model <span class="op">//</span> n_heads  <span class="co"># dimension per head</span></span>
<span id="cb12-783"><a href="#cb12-783" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-784"><a href="#cb12-784" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear projections for Q, K, V</span></span>
<span id="cb12-785"><a href="#cb12-785" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_q <span class="op">=</span> np.random.randn(d_model, d_model) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb12-786"><a href="#cb12-786" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_k <span class="op">=</span> np.random.randn(d_model, d_model) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb12-787"><a href="#cb12-787" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_v <span class="op">=</span> np.random.randn(d_model, d_model) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb12-788"><a href="#cb12-788" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_o <span class="op">=</span> np.random.randn(d_model, d_model) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb12-789"><a href="#cb12-789" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-790"><a href="#cb12-790" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> split_heads(<span class="va">self</span>, x):</span>
<span id="cb12-791"><a href="#cb12-791" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Split the last dimension into (n_heads, d_k)."""</span></span>
<span id="cb12-792"><a href="#cb12-792" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_len, d_model <span class="op">=</span> x.shape</span>
<span id="cb12-793"><a href="#cb12-793" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape to (batch_size, seq_len, n_heads, d_k)</span></span>
<span id="cb12-794"><a href="#cb12-794" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(batch_size, seq_len, <span class="va">self</span>.n_heads, <span class="va">self</span>.d_k)</span>
<span id="cb12-795"><a href="#cb12-795" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transpose to (batch_size, n_heads, seq_len, d_k)</span></span>
<span id="cb12-796"><a href="#cb12-796" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb12-797"><a href="#cb12-797" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-798"><a href="#cb12-798" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> attention(<span class="va">self</span>, q, k, v):</span>
<span id="cb12-799"><a href="#cb12-799" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Scaled dot-product attention."""</span></span>
<span id="cb12-800"><a href="#cb12-800" aria-hidden="true" tabindex="-1"></a>        d_k <span class="op">=</span> q.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb12-801"><a href="#cb12-801" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> np.matmul(q, k.transpose(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)) <span class="op">/</span> np.sqrt(d_k)</span>
<span id="cb12-802"><a href="#cb12-802" aria-hidden="true" tabindex="-1"></a>        attention_weights <span class="op">=</span> np.exp(scores) <span class="op">/</span> np.exp(scores).<span class="bu">sum</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-803"><a href="#cb12-803" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> np.matmul(attention_weights, v)</span>
<span id="cb12-804"><a href="#cb12-804" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention_weights</span>
<span id="cb12-805"><a href="#cb12-805" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-806"><a href="#cb12-806" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-807"><a href="#cb12-807" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-808"><a href="#cb12-808" aria-hidden="true" tabindex="-1"></a><span class="co">        Forward pass.</span></span>
<span id="cb12-809"><a href="#cb12-809" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb12-810"><a href="#cb12-810" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-811"><a href="#cb12-811" aria-hidden="true" tabindex="-1"></a><span class="co">            x: input tensor of shape (batch_size, seq_len, d_model)</span></span>
<span id="cb12-812"><a href="#cb12-812" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb12-813"><a href="#cb12-813" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb12-814"><a href="#cb12-814" aria-hidden="true" tabindex="-1"></a><span class="co">            output of shape (batch_size, seq_len, d_model)</span></span>
<span id="cb12-815"><a href="#cb12-815" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-816"><a href="#cb12-816" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_len, _ <span class="op">=</span> x.shape</span>
<span id="cb12-817"><a href="#cb12-817" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-818"><a href="#cb12-818" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Linear projections</span></span>
<span id="cb12-819"><a href="#cb12-819" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> np.dot(x, <span class="va">self</span>.W_q)</span>
<span id="cb12-820"><a href="#cb12-820" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> np.dot(x, <span class="va">self</span>.W_k)</span>
<span id="cb12-821"><a href="#cb12-821" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> np.dot(x, <span class="va">self</span>.W_v)</span>
<span id="cb12-822"><a href="#cb12-822" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-823"><a href="#cb12-823" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split into multiple heads</span></span>
<span id="cb12-824"><a href="#cb12-824" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.split_heads(q.reshape(batch_size, seq_len, <span class="va">self</span>.d_model))</span>
<span id="cb12-825"><a href="#cb12-825" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">self</span>.split_heads(k.reshape(batch_size, seq_len, <span class="va">self</span>.d_model))</span>
<span id="cb12-826"><a href="#cb12-826" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.split_heads(v.reshape(batch_size, seq_len, <span class="va">self</span>.d_model))</span>
<span id="cb12-827"><a href="#cb12-827" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-828"><a href="#cb12-828" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply attention</span></span>
<span id="cb12-829"><a href="#cb12-829" aria-hidden="true" tabindex="-1"></a>        attended, attention_weights <span class="op">=</span> <span class="va">self</span>.attention(q, k, v)</span>
<span id="cb12-830"><a href="#cb12-830" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-831"><a href="#cb12-831" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concatenate heads</span></span>
<span id="cb12-832"><a href="#cb12-832" aria-hidden="true" tabindex="-1"></a>        attended <span class="op">=</span> attended.transpose(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb12-833"><a href="#cb12-833" aria-hidden="true" tabindex="-1"></a>        concatenated <span class="op">=</span> attended.reshape(batch_size, seq_len, <span class="va">self</span>.d_model)</span>
<span id="cb12-834"><a href="#cb12-834" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-835"><a href="#cb12-835" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Final linear projection</span></span>
<span id="cb12-836"><a href="#cb12-836" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> np.dot(concatenated, <span class="va">self</span>.W_o)</span>
<span id="cb12-837"><a href="#cb12-837" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-838"><a href="#cb12-838" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attention_weights</span>
<span id="cb12-839"><a href="#cb12-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-840"><a href="#cb12-840" aria-hidden="true" tabindex="-1"></a><span class="co"># Create multi-head attention</span></span>
<span id="cb12-841"><a href="#cb12-841" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb12-842"><a href="#cb12-842" aria-hidden="true" tabindex="-1"></a>n_heads <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb12-843"><a href="#cb12-843" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadAttention(d_model, n_heads)</span>
<span id="cb12-844"><a href="#cb12-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-845"><a href="#cb12-845" aria-hidden="true" tabindex="-1"></a><span class="co"># Process a sequence</span></span>
<span id="cb12-846"><a href="#cb12-846" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb12-847"><a href="#cb12-847" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb12-848"><a href="#cb12-848" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randn(batch_size, seq_len, d_model)</span>
<span id="cb12-849"><a href="#cb12-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-850"><a href="#cb12-850" aria-hidden="true" tabindex="-1"></a>output, attention_weights <span class="op">=</span> mha.forward(x)</span>
<span id="cb12-851"><a href="#cb12-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-852"><a href="#cb12-852" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>x<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-853"><a href="#cb12-853" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-854"><a href="#cb12-854" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention weights shape: </span><span class="sc">{</span>attention_weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-855"><a href="#cb12-855" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Number of heads: </span><span class="sc">{</span>n_heads<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-856"><a href="#cb12-856" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Each head attends to sequence of length: </span><span class="sc">{</span>seq_len<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-857"><a href="#cb12-857" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-858"><a href="#cb12-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-859"><a href="#cb12-859" aria-hidden="true" tabindex="-1"></a>**What to notice:** </span>
<span id="cb12-860"><a href="#cb12-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-861"><a href="#cb12-861" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each head learns different attention patterns</span>
<span id="cb12-862"><a href="#cb12-862" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple heads capture multiple types of relationships simultaneously</span>
<span id="cb12-863"><a href="#cb12-863" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This is much more powerful than single-head attention</span>
<span id="cb12-864"><a href="#cb12-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-865"><a href="#cb12-865" aria-hidden="true" tabindex="-1"></a><span class="fu">### How Attention Changes Encodings</span></span>
<span id="cb12-866"><a href="#cb12-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-867"><a href="#cb12-867" aria-hidden="true" tabindex="-1"></a>Let's see how attention modifies representations compared to a simple feedforward layer:</span>
<span id="cb12-868"><a href="#cb12-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-871"><a href="#cb12-871" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb12-872"><a href="#cb12-872" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-width: 12</span></span>
<span id="cb12-873"><a href="#cb12-873" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-cap: "Feedforward vs Attention: The Key Difference"</span></span>
<span id="cb12-874"><a href="#cb12-874" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart TB</span></span>
<span id="cb12-875"><a href="#cb12-875" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph comparison["Processing Paradigms"]</span></span>
<span id="cb12-876"><a href="#cb12-876" aria-hidden="true" tabindex="-1"></a><span class="in">        direction LR</span></span>
<span id="cb12-877"><a href="#cb12-877" aria-hidden="true" tabindex="-1"></a><span class="in">        </span></span>
<span id="cb12-878"><a href="#cb12-878" aria-hidden="true" tabindex="-1"></a><span class="in">        subgraph ff["Feedforward Network"]</span></span>
<span id="cb12-879"><a href="#cb12-879" aria-hidden="true" tabindex="-1"></a><span class="in">            direction TB</span></span>
<span id="cb12-880"><a href="#cb12-880" aria-hidden="true" tabindex="-1"></a><span class="in">            ff_input["Token 1 | Token 2 | Token 3 | Token 4"]</span></span>
<span id="cb12-881"><a href="#cb12-881" aria-hidden="true" tabindex="-1"></a><span class="in">            ff_process["↓ Independent Processing ↓"]</span></span>
<span id="cb12-882"><a href="#cb12-882" aria-hidden="true" tabindex="-1"></a><span class="in">            ff_layer["Layer applies SAME transformation&lt;br/&gt;to each position separately"]</span></span>
<span id="cb12-883"><a href="#cb12-883" aria-hidden="true" tabindex="-1"></a><span class="in">            ff_output["Output 1 | Output 2 | Output 3 | Output 4"]</span></span>
<span id="cb12-884"><a href="#cb12-884" aria-hidden="true" tabindex="-1"></a><span class="in">            ff_note["❌ No communication between positions"]</span></span>
<span id="cb12-885"><a href="#cb12-885" aria-hidden="true" tabindex="-1"></a><span class="in">            </span></span>
<span id="cb12-886"><a href="#cb12-886" aria-hidden="true" tabindex="-1"></a><span class="in">            ff_input --&gt; ff_process</span></span>
<span id="cb12-887"><a href="#cb12-887" aria-hidden="true" tabindex="-1"></a><span class="in">            ff_process --&gt; ff_layer</span></span>
<span id="cb12-888"><a href="#cb12-888" aria-hidden="true" tabindex="-1"></a><span class="in">            ff_layer --&gt; ff_output</span></span>
<span id="cb12-889"><a href="#cb12-889" aria-hidden="true" tabindex="-1"></a><span class="in">            ff_output --&gt; ff_note</span></span>
<span id="cb12-890"><a href="#cb12-890" aria-hidden="true" tabindex="-1"></a><span class="in">        end</span></span>
<span id="cb12-891"><a href="#cb12-891" aria-hidden="true" tabindex="-1"></a><span class="in">        </span></span>
<span id="cb12-892"><a href="#cb12-892" aria-hidden="true" tabindex="-1"></a><span class="in">        subgraph attn["Attention Network"]</span></span>
<span id="cb12-893"><a href="#cb12-893" aria-hidden="true" tabindex="-1"></a><span class="in">            direction TB</span></span>
<span id="cb12-894"><a href="#cb12-894" aria-hidden="true" tabindex="-1"></a><span class="in">            attn_input["Token 1 | Token 2 | Token 3 | Token 4"]</span></span>
<span id="cb12-895"><a href="#cb12-895" aria-hidden="true" tabindex="-1"></a><span class="in">            attn_process["↓ Context-Aware Processing ↓"]</span></span>
<span id="cb12-896"><a href="#cb12-896" aria-hidden="true" tabindex="-1"></a><span class="in">            attn_layer["Each position looks at ALL positions&lt;br/&gt;Weighted combination based on relevance"]</span></span>
<span id="cb12-897"><a href="#cb12-897" aria-hidden="true" tabindex="-1"></a><span class="in">            attn_output["Output 1 | Output 2 | Output 3 | Output 4"]</span></span>
<span id="cb12-898"><a href="#cb12-898" aria-hidden="true" tabindex="-1"></a><span class="in">            attn_note["✅ Each output informed by full context"]</span></span>
<span id="cb12-899"><a href="#cb12-899" aria-hidden="true" tabindex="-1"></a><span class="in">            </span></span>
<span id="cb12-900"><a href="#cb12-900" aria-hidden="true" tabindex="-1"></a><span class="in">            attn_input --&gt; attn_process</span></span>
<span id="cb12-901"><a href="#cb12-901" aria-hidden="true" tabindex="-1"></a><span class="in">            attn_process --&gt; attn_layer</span></span>
<span id="cb12-902"><a href="#cb12-902" aria-hidden="true" tabindex="-1"></a><span class="in">            attn_layer --&gt; attn_output</span></span>
<span id="cb12-903"><a href="#cb12-903" aria-hidden="true" tabindex="-1"></a><span class="in">            attn_output --&gt; attn_note</span></span>
<span id="cb12-904"><a href="#cb12-904" aria-hidden="true" tabindex="-1"></a><span class="in">        end</span></span>
<span id="cb12-905"><a href="#cb12-905" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-906"><a href="#cb12-906" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-907"><a href="#cb12-907" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph examples["Real-World Example"]</span></span>
<span id="cb12-908"><a href="#cb12-908" aria-hidden="true" tabindex="-1"></a><span class="in">        direction LR</span></span>
<span id="cb12-909"><a href="#cb12-909" aria-hidden="true" tabindex="-1"></a><span class="in">        sentence["'The animal didn't cross the street because it was too tired'"]</span></span>
<span id="cb12-910"><a href="#cb12-910" aria-hidden="true" tabindex="-1"></a><span class="in">        ff_ex["Feedforward: 'it' processed in isolation&lt;br/&gt;❌ Can't determine if 'it' = animal or street"]</span></span>
<span id="cb12-911"><a href="#cb12-911" aria-hidden="true" tabindex="-1"></a><span class="in">        attn_ex["Attention: 'it' attends to all words&lt;br/&gt;✅ Learns 'it' → 'animal' (via context)"]</span></span>
<span id="cb12-912"><a href="#cb12-912" aria-hidden="true" tabindex="-1"></a><span class="in">        </span></span>
<span id="cb12-913"><a href="#cb12-913" aria-hidden="true" tabindex="-1"></a><span class="in">        sentence --&gt; ff_ex</span></span>
<span id="cb12-914"><a href="#cb12-914" aria-hidden="true" tabindex="-1"></a><span class="in">        sentence --&gt; attn_ex</span></span>
<span id="cb12-915"><a href="#cb12-915" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-916"><a href="#cb12-916" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-917"><a href="#cb12-917" aria-hidden="true" tabindex="-1"></a><span class="in">    style ff fill:#ffe1e1</span></span>
<span id="cb12-918"><a href="#cb12-918" aria-hidden="true" tabindex="-1"></a><span class="in">    style attn fill:#e1ffe1</span></span>
<span id="cb12-919"><a href="#cb12-919" aria-hidden="true" tabindex="-1"></a><span class="in">    style ff_note fill:#ffcccc</span></span>
<span id="cb12-920"><a href="#cb12-920" aria-hidden="true" tabindex="-1"></a><span class="in">    style attn_note fill:#ccffcc</span></span>
<span id="cb12-921"><a href="#cb12-921" aria-hidden="true" tabindex="-1"></a><span class="in">    style examples fill:#fff9e1</span></span>
<span id="cb12-922"><a href="#cb12-922" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-923"><a href="#cb12-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-924"><a href="#cb12-924" aria-hidden="true" tabindex="-1"></a>**Why this is revolutionary**: Attention allows the network to dynamically route information based on context, rather than applying fixed transformations. This is essential for understanding language, time series, and sequential data where relationships between elements matter.</span>
<span id="cb12-925"><a href="#cb12-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-928"><a href="#cb12-928" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-929"><a href="#cb12-929" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample sequence data</span></span>
<span id="cb12-930"><a href="#cb12-930" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb12-931"><a href="#cb12-931" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb12-932"><a href="#cb12-932" aria-hidden="true" tabindex="-1"></a>d_model <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb12-933"><a href="#cb12-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-934"><a href="#cb12-934" aria-hidden="true" tabindex="-1"></a><span class="co"># Input sequence</span></span>
<span id="cb12-935"><a href="#cb12-935" aria-hidden="true" tabindex="-1"></a>input_seq <span class="op">=</span> np.random.randn(<span class="dv">1</span>, seq_length, d_model)</span>
<span id="cb12-936"><a href="#cb12-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-937"><a href="#cb12-937" aria-hidden="true" tabindex="-1"></a><span class="co"># Option 1: Process with feedforward layer (no attention)</span></span>
<span id="cb12-938"><a href="#cb12-938" aria-hidden="true" tabindex="-1"></a>ff_layer <span class="op">=</span> NeuralLayer(d_model, d_model)</span>
<span id="cb12-939"><a href="#cb12-939" aria-hidden="true" tabindex="-1"></a>ff_output <span class="op">=</span> ff_layer.forward(input_seq.reshape(<span class="op">-</span><span class="dv">1</span>, d_model), activation_fn<span class="op">=</span>relu)</span>
<span id="cb12-940"><a href="#cb12-940" aria-hidden="true" tabindex="-1"></a>ff_output <span class="op">=</span> ff_output.reshape(<span class="dv">1</span>, seq_length, d_model)</span>
<span id="cb12-941"><a href="#cb12-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-942"><a href="#cb12-942" aria-hidden="true" tabindex="-1"></a><span class="co"># Option 2: Process with attention</span></span>
<span id="cb12-943"><a href="#cb12-943" aria-hidden="true" tabindex="-1"></a>mha <span class="op">=</span> MultiHeadAttention(d_model, n_heads<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-944"><a href="#cb12-944" aria-hidden="true" tabindex="-1"></a>attn_output, attn_weights <span class="op">=</span> mha.forward(input_seq)</span>
<span id="cb12-945"><a href="#cb12-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-946"><a href="#cb12-946" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the difference</span></span>
<span id="cb12-947"><a href="#cb12-947" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">4</span>))</span>
<span id="cb12-948"><a href="#cb12-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-949"><a href="#cb12-949" aria-hidden="true" tabindex="-1"></a><span class="co"># Input</span></span>
<span id="cb12-950"><a href="#cb12-950" aria-hidden="true" tabindex="-1"></a>im0 <span class="op">=</span> axes[<span class="dv">0</span>].imshow(input_seq[<span class="dv">0</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-951"><a href="#cb12-951" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">'Input Sequence'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-952"><a href="#cb12-952" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Position in Sequence'</span>)</span>
<span id="cb12-953"><a href="#cb12-953" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Feature Dimension'</span>)</span>
<span id="cb12-954"><a href="#cb12-954" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im0, ax<span class="op">=</span>axes[<span class="dv">0</span>])</span>
<span id="cb12-955"><a href="#cb12-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-956"><a href="#cb12-956" aria-hidden="true" tabindex="-1"></a><span class="co"># Feedforward output</span></span>
<span id="cb12-957"><a href="#cb12-957" aria-hidden="true" tabindex="-1"></a>im1 <span class="op">=</span> axes[<span class="dv">1</span>].imshow(ff_output[<span class="dv">0</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-958"><a href="#cb12-958" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">'After Feedforward Layer</span><span class="ch">\n</span><span class="st">(No attention)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-959"><a href="#cb12-959" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Position in Sequence'</span>)</span>
<span id="cb12-960"><a href="#cb12-960" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Feature Dimension'</span>)</span>
<span id="cb12-961"><a href="#cb12-961" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im1, ax<span class="op">=</span>axes[<span class="dv">1</span>])</span>
<span id="cb12-962"><a href="#cb12-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-963"><a href="#cb12-963" aria-hidden="true" tabindex="-1"></a><span class="co"># Attention output</span></span>
<span id="cb12-964"><a href="#cb12-964" aria-hidden="true" tabindex="-1"></a>im2 <span class="op">=</span> axes[<span class="dv">2</span>].imshow(attn_output[<span class="dv">0</span>].T, cmap<span class="op">=</span><span class="st">'RdBu'</span>, aspect<span class="op">=</span><span class="st">'auto'</span>, vmin<span class="op">=-</span><span class="dv">2</span>, vmax<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-965"><a href="#cb12-965" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">'After Multi-Head Attention</span><span class="ch">\n</span><span class="st">(Context-aware)'</span>, fontsize<span class="op">=</span><span class="dv">12</span>, fontweight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb12-966"><a href="#cb12-966" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_xlabel(<span class="st">'Position in Sequence'</span>)</span>
<span id="cb12-967"><a href="#cb12-967" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">'Feature Dimension'</span>)</span>
<span id="cb12-968"><a href="#cb12-968" aria-hidden="true" tabindex="-1"></a>plt.colorbar(im2, ax<span class="op">=</span>axes[<span class="dv">2</span>])</span>
<span id="cb12-969"><a href="#cb12-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-970"><a href="#cb12-970" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-971"><a href="#cb12-971" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-972"><a href="#cb12-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-973"><a href="#cb12-973" aria-hidden="true" tabindex="-1"></a><span class="co"># Show how representations relate to each other</span></span>
<span id="cb12-974"><a href="#cb12-974" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Cosine similarity between position encodings:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb12-975"><a href="#cb12-975" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Feedforward (independent processing):"</span>)</span>
<span id="cb12-976"><a href="#cb12-976" aria-hidden="true" tabindex="-1"></a>ff_norm <span class="op">=</span> ff_output[<span class="dv">0</span>] <span class="op">/</span> np.linalg.norm(ff_output[<span class="dv">0</span>], axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-977"><a href="#cb12-977" aria-hidden="true" tabindex="-1"></a>ff_similarity <span class="op">=</span> np.dot(ff_norm, ff_norm.T)</span>
<span id="cb12-978"><a href="#cb12-978" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average off-diagonal similarity: </span><span class="sc">{</span>(ff_similarity.<span class="bu">sum</span>() <span class="op">-</span> seq_length) <span class="op">/</span> (seq_length <span class="op">*</span> (seq_length <span class="op">-</span> <span class="dv">1</span>))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-979"><a href="#cb12-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-980"><a href="#cb12-980" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Attention (context-aware processing):"</span>)</span>
<span id="cb12-981"><a href="#cb12-981" aria-hidden="true" tabindex="-1"></a>attn_norm <span class="op">=</span> attn_output[<span class="dv">0</span>] <span class="op">/</span> np.linalg.norm(attn_output[<span class="dv">0</span>], axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-982"><a href="#cb12-982" aria-hidden="true" tabindex="-1"></a>attn_similarity <span class="op">=</span> np.dot(attn_norm, attn_norm.T)</span>
<span id="cb12-983"><a href="#cb12-983" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Average off-diagonal similarity: </span><span class="sc">{</span>(attn_similarity.<span class="bu">sum</span>() <span class="op">-</span> seq_length) <span class="op">/</span> (seq_length <span class="op">*</span> (seq_length <span class="op">-</span> <span class="dv">1</span>))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb12-984"><a href="#cb12-984" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-985"><a href="#cb12-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-986"><a href="#cb12-986" aria-hidden="true" tabindex="-1"></a>**Why this matters:** </span>
<span id="cb12-987"><a href="#cb12-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-988"><a href="#cb12-988" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feedforward layers** process each position independently - no position "knows" about others</span>
<span id="cb12-989"><a href="#cb12-989" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Attention layers** mix information across positions - each position is informed by context</span>
<span id="cb12-990"><a href="#cb12-990" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This context-awareness is crucial for sequential data like language, time series, or video</span>
<span id="cb12-991"><a href="#cb12-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-992"><a href="#cb12-992" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-993"><a href="#cb12-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-994"><a href="#cb12-994" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 4: Putting It All Together - The Transformer Block</span></span>
<span id="cb12-995"><a href="#cb12-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-996"><a href="#cb12-996" aria-hidden="true" tabindex="-1"></a>A complete transformer block combines attention with feedforward layers:</span>
<span id="cb12-997"><a href="#cb12-997" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1000"><a href="#cb12-1000" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb12-1001"><a href="#cb12-1001" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-width: 8</span></span>
<span id="cb12-1002"><a href="#cb12-1002" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-cap: "Complete Transformer Block Architecture"</span></span>
<span id="cb12-1003"><a href="#cb12-1003" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart TB</span></span>
<span id="cb12-1004"><a href="#cb12-1004" aria-hidden="true" tabindex="-1"></a><span class="in">    input["Input&lt;br/&gt;(Sequence of Embeddings)"]</span></span>
<span id="cb12-1005"><a href="#cb12-1005" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1006"><a href="#cb12-1006" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph mha_block["Multi-Head Attention Block"]</span></span>
<span id="cb12-1007"><a href="#cb12-1007" aria-hidden="true" tabindex="-1"></a><span class="in">        mha["Multi-Head&lt;br/&gt;Attention"]</span></span>
<span id="cb12-1008"><a href="#cb12-1008" aria-hidden="true" tabindex="-1"></a><span class="in">        add1["Add"]</span></span>
<span id="cb12-1009"><a href="#cb12-1009" aria-hidden="true" tabindex="-1"></a><span class="in">        norm1["Layer Norm"]</span></span>
<span id="cb12-1010"><a href="#cb12-1010" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-1011"><a href="#cb12-1011" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1012"><a href="#cb12-1012" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph ff_block["Feedforward Block"]</span></span>
<span id="cb12-1013"><a href="#cb12-1013" aria-hidden="true" tabindex="-1"></a><span class="in">        ff1["Linear&lt;br/&gt;(expand)"]</span></span>
<span id="cb12-1014"><a href="#cb12-1014" aria-hidden="true" tabindex="-1"></a><span class="in">        relu["ReLU"]</span></span>
<span id="cb12-1015"><a href="#cb12-1015" aria-hidden="true" tabindex="-1"></a><span class="in">        ff2["Linear&lt;br/&gt;(project)"]</span></span>
<span id="cb12-1016"><a href="#cb12-1016" aria-hidden="true" tabindex="-1"></a><span class="in">        add2["Add"]</span></span>
<span id="cb12-1017"><a href="#cb12-1017" aria-hidden="true" tabindex="-1"></a><span class="in">        norm2["Layer Norm"]</span></span>
<span id="cb12-1018"><a href="#cb12-1018" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-1019"><a href="#cb12-1019" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1020"><a href="#cb12-1020" aria-hidden="true" tabindex="-1"></a><span class="in">    output["Output&lt;br/&gt;(Enriched Representations)"]</span></span>
<span id="cb12-1021"><a href="#cb12-1021" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1022"><a href="#cb12-1022" aria-hidden="true" tabindex="-1"></a><span class="in">    input --&gt; mha</span></span>
<span id="cb12-1023"><a href="#cb12-1023" aria-hidden="true" tabindex="-1"></a><span class="in">    input -.-&gt;|"Residual&lt;br/&gt;Connection"| add1</span></span>
<span id="cb12-1024"><a href="#cb12-1024" aria-hidden="true" tabindex="-1"></a><span class="in">    mha --&gt; add1</span></span>
<span id="cb12-1025"><a href="#cb12-1025" aria-hidden="true" tabindex="-1"></a><span class="in">    add1 --&gt; norm1</span></span>
<span id="cb12-1026"><a href="#cb12-1026" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1027"><a href="#cb12-1027" aria-hidden="true" tabindex="-1"></a><span class="in">    norm1 --&gt; ff1</span></span>
<span id="cb12-1028"><a href="#cb12-1028" aria-hidden="true" tabindex="-1"></a><span class="in">    ff1 --&gt; relu</span></span>
<span id="cb12-1029"><a href="#cb12-1029" aria-hidden="true" tabindex="-1"></a><span class="in">    relu --&gt; ff2</span></span>
<span id="cb12-1030"><a href="#cb12-1030" aria-hidden="true" tabindex="-1"></a><span class="in">    norm1 -.-&gt;|"Residual&lt;br/&gt;Connection"| add2</span></span>
<span id="cb12-1031"><a href="#cb12-1031" aria-hidden="true" tabindex="-1"></a><span class="in">    ff2 --&gt; add2</span></span>
<span id="cb12-1032"><a href="#cb12-1032" aria-hidden="true" tabindex="-1"></a><span class="in">    add2 --&gt; norm2</span></span>
<span id="cb12-1033"><a href="#cb12-1033" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1034"><a href="#cb12-1034" aria-hidden="true" tabindex="-1"></a><span class="in">    norm2 --&gt; output</span></span>
<span id="cb12-1035"><a href="#cb12-1035" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1036"><a href="#cb12-1036" aria-hidden="true" tabindex="-1"></a><span class="in">    style input fill:#e1f5ff</span></span>
<span id="cb12-1037"><a href="#cb12-1037" aria-hidden="true" tabindex="-1"></a><span class="in">    style mha_block fill:#f0e1ff</span></span>
<span id="cb12-1038"><a href="#cb12-1038" aria-hidden="true" tabindex="-1"></a><span class="in">    style ff_block fill:#ffe8e1</span></span>
<span id="cb12-1039"><a href="#cb12-1039" aria-hidden="true" tabindex="-1"></a><span class="in">    style output fill:#e1ffe1</span></span>
<span id="cb12-1040"><a href="#cb12-1040" aria-hidden="true" tabindex="-1"></a><span class="in">    style mha fill:#d8b3ff</span></span>
<span id="cb12-1041"><a href="#cb12-1041" aria-hidden="true" tabindex="-1"></a><span class="in">    style add1 fill:#ffd8b3</span></span>
<span id="cb12-1042"><a href="#cb12-1042" aria-hidden="true" tabindex="-1"></a><span class="in">    style add2 fill:#ffd8b3</span></span>
<span id="cb12-1043"><a href="#cb12-1043" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-1044"><a href="#cb12-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1045"><a href="#cb12-1045" aria-hidden="true" tabindex="-1"></a>**Key components**:</span>
<span id="cb12-1046"><a href="#cb12-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1047"><a href="#cb12-1047" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Multi-Head Attention**: Mix information across positions (context)</span>
<span id="cb12-1048"><a href="#cb12-1048" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Residual Connection**: Add input back to help gradient flow</span>
<span id="cb12-1049"><a href="#cb12-1049" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Layer Normalization**: Stabilize training</span>
<span id="cb12-1050"><a href="#cb12-1050" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Feedforward Network**: Transform features independently</span>
<span id="cb12-1051"><a href="#cb12-1051" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Another Residual Connection**: More gradient flow</span>
<span id="cb12-1052"><a href="#cb12-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1053"><a href="#cb12-1053" aria-hidden="true" tabindex="-1"></a>This pattern repeats for each transformer layer in a model!</span>
<span id="cb12-1054"><a href="#cb12-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1057"><a href="#cb12-1057" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-1058"><a href="#cb12-1058" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock:</span>
<span id="cb12-1059"><a href="#cb12-1059" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A single transformer block with attention and feedforward."""</span></span>
<span id="cb12-1060"><a href="#cb12-1060" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-1061"><a href="#cb12-1061" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, d_model, n_heads, d_ff):</span>
<span id="cb12-1062"><a href="#cb12-1062" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-1063"><a href="#cb12-1063" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-1064"><a href="#cb12-1064" aria-hidden="true" tabindex="-1"></a><span class="co">            d_model: model dimension</span></span>
<span id="cb12-1065"><a href="#cb12-1065" aria-hidden="true" tabindex="-1"></a><span class="co">            n_heads: number of attention heads</span></span>
<span id="cb12-1066"><a href="#cb12-1066" aria-hidden="true" tabindex="-1"></a><span class="co">            d_ff: feedforward network dimension</span></span>
<span id="cb12-1067"><a href="#cb12-1067" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-1068"><a href="#cb12-1068" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(d_model, n_heads)</span>
<span id="cb12-1069"><a href="#cb12-1069" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff1 <span class="op">=</span> NeuralLayer(d_model, d_ff)</span>
<span id="cb12-1070"><a href="#cb12-1070" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff2 <span class="op">=</span> NeuralLayer(d_ff, d_model)</span>
<span id="cb12-1071"><a href="#cb12-1071" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-1072"><a href="#cb12-1072" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-1073"><a href="#cb12-1073" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb12-1074"><a href="#cb12-1074" aria-hidden="true" tabindex="-1"></a><span class="co">        Forward pass through transformer block.</span></span>
<span id="cb12-1075"><a href="#cb12-1075" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb12-1076"><a href="#cb12-1076" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb12-1077"><a href="#cb12-1077" aria-hidden="true" tabindex="-1"></a><span class="co">            x: input of shape (batch_size, seq_len, d_model)</span></span>
<span id="cb12-1078"><a href="#cb12-1078" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb12-1079"><a href="#cb12-1079" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb12-1080"><a href="#cb12-1080" aria-hidden="true" tabindex="-1"></a><span class="co">            output of same shape as input</span></span>
<span id="cb12-1081"><a href="#cb12-1081" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb12-1082"><a href="#cb12-1082" aria-hidden="true" tabindex="-1"></a>        batch_size, seq_len, d_model <span class="op">=</span> x.shape</span>
<span id="cb12-1083"><a href="#cb12-1083" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-1084"><a href="#cb12-1084" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 1: Multi-head attention</span></span>
<span id="cb12-1085"><a href="#cb12-1085" aria-hidden="true" tabindex="-1"></a>        attn_out, attn_weights <span class="op">=</span> <span class="va">self</span>.attention.forward(x)</span>
<span id="cb12-1086"><a href="#cb12-1086" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-1087"><a href="#cb12-1087" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 2: Add &amp; Norm (simplified - just add)</span></span>
<span id="cb12-1088"><a href="#cb12-1088" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> attn_out</span>
<span id="cb12-1089"><a href="#cb12-1089" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-1090"><a href="#cb12-1090" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 3: Feedforward network</span></span>
<span id="cb12-1091"><a href="#cb12-1091" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape for layer processing</span></span>
<span id="cb12-1092"><a href="#cb12-1092" aria-hidden="true" tabindex="-1"></a>        x_flat <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>, d_model)</span>
<span id="cb12-1093"><a href="#cb12-1093" aria-hidden="true" tabindex="-1"></a>        ff_out <span class="op">=</span> <span class="va">self</span>.ff1.forward(x_flat, activation_fn<span class="op">=</span>relu)</span>
<span id="cb12-1094"><a href="#cb12-1094" aria-hidden="true" tabindex="-1"></a>        ff_out <span class="op">=</span> <span class="va">self</span>.ff2.forward(ff_out, activation_fn<span class="op">=</span><span class="kw">lambda</span> x: x)</span>
<span id="cb12-1095"><a href="#cb12-1095" aria-hidden="true" tabindex="-1"></a>        ff_out <span class="op">=</span> ff_out.reshape(batch_size, seq_len, d_model)</span>
<span id="cb12-1096"><a href="#cb12-1096" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-1097"><a href="#cb12-1097" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Add &amp; Norm (simplified - just add)</span></span>
<span id="cb12-1098"><a href="#cb12-1098" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> x <span class="op">+</span> ff_out</span>
<span id="cb12-1099"><a href="#cb12-1099" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-1100"><a href="#cb12-1100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> output, attn_weights</span>
<span id="cb12-1101"><a href="#cb12-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1102"><a href="#cb12-1102" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and test a transformer block</span></span>
<span id="cb12-1103"><a href="#cb12-1103" aria-hidden="true" tabindex="-1"></a>transformer <span class="op">=</span> TransformerBlock(d_model<span class="op">=</span><span class="dv">8</span>, n_heads<span class="op">=</span><span class="dv">2</span>, d_ff<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb12-1104"><a href="#cb12-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1105"><a href="#cb12-1105" aria-hidden="true" tabindex="-1"></a><span class="co"># Process a sequence</span></span>
<span id="cb12-1106"><a href="#cb12-1106" aria-hidden="true" tabindex="-1"></a>input_seq <span class="op">=</span> np.random.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">8</span>)</span>
<span id="cb12-1107"><a href="#cb12-1107" aria-hidden="true" tabindex="-1"></a>output, attention <span class="op">=</span> transformer.forward(input_seq)</span>
<span id="cb12-1108"><a href="#cb12-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1109"><a href="#cb12-1109" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>input_seq<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-1110"><a href="#cb12-1110" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output shape: </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-1111"><a href="#cb12-1111" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Transformer block completed:"</span>)</span>
<span id="cb12-1112"><a href="#cb12-1112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  1. Multi-head attention (context mixing)"</span>)</span>
<span id="cb12-1113"><a href="#cb12-1113" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  2. Residual connection"</span>)</span>
<span id="cb12-1114"><a href="#cb12-1114" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  3. Feedforward network (feature transformation)"</span>)</span>
<span id="cb12-1115"><a href="#cb12-1115" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  4. Residual connection"</span>)</span>
<span id="cb12-1116"><a href="#cb12-1116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-1117"><a href="#cb12-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1118"><a href="#cb12-1118" aria-hidden="true" tabindex="-1"></a>**What to notice:** The transformer combines two key ideas:</span>
<span id="cb12-1119"><a href="#cb12-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1120"><a href="#cb12-1120" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Attention**: Mix information across sequence positions (capture context)</span>
<span id="cb12-1121"><a href="#cb12-1121" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Feedforward**: Transform features independently at each position (extract patterns)</span>
<span id="cb12-1122"><a href="#cb12-1122" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Residual connections**: Add the input back to help gradients flow</span>
<span id="cb12-1123"><a href="#cb12-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1124"><a href="#cb12-1124" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1125"><a href="#cb12-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1126"><a href="#cb12-1126" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary: The Neural Network Hierarchy</span></span>
<span id="cb12-1127"><a href="#cb12-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1128"><a href="#cb12-1128" aria-hidden="true" tabindex="-1"></a>Let's recap the progressive complexity:</span>
<span id="cb12-1129"><a href="#cb12-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1132"><a href="#cb12-1132" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb12-1133"><a href="#cb12-1133" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-width: 12</span></span>
<span id="cb12-1134"><a href="#cb12-1134" aria-hidden="true" tabindex="-1"></a><span class="in">%%| fig-cap: "The Complete Neural Network Hierarchy: From Neurons to Transformers"</span></span>
<span id="cb12-1135"><a href="#cb12-1135" aria-hidden="true" tabindex="-1"></a><span class="in">flowchart TD</span></span>
<span id="cb12-1136"><a href="#cb12-1136" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph level1["Level 1: Single Neuron"]</span></span>
<span id="cb12-1137"><a href="#cb12-1137" aria-hidden="true" tabindex="-1"></a><span class="in">        n1["Weighted Sum&lt;br/&gt;+ Activation&lt;br/&gt;────&lt;br/&gt;z = Σ(wᵢxᵢ) + b&lt;br/&gt;output = f(z)"]</span></span>
<span id="cb12-1138"><a href="#cb12-1138" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-1139"><a href="#cb12-1139" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1140"><a href="#cb12-1140" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph level2["Level 2: Neural Layer"]</span></span>
<span id="cb12-1141"><a href="#cb12-1141" aria-hidden="true" tabindex="-1"></a><span class="in">        layer["Vectorized Operations&lt;br/&gt;────&lt;br/&gt;Multiple neurons in parallel&lt;br/&gt;Matrix: (n_in, n_out)"]</span></span>
<span id="cb12-1142"><a href="#cb12-1142" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-1143"><a href="#cb12-1143" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1144"><a href="#cb12-1144" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph level3["Level 3: Deep Network"]</span></span>
<span id="cb12-1145"><a href="#cb12-1145" aria-hidden="true" tabindex="-1"></a><span class="in">        deep["Stacked Layers&lt;br/&gt;────&lt;br/&gt;Input → Hidden₁ → Hidden₂ → Output&lt;br/&gt;Hierarchical features"]</span></span>
<span id="cb12-1146"><a href="#cb12-1146" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-1147"><a href="#cb12-1147" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1148"><a href="#cb12-1148" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph level4a["Level 4a: Attention Mechanism"]</span></span>
<span id="cb12-1149"><a href="#cb12-1149" aria-hidden="true" tabindex="-1"></a><span class="in">        attention["Query-Key-Value&lt;br/&gt;────&lt;br/&gt;Context-aware weighting&lt;br/&gt;Tokens attend to each other"]</span></span>
<span id="cb12-1150"><a href="#cb12-1150" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-1151"><a href="#cb12-1151" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1152"><a href="#cb12-1152" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph level4b["Level 4b: Multi-Head Attention"]</span></span>
<span id="cb12-1153"><a href="#cb12-1153" aria-hidden="true" tabindex="-1"></a><span class="in">        multihead["Parallel Attention Heads&lt;br/&gt;────&lt;br/&gt;Multiple relationship types&lt;br/&gt;8 heads × 64 dims = 512 dims"]</span></span>
<span id="cb12-1154"><a href="#cb12-1154" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-1155"><a href="#cb12-1155" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1156"><a href="#cb12-1156" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph level5["Level 5: Transformer Block"]</span></span>
<span id="cb12-1157"><a href="#cb12-1157" aria-hidden="true" tabindex="-1"></a><span class="in">        transformer["Complete Architecture&lt;br/&gt;────&lt;br/&gt;Multi-Head Attention&lt;br/&gt;+ Residual Connection&lt;br/&gt;+ Feedforward Network&lt;br/&gt;+ Residual Connection"]</span></span>
<span id="cb12-1158"><a href="#cb12-1158" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-1159"><a href="#cb12-1159" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1160"><a href="#cb12-1160" aria-hidden="true" tabindex="-1"></a><span class="in">    subgraph level6["Level 6: Full Transformer"]</span></span>
<span id="cb12-1161"><a href="#cb12-1161" aria-hidden="true" tabindex="-1"></a><span class="in">        full["Stack of N Blocks&lt;br/&gt;────&lt;br/&gt;Block₁ → Block₂ → ... → Blockₙ&lt;br/&gt;+ Positional Encoding&lt;br/&gt;+ Output Head"]</span></span>
<span id="cb12-1162"><a href="#cb12-1162" aria-hidden="true" tabindex="-1"></a><span class="in">    end</span></span>
<span id="cb12-1163"><a href="#cb12-1163" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1164"><a href="#cb12-1164" aria-hidden="true" tabindex="-1"></a><span class="in">    level1 -.-&gt;|"Parallelize"| level2</span></span>
<span id="cb12-1165"><a href="#cb12-1165" aria-hidden="true" tabindex="-1"></a><span class="in">    level2 -.-&gt;|"Stack"| level3</span></span>
<span id="cb12-1166"><a href="#cb12-1166" aria-hidden="true" tabindex="-1"></a><span class="in">    level3 -.-&gt;|"Add Context&lt;br/&gt;Awareness"| level4a</span></span>
<span id="cb12-1167"><a href="#cb12-1167" aria-hidden="true" tabindex="-1"></a><span class="in">    level4a -.-&gt;|"Multiple&lt;br/&gt;Heads"| level4b</span></span>
<span id="cb12-1168"><a href="#cb12-1168" aria-hidden="true" tabindex="-1"></a><span class="in">    level4b -.-&gt;|"+ Feedforward&lt;br/&gt;+ Residuals"| level5</span></span>
<span id="cb12-1169"><a href="#cb12-1169" aria-hidden="true" tabindex="-1"></a><span class="in">    level5 -.-&gt;|"Stack&lt;br/&gt;Layers"| level6</span></span>
<span id="cb12-1170"><a href="#cb12-1170" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1171"><a href="#cb12-1171" aria-hidden="true" tabindex="-1"></a><span class="in">    capability1["Independent&lt;br/&gt;Processing"] -.-&gt; level1</span></span>
<span id="cb12-1172"><a href="#cb12-1172" aria-hidden="true" tabindex="-1"></a><span class="in">    capability2["Batch&lt;br/&gt;Processing"] -.-&gt; level2</span></span>
<span id="cb12-1173"><a href="#cb12-1173" aria-hidden="true" tabindex="-1"></a><span class="in">    capability3["Hierarchical&lt;br/&gt;Features"] -.-&gt; level3</span></span>
<span id="cb12-1174"><a href="#cb12-1174" aria-hidden="true" tabindex="-1"></a><span class="in">    capability4["Sequential&lt;br/&gt;Dependencies"] -.-&gt; level4a</span></span>
<span id="cb12-1175"><a href="#cb12-1175" aria-hidden="true" tabindex="-1"></a><span class="in">    capability5["Rich&lt;br/&gt;Relationships"] -.-&gt; level4b</span></span>
<span id="cb12-1176"><a href="#cb12-1176" aria-hidden="true" tabindex="-1"></a><span class="in">    capability6["Stable Deep&lt;br/&gt;Learning"] -.-&gt; level5</span></span>
<span id="cb12-1177"><a href="#cb12-1177" aria-hidden="true" tabindex="-1"></a><span class="in">    capability7["Complex&lt;br/&gt;Understanding"] -.-&gt; level6</span></span>
<span id="cb12-1178"><a href="#cb12-1178" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1179"><a href="#cb12-1179" aria-hidden="true" tabindex="-1"></a><span class="in">    style level1 fill:#e1f5ff</span></span>
<span id="cb12-1180"><a href="#cb12-1180" aria-hidden="true" tabindex="-1"></a><span class="in">    style level2 fill:#e8f0ff</span></span>
<span id="cb12-1181"><a href="#cb12-1181" aria-hidden="true" tabindex="-1"></a><span class="in">    style level3 fill:#e8e5ff</span></span>
<span id="cb12-1182"><a href="#cb12-1182" aria-hidden="true" tabindex="-1"></a><span class="in">    style level4a fill:#f0e1ff</span></span>
<span id="cb12-1183"><a href="#cb12-1183" aria-hidden="true" tabindex="-1"></a><span class="in">    style level4b fill:#f8e1ff</span></span>
<span id="cb12-1184"><a href="#cb12-1184" aria-hidden="true" tabindex="-1"></a><span class="in">    style level5 fill:#ffe1f0</span></span>
<span id="cb12-1185"><a href="#cb12-1185" aria-hidden="true" tabindex="-1"></a><span class="in">    style level6 fill:#ffe1e1</span></span>
<span id="cb12-1186"><a href="#cb12-1186" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb12-1187"><a href="#cb12-1187" aria-hidden="true" tabindex="-1"></a><span class="in">    style capability1 fill:#fff9e1</span></span>
<span id="cb12-1188"><a href="#cb12-1188" aria-hidden="true" tabindex="-1"></a><span class="in">    style capability2 fill:#fff9e1</span></span>
<span id="cb12-1189"><a href="#cb12-1189" aria-hidden="true" tabindex="-1"></a><span class="in">    style capability3 fill:#fff9e1</span></span>
<span id="cb12-1190"><a href="#cb12-1190" aria-hidden="true" tabindex="-1"></a><span class="in">    style capability4 fill:#fff9e1</span></span>
<span id="cb12-1191"><a href="#cb12-1191" aria-hidden="true" tabindex="-1"></a><span class="in">    style capability5 fill:#fff9e1</span></span>
<span id="cb12-1192"><a href="#cb12-1192" aria-hidden="true" tabindex="-1"></a><span class="in">    style capability6 fill:#fff9e1</span></span>
<span id="cb12-1193"><a href="#cb12-1193" aria-hidden="true" tabindex="-1"></a><span class="in">    style capability7 fill:#fff9e1</span></span>
<span id="cb12-1194"><a href="#cb12-1194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-1195"><a href="#cb12-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1196"><a href="#cb12-1196" aria-hidden="true" tabindex="-1"></a><span class="fu">### Level 1: Single Neuron</span></span>
<span id="cb12-1197"><a href="#cb12-1197" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computes weighted sum of inputs</span>
<span id="cb12-1198"><a href="#cb12-1198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Applies activation function (e.g., ReLU)</span>
<span id="cb12-1199"><a href="#cb12-1199" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transforms: scalar inputs → scalar output</span>
<span id="cb12-1200"><a href="#cb12-1200" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key insight**: Non-linearity enables learning complex patterns</span>
<span id="cb12-1201"><a href="#cb12-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1202"><a href="#cb12-1202" aria-hidden="true" tabindex="-1"></a><span class="fu">### Level 2: Layer of Neurons</span></span>
<span id="cb12-1203"><a href="#cb12-1203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multiple neurons computed in parallel (vectorized)</span>
<span id="cb12-1204"><a href="#cb12-1204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Matrix multiplication: efficient batch processing</span>
<span id="cb12-1205"><a href="#cb12-1205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transforms: vector input → vector output</span>
<span id="cb12-1206"><a href="#cb12-1206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key insight**: Multiple features extracted simultaneously</span>
<span id="cb12-1207"><a href="#cb12-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1208"><a href="#cb12-1208" aria-hidden="true" tabindex="-1"></a><span class="fu">### Level 3: Multi-Layer Network</span></span>
<span id="cb12-1209"><a href="#cb12-1209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stack layers to create deep representations</span>
<span id="cb12-1210"><a href="#cb12-1210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each layer builds on previous abstractions</span>
<span id="cb12-1211"><a href="#cb12-1211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transforms: input space → hidden spaces → output space</span>
<span id="cb12-1212"><a href="#cb12-1212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key insight**: Depth creates hierarchical features</span>
<span id="cb12-1213"><a href="#cb12-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1214"><a href="#cb12-1214" aria-hidden="true" tabindex="-1"></a><span class="fu">### Level 4: Attention &amp; Transformers</span></span>
<span id="cb12-1215"><a href="#cb12-1215" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Attention: dynamically weight inputs based on context</span>
<span id="cb12-1216"><a href="#cb12-1216" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-head: capture multiple relationship types</span>
<span id="cb12-1217"><a href="#cb12-1217" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transformer: attention + feedforward with residual connections</span>
<span id="cb12-1218"><a href="#cb12-1218" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key insight**: Context-aware processing for sequential data</span>
<span id="cb12-1219"><a href="#cb12-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1220"><a href="#cb12-1220" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb12-1221"><a href="#cb12-1221" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Key Difference</span></span>
<span id="cb12-1222"><a href="#cb12-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1223"><a href="#cb12-1223" aria-hidden="true" tabindex="-1"></a>**Traditional Neural Networks**: Process each input independently</span>
<span id="cb12-1224"><a href="#cb12-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1225"><a href="#cb12-1225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good for: images, tabular data where position doesn't matter</span>
<span id="cb12-1226"><a href="#cb12-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1227"><a href="#cb12-1227" aria-hidden="true" tabindex="-1"></a>**Transformers with Attention**: Process inputs in context of each other</span>
<span id="cb12-1228"><a href="#cb12-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1229"><a href="#cb12-1229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good for: language, time series, any sequential data</span>
<span id="cb12-1230"><a href="#cb12-1230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Revolution: Enables models like GPT, BERT, and modern GFMs</span>
<span id="cb12-1231"><a href="#cb12-1231" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1232"><a href="#cb12-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1233"><a href="#cb12-1233" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1234"><a href="#cb12-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1235"><a href="#cb12-1235" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interactive Exploration</span></span>
<span id="cb12-1236"><a href="#cb12-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1237"><a href="#cb12-1237" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb12-1238"><a href="#cb12-1238" aria-hidden="true" tabindex="-1"></a><span class="fu">## Try This Yourself</span></span>
<span id="cb12-1239"><a href="#cb12-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1240"><a href="#cb12-1240" aria-hidden="true" tabindex="-1"></a>Experiment with the code above:</span>
<span id="cb12-1241"><a href="#cb12-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1242"><a href="#cb12-1242" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Activation functions**: Change the activation in <span class="in">`single_neuron()`</span> and observe output changes</span>
<span id="cb12-1243"><a href="#cb12-1243" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Layer sizes**: Modify <span class="in">`layer_sizes`</span> in <span class="in">`SimpleNeuralNetwork`</span> - what happens with very wide or very deep networks?</span>
<span id="cb12-1244"><a href="#cb12-1244" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Attention heads**: Increase <span class="in">`n_heads`</span> in <span class="in">`MultiHeadAttention`</span> - do patterns change?</span>
<span id="cb12-1245"><a href="#cb12-1245" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Sequence length**: Use longer sequences in attention examples - observe attention patterns</span>
<span id="cb12-1246"><a href="#cb12-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1247"><a href="#cb12-1247" aria-hidden="true" tabindex="-1"></a>These experiments will deepen your intuition about how neural networks transform data.</span>
<span id="cb12-1248"><a href="#cb12-1248" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-1249"><a href="#cb12-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1250"><a href="#cb12-1250" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1251"><a href="#cb12-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1252"><a href="#cb12-1252" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Resources</span></span>
<span id="cb12-1253"><a href="#cb12-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1254"><a href="#cb12-1254" aria-hidden="true" tabindex="-1"></a><span class="fu">### Video Explanations (3Blue1Brown)</span></span>
<span id="cb12-1255"><a href="#cb12-1255" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span><span class="co">[</span><span class="ot">Neural Networks</span><span class="co">](https://www.youtube.com/watch?v=aircAruvnKk)</span> - Core concepts</span>
<span id="cb12-1256"><a href="#cb12-1256" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="co">[</span><span class="ot">Gradient Descent</span><span class="co">](https://www.youtube.com/watch?v=IHZwWFHWa-w)</span> - How networks learn</span>
<span id="cb12-1257"><a href="#cb12-1257" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span><span class="co">[</span><span class="ot">Backpropagation</span><span class="co">](https://www.youtube.com/watch?v=Ilg3gGewQ5U)</span> - How gradients flow</span>
<span id="cb12-1258"><a href="#cb12-1258" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span><span class="co">[</span><span class="ot">Attention &amp; Transformers</span><span class="co">](https://www.youtube.com/watch?v=eMlx5fFNoYc)</span> - Modern architecture</span>
<span id="cb12-1259"><a href="#cb12-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1260"><a href="#cb12-1260" aria-hidden="true" tabindex="-1"></a><span class="fu">### Key Papers</span></span>
<span id="cb12-1261"><a href="#cb12-1261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Attention Is All You Need" (Vaswani et al., 2017) - The original transformer paper</span>
<span id="cb12-1262"><a href="#cb12-1262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Deep Residual Learning" (He et al., 2015) - Residual connections</span>
<span id="cb12-1263"><a href="#cb12-1263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>"Understanding Deep Learning Requires Rethinking Generalization" (Zhang et al., 2017)</span>
<span id="cb12-1264"><a href="#cb12-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1265"><a href="#cb12-1265" aria-hidden="true" tabindex="-1"></a><span class="fu">### Next Steps in This Course</span></span>
<span id="cb12-1266"><a href="#cb12-1266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Week 2**: Spatial-temporal attention for geospatial data</span>
<span id="cb12-1267"><a href="#cb12-1267" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Week 3**: Vision Transformers adapted for satellite imagery</span>
<span id="cb12-1268"><a href="#cb12-1268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Week 4**: Pretraining strategies (masked autoencoders)</span>
<span id="cb12-1269"><a href="#cb12-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1270"><a href="#cb12-1270" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb12-1271"><a href="#cb12-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-1272"><a href="#cb12-1272" aria-hidden="true" tabindex="-1"></a>*This explainer is designed to build progressive understanding. Each section assumes you've understood the previous ones. Take time to run the code, observe the outputs, and experiment!*</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/geog-logo.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Department of Geography logo</figcaption>
</figure>
</div>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is built with <a href="https://github.com/kcaylor/GEOG-288KC-geospatial-foundation-models"><i class="fa-brands fa-github" title="the github octocat logo" aria-label="github"></i></a> and <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>