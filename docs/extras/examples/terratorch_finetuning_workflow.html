<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Getting started with fine tuning with TerraTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">GEOG 288KC</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">üè† home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Syllabus.html"> 
<span class="menu-text">üìã syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-weekly-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üíª weekly sessions</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-weekly-sessions">    
        <li>
    <a class="dropdown-item" href="../../chapters/c01-geospatial-data-foundations.html">
 <span class="dropdown-text">Week 1 - üöÄ Core Tools and Data Access</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c02-spatial-temporal-attention-mechanisms.html">
 <span class="dropdown-text">Week 2 - ‚ö° Rapid Remote Sensing Preprocessing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c03a-terratorch-foundations.html">
 <span class="dropdown-text">Week 3a - üåç TerraTorch Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c03-complete-gfm-architecture.html">
 <span class="dropdown-text">Week 3b - ü§ñ Machine Learning on Remote Sensing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c04-pretraining-implementation.html">
 <span class="dropdown-text">Week 4 - üèóÔ∏è Foundation Models in Practice</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c05-training-loop-optimization.html">
 <span class="dropdown-text">Week 5 - üîß Fine-Tuning &amp; Transfer Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c06-model-evaluation-analysis.html">
 <span class="dropdown-text">Week 6 - ‚è∞ Spatiotemporal Modeling &amp; Projects</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/terratorch_finetuning_workflow.html">
 <span class="dropdown-text">Week 7 - ‚ö° TerraTorch + Lightning Fine-Tuning Workflows</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-cheatsheets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üëÄ cheatsheets</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-cheatsheets">    
        <li>
    <a class="dropdown-item" href="../../cheatsheets.html">
 <span class="dropdown-text">üìã All Cheatsheets</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">‚ö° Quick Starts</li>
        <li>
    <a class="dropdown-item" href="../../extras/cheatsheets/week01_imports.html">
 <span class="dropdown-text">Week 01: Import Guide</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/cheatsheets/dataset_organization_terratorch.html">
 <span class="dropdown-text">Dataset Organization for Fine-Tuning</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-explainers" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üß© explainers</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-explainers">    
        <li class="dropdown-header">1Ô∏è‚É£ Week 1</li>
        <li>
    <a class="dropdown-item" href="../../extras/ai-ml-dl-fm-hierarchy.html">
 <span class="dropdown-text">ü§ñ AI/ML/DL/FM Hierarchy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-foundation-model-predictions-standalone.html">
 <span class="dropdown-text">üéØ GFM Predictions (Standalone)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-prediction-hierarchy.html">
 <span class="dropdown-text">‚úÖ Geospatial Task/Prediction Types</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/neural_networks_explainer.html">
 <span class="dropdown-text">üß† Neural Networks: Neurons to Transformers</span></a>
  </li>  
        <li class="dropdown-header">2Ô∏è‚É£ Week 2</li>
        <li>
    <a class="dropdown-item" href="../../chapters/c00a-foundation_model_architectures.html">
 <span class="dropdown-text">üèóÔ∏è Foundation Model Architectures</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c00b-introduction-to-deeplearning-architecture.html">
 <span class="dropdown-text">üéì Introduction to Deep Learning Architecture</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-extras" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üìñ extras</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-extras">    
        <li class="dropdown-header">üéØ Practical Examples</li>
        <li>
    <a class="dropdown-item" href="../../extras/examples/normalization_comparison.html">
 <span class="dropdown-text">Normalization Comparison</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/resnet.html">
 <span class="dropdown-text">ResNet Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/segmentation_finetuning.html">
 <span class="dropdown-text">Segmentation Fine-Tuning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/Terramind_EuroSAT.html">
 <span class="dropdown-text">TerraMind EuroSAT Classification</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/terratorch_finetuning_workflow.html">
 <span class="dropdown-text">TerraTorch Fine-Tuning Workflow</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/text_encoder.html">
 <span class="dropdown-text">Text Encoder</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/tiling-and-patches.html">
 <span class="dropdown-text">Tiling and Patches</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/terratorch_workflows.html">
 <span class="dropdown-text">TerraTorch Workflows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/resources/course_resources.html">
 <span class="dropdown-text">üìö Reference Materials</span></a>
  </li>  
        <li class="dropdown-header">üìÅ Project Templates</li>
        <li>
    <a class="dropdown-item" href="../../extras/projects/project-proposal-template.html">
 <span class="dropdown-text">Project Proposal Template</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/projects/mvp-template.html">
 <span class="dropdown-text">Project Results Template</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gfms-from-scratch/gfms-from-scratch.github.io" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Getting started with fine tuning with TerraTorch</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#step-0-setup" id="toc-step-0-setup" class="nav-link active" data-scroll-target="#step-0-setup">Step 0: Setup</a>
  <ul class="collapse">
  <li><a href="#huggingface-cache" id="toc-huggingface-cache" class="nav-link" data-scroll-target="#huggingface-cache">HuggingFace cache</a></li>
  </ul></li>
  <li><a href="#step-1-get-your-data-in-the-right-format-for-terratorch" id="toc-step-1-get-your-data-in-the-right-format-for-terratorch" class="nav-link" data-scroll-target="#step-1-get-your-data-in-the-right-format-for-terratorch">Step 1: Get your data in the right format for TerraTorch</a>
  <ul class="collapse">
  <li><a href="#file-organization" id="toc-file-organization" class="nav-link" data-scroll-target="#file-organization">File organization</a></li>
  <li><a href="#specify-splits" id="toc-specify-splits" class="nav-link" data-scroll-target="#specify-splits">Specify splits</a></li>
  </ul></li>
  <li><a href="#step-2-get-your-datamodule-set-up" id="toc-step-2-get-your-datamodule-set-up" class="nav-link" data-scroll-target="#step-2-get-your-datamodule-set-up">Step 2: Get your DataModule set up</a>
  <ul class="collapse">
  <li><a href="#inspect-your-data" id="toc-inspect-your-data" class="nav-link" data-scroll-target="#inspect-your-data">Inspect your data</a></li>
  <li><a href="#define-bands-statistics-and-transforms" id="toc-define-bands-statistics-and-transforms" class="nav-link" data-scroll-target="#define-bands-statistics-and-transforms">Define bands, statistics, and transforms</a></li>
  <li><a href="#define-your-datamodule" id="toc-define-your-datamodule" class="nav-link" data-scroll-target="#define-your-datamodule">Define your DataModule</a></li>
  <li><a href="#check-your-datamodule" id="toc-check-your-datamodule" class="nav-link" data-scroll-target="#check-your-datamodule">Check your DataModule</a></li>
  </ul></li>
  <li><a href="#step-3-get-your-task-set-up" id="toc-step-3-get-your-task-set-up" class="nav-link" data-scroll-target="#step-3-get-your-task-set-up">Step 3: Get your Task set up</a>
  <ul class="collapse">
  <li><a href="#choose-your-model-from-the-model-factory" id="toc-choose-your-model-from-the-model-factory" class="nav-link" data-scroll-target="#choose-your-model-from-the-model-factory">Choose your model from the model factory</a></li>
  <li><a href="#define-your-task" id="toc-define-your-task" class="nav-link" data-scroll-target="#define-your-task">Define your Task</a></li>
  </ul></li>
  <li><a href="#step-4-train-your-model" id="toc-step-4-train-your-model" class="nav-link" data-scroll-target="#step-4-train-your-model">Step 4: Train your model</a></li>
  <li><a href="#step-5-run-inference-with-your-trained-model" id="toc-step-5-run-inference-with-your-trained-model" class="nav-link" data-scroll-target="#step-5-run-inference-with-your-trained-model">Step 5: Run inference with your trained model</a>
  <ul class="collapse">
  <li><a href="#load-the-best-checkpoint" id="toc-load-the-best-checkpoint" class="nav-link" data-scroll-target="#load-the-best-checkpoint">Load the best checkpoint</a></li>
  <li><a href="#get-some-test-images" id="toc-get-some-test-images" class="nav-link" data-scroll-target="#get-some-test-images">Get some test images</a></li>
  <li><a href="#run-predictions" id="toc-run-predictions" class="nav-link" data-scroll-target="#run-predictions">Run predictions</a></li>
  <li><a href="#visualize-predictions" id="toc-visualize-predictions" class="nav-link" data-scroll-target="#visualize-predictions">Visualize predictions</a></li>
  <li><a href="#get-prediction-probabilities" id="toc-get-prediction-probabilities" class="nav-link" data-scroll-target="#get-prediction-probabilities">Get prediction probabilities</a></li>
  </ul></li>
  <li><a href="#bonus-exploring-model-embeddings" id="toc-bonus-exploring-model-embeddings" class="nav-link" data-scroll-target="#bonus-exploring-model-embeddings">Bonus: Exploring model embeddings</a>
  <ul class="collapse">
  <li><a href="#visualize-embedding-space-with-pca" id="toc-visualize-embedding-space-with-pca" class="nav-link" data-scroll-target="#visualize-embedding-space-with-pca">Visualize embedding space with PCA</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<p>TerraTorch is a package that allows you to easily use and fine tune geospatial foundation models (GeoFMs). This tutorial walks through how to get started with TerraTorch to fine tune a GeoFM on your own data and task, whatever that may be.</p>
<p>Achieving fine tuning and inference using TerraTorch can be achieved in five steps:</p>
<ol type="1">
<li><strong>Organize your data:</strong> Make sure your inputs, labels, and splits are organized in a standardized way that TerraTorch generic DataModules can interpret</li>
<li><strong>Set up your DataModule:</strong> Use TerraTorch generic DataModules to create an object which reads in your data, makes any necessary adjustments, and passes it to your model in the format it expects for training.</li>
<li><strong>Set up your Task:</strong> Identify your chosen task, choose your model and define hyperparameters</li>
<li><strong>Train your model:</strong> Use PyTorch Lightning to handle training and logging for a set number of epochs.</li>
<li><strong>Run inference:</strong> Once training is complete, use your fine-tuned model to make predictions on new data and inspect embeddings.</li>
</ol>
<p>In this demo, we will show how to fine tune Prithvi on a subset of the EuroSAT benchmark, which classifies 64x64 pixel Sentinel 2 L1C images into one of 10 classes. The demo is written in a general way to help users adapt this code to their specific use case.</p>
<section id="step-0-setup" class="level2">
<h2 class="anchored" data-anchor-id="step-0-setup">Step 0: Setup</h2>
<section id="huggingface-cache" class="level3">
<h3 class="anchored" data-anchor-id="huggingface-cache">HuggingFace cache</h3>
<p>Because we are on a shared server with shared conda environment, we need to reorient where huggingface downloads things like model weights and datasets</p>
<div id="04998699" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pathlib</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppress PIL warnings about multi-band TIFFs (common with satellite imagery)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>logging.getLogger(<span class="st">'PIL.TiffImagePlugin'</span>).setLevel(logging.CRITICAL)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppress other common warnings</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>, category<span class="op">=</span><span class="pp">UserWarning</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify where you want model weights/datasets to be saved</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>DATA_PATH <span class="op">=</span> <span class="st">'/Users/kellycaylor/dev/geoAI/data'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f5576d7a" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up environment variables that are used by HuggingFace</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"HF_HOME"</span>] <span class="op">=</span> os.path.join(DATA_PATH, <span class="st">"hfhome"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"HF_HUB_CACHE"</span>] <span class="op">=</span> os.path.join(DATA_PATH, <span class="st">"hub"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"HF_DATASETS_CACHE"</span>] <span class="op">=</span> os.path.join(DATA_PATH, <span class="st">"datasets"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"TRANSFORMERS_CACHE"</span>] <span class="op">=</span> os.path.join(DATA_PATH, <span class="st">"transformers"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="step-1-get-your-data-in-the-right-format-for-terratorch" class="level2">
<h2 class="anchored" data-anchor-id="step-1-get-your-data-in-the-right-format-for-terratorch">Step 1: Get your data in the right format for TerraTorch</h2>
<p>TerraTorch makes your life easier if you organize your data in standard ways. Specifically, following these conventions will allow you to use their generic DataModules and save your the trouble of writing your own.</p>
<section id="file-organization" class="level3">
<h3 class="anchored" data-anchor-id="file-organization">File organization</h3>
<ul>
<li><strong>Classification:</strong> Organize your images into different folders depending on their class (this is how EuroSAT is organized). Alternatively, have a metadata file listing</li>
<li><strong>Segmentation/regression:</strong> Have images and their labels (masks or continuous outputs) have the same name but followed by an image vs.&nbsp;mask or label identifier (e.g.&nbsp;image: <code>Im_1_img.tif</code> and <code>Im_1_mask.tif</code></li>
</ul>
<section id="eurosat-classification-example" class="level4">
<h4 class="anchored" data-anchor-id="eurosat-classification-example">EuroSAT classification example</h4>
<p>We can retrieve a subsample of the EuroSAT dataset using torchgeo. Luckily, we will show it is already in the format TerraTorch expects for a classification task.</p>
<div id="8950e197" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchgeo.datasets <span class="im">import</span> EuroSAT100 <span class="co"># 100 image subset from EuroSAT</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>EuroSAT100(</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span>DATA_PATH, <span class="co"># make sure you have defined the path where you want data saved</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    split<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>EuroSAT100(</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span>DATA_PATH,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    split<span class="op">=</span><span class="st">'val'</span>,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>EuroSAT100(</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span>DATA_PATH,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    split<span class="op">=</span><span class="st">'test'</span>,</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>Dataset EuroSAT100
    Number of datapoints: 20
    Root location: /Users/kellycaylor/dev/geoAI/data/ds/images/remote_sensing/otherDatasets/sentinel_2/tif</code></pre>
</div>
</div>
<p>When opening inspecting the downloaded dataset, you will find that the data will be stored at the following path:</p>
<div id="014fbe78" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the location of the saved EUROSAT data</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>EUROSAT <span class="op">=</span> DATA_PATH <span class="op">+</span> <span class="st">"/ds/images/remote_sensing/otherDatasets/sentinel_2/tif/"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>There is one folder for each class, each containing <code>.tif</code> files with the images to be classified.</p>
<div id="c8dde786" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># List the folders each containing images for a given EuroSAT class</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>class_paths <span class="op">=</span> [item <span class="cf">for</span> item <span class="kw">in</span> Path(EUROSAT).iterdir()]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [item.name <span class="cf">for</span> item <span class="kw">in</span> class_paths]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_names)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['Forest', 'River', 'Highway', 'AnnualCrop', 'SeaLake', 'HerbaceousVegetation', 'Industrial', 'Residential', 'PermanentCrop', 'Pasture']</code></pre>
</div>
</div>
</section>
</section>
<section id="specify-splits" class="level3">
<h3 class="anchored" data-anchor-id="specify-splits">Specify splits</h3>
<p>To specify which images belong to which split, you have two options:</p>
<ul>
<li>provide <code>.txt</code> files that have the names of the images in each split in them</li>
<li>organize the splits into different folders</li>
</ul>
<section id="eurosat-example" class="level4">
<h4 class="anchored" data-anchor-id="eurosat-example">EuroSAT example</h4>
<p>In the case of EuroSAT, the names of the images belonging to each split are saved as <code>.txt</code> files.</p>
<div id="60598cbe" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the paths to each split for use in the DataModule setup</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>TRAIN_SPLIT <span class="op">=</span> DATA_PATH <span class="op">+</span> <span class="st">'/eurosat-100-train.txt'</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>VAL_SPLIT <span class="op">=</span> DATA_PATH <span class="op">+</span> <span class="st">'/eurosat-100-val.txt'</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>TEST_SPLIT <span class="op">=</span> DATA_PATH <span class="op">+</span> <span class="st">'/eurosat-100-test.txt'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="step-2-get-your-datamodule-set-up" class="level2">
<h2 class="anchored" data-anchor-id="step-2-get-your-datamodule-set-up">Step 2: Get your DataModule set up</h2>
<section id="inspect-your-data" class="level3">
<h3 class="anchored" data-anchor-id="inspect-your-data">Inspect your data</h3>
<p>It is important to have a good grasp of the contents of your data in order to set up the DataModule properly. It can therefore be useful to do a bit of exploration and visualization.</p>
<div id="0382e00c" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read in the first image in each class for inspection</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rasterio</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># get the first image in each class</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>image_sample <span class="op">=</span> [<span class="bu">next</span>(folder.iterdir()) <span class="cf">for</span> folder <span class="kw">in</span> class_paths] <span class="co"># list of 10 images</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># read all 10 sample images</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>ims <span class="op">=</span> [rasterio.<span class="bu">open</span>(image).read() <span class="cf">for</span> image <span class="kw">in</span> image_sample]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="image-statistics" class="level4">
<h4 class="anchored" data-anchor-id="image-statistics">Image statistics</h4>
<p>By inspecting the first image, we find that we have 13 bands per image (corresponding to the 13 Sentinel-2 L1C bands) and images are 64x64 pixels. Neither of these is what Prithvi expects, so it‚Äôs important to know this so we can adjust it in the DataLoader.</p>
<p>We also note that the units are in reflectance values.</p>
<div id="2c54ec13" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print some statistics for the first image</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape:"</span>, ims[<span class="dv">1</span>].shape)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Means by band:"</span>, ims[<span class="dv">1</span>].mean(axis<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>)))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stds by band:"</span>, ims[<span class="dv">1</span>].std(axis<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape: (13, 64, 64)
Means by band: [1288.97705078 1025.03173828  944.13305664  725.19604492  888.26782227
 1701.43383789 2149.0300293  2014.53515625  424.36669922    8.78369141
 1140.24414062  637.13842773 2281.08129883]
Stds by band: [  50.77284027  110.01204421  197.95281388  304.08487263  371.85495521
  836.55788254 1173.53766023 1202.2317915   191.06255061    1.39687418
  787.56197144  505.03331818 1318.82967564]</code></pre>
</div>
</div>
</section>
<section id="visualization" class="level4">
<h4 class="anchored" data-anchor-id="visualization">Visualization</h4>
<div id="d77bf59c" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_rgb(img, bands<span class="op">=</span>[<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>]):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># select the r,g,b bands</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> img[bands].astype(<span class="bu">float</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Follow EuroSAT guidelines for visualization</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># clip to a max of 2750</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># normalize to 0-1</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> np.clip(rgb, <span class="dv">0</span>, <span class="dv">2750</span>) <span class="op">/</span> <span class="dv">2750</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># go from (CxWxH) to (WxHxC)</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> np.transpose(rgb, (<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rgb</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.ravel()</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (ax, img, name) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(axes, ims, class_names)):</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> to_rgb(img)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    ax.imshow(rgb)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    ax.set_title(name)</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">"off"</span>)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="terratorch_finetuning_workflow_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="define-bands-statistics-and-transforms" class="level3">
<h3 class="anchored" data-anchor-id="define-bands-statistics-and-transforms">Define bands, statistics, and transforms</h3>
<p>There are a few important parameters we will need to define when setting up our DataModule in addition to the location of the data and splits that we defined in Step 1.</p>
<p>We specifically use the GenericNonGeoClassificationDataModule, though all generic DataModules will need these inputs. You can always learn more about the arguments for a given DataModule by adapting the following code:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.datamodules <span class="im">import</span> GenericNonGeoClassificationDataModule</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>?GenericNonGeoClassificationDataModule</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="band-names" class="level4">
<h4 class="anchored" data-anchor-id="band-names">Band names</h4>
<p>We need to tell the datamodule which bands we actually want to go into the model.</p>
<p>In our case, we have 13 sentinel 2 bands, but Prithvi is trained on 6 landsay bands, so we need to subset them.</p>
<div id="1e282f28" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>sentinel2_bands <span class="op">=</span> [</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"B1"</span>, <span class="st">"B2"</span>, <span class="st">"B3"</span>, <span class="st">"B4"</span>, <span class="st">"B5"</span>, <span class="st">"B6"</span>, <span class="st">"B7"</span>,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"B8"</span>, <span class="st">"B8A"</span>, <span class="st">"B9"</span>, <span class="st">"B10"</span>, <span class="st">"B11"</span>, <span class="st">"B12"</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The subset of our bands Prithvi will take</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>prithvi_subset <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">11</span>, <span class="dv">12</span>]</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>prithvi_bands <span class="op">=</span> [sentinel2_bands[i] <span class="cf">for</span> i <span class="kw">in</span> prithvi_subset]</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prithvi_bands)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['B2', 'B3', 'B4', 'B8A', 'B11', 'B12']</code></pre>
</div>
</div>
</section>
<section id="means-and-standard-deviations" class="level4">
<h4 class="anchored" data-anchor-id="means-and-standard-deviations">Means and standard deviations</h4>
<p>We need band-level means and standard deviations. Either compute them yourself over your entire dataset, or get general numbers that work well for your data type.</p>
<p>We have S2 L1C data. We can use TerraMesh statistics I grabbed from their huggingface: https://huggingface.co/api/resolve-cache/datasets/ibm-esa-geospatial/TerraMesh/6c548cdacdd70e98a236de9f5b708d4b9dadf253/terramesh.py</p>
<div id="46aeed69" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>statistics <span class="op">=</span> {</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mean"</span>: {</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2L1C"</span>: [<span class="fl">2357.090</span>, <span class="fl">2137.398</span>, <span class="fl">2018.799</span>, <span class="fl">2082.998</span>, <span class="fl">2295.663</span>, <span class="fl">2854.548</span>, <span class="fl">3122.860</span>, <span class="fl">3040.571</span>, <span class="fl">3306.491</span>, <span class="fl">1473.849</span>,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>                  <span class="fl">506.072</span>, <span class="fl">2472.840</span>, <span class="fl">1838.943</span>],</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2L2A"</span>: [<span class="fl">1390.461</span>, <span class="fl">1503.332</span>, <span class="fl">1718.211</span>, <span class="fl">1853.926</span>, <span class="fl">2199.116</span>, <span class="fl">2779.989</span>, <span class="fl">2987.025</span>, <span class="fl">3083.248</span>, <span class="fl">3132.235</span>, <span class="fl">3162.989</span>,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                  <span class="fl">2424.902</span>, <span class="fl">1857.665</span>],</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2RGB"</span>: [<span class="fl">110.349</span>, <span class="fl">99.507</span>, <span class="fl">75.843</span>],</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S1GRD"</span>: [<span class="op">-</span><span class="fl">12.577</span>, <span class="op">-</span><span class="fl">20.265</span>],</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S1RTC"</span>: [<span class="op">-</span><span class="fl">10.93</span>, <span class="op">-</span><span class="fl">17.329</span>],</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"NDVI"</span>: [<span class="fl">0.327</span>],</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"DEM"</span>: [<span class="fl">651.663</span>],</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"std"</span>: {</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2L1C"</span>: [<span class="fl">1673.639</span>, <span class="fl">1722.641</span>, <span class="fl">1602.205</span>, <span class="fl">1873.138</span>, <span class="fl">1866.055</span>, <span class="fl">1779.839</span>, <span class="fl">1776.496</span>, <span class="fl">1724.114</span>, <span class="fl">1771.041</span>, <span class="fl">1079.786</span>,</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>                  <span class="fl">512.404</span>, <span class="fl">1340.879</span>, <span class="fl">1172.435</span>],</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2L2A"</span>: [<span class="fl">2131.157</span>, <span class="fl">2163.666</span>, <span class="fl">2059.311</span>, <span class="fl">2152.477</span>, <span class="fl">2105.179</span>, <span class="fl">1912.773</span>, <span class="fl">1842.326</span>, <span class="fl">1893.568</span>, <span class="fl">1775.656</span>, <span class="fl">1814.907</span>,</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>                  <span class="fl">1436.282</span>, <span class="fl">1336.155</span>],</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2RGB"</span>: [<span class="fl">69.905</span>, <span class="fl">53.708</span>, <span class="fl">53.378</span>],</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S1GRD"</span>: [<span class="fl">5.179</span>, <span class="fl">5.872</span>],</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S1RTC"</span>: [<span class="fl">4.391</span>, <span class="fl">4.459</span>],</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="st">"NDVI"</span>: [<span class="fl">0.322</span>],</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">"DEM"</span>: [<span class="fl">928.168</span>]</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We specifically need to pass Sentinel 2 L1C means and stds, but only for the 6 bands we are subsetting for input into prithvi.</p>
<div id="5b1f4638" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>S2L1C_prithvi_means <span class="op">=</span> [statistics[<span class="st">"mean"</span>][<span class="st">"S2L1C"</span>][i] <span class="cf">for</span> i <span class="kw">in</span> prithvi_subset]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>S2L1C_prithvi_stds <span class="op">=</span> [statistics[<span class="st">"std"</span>][<span class="st">"S2L1C"</span>][i] <span class="cf">for</span> i <span class="kw">in</span> prithvi_subset]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(S2L1C_prithvi_means)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(S2L1C_prithvi_stds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[2137.398, 2018.799, 2082.998, 3306.491, 2472.84, 1838.943]
[1722.641, 1602.205, 1873.138, 1771.041, 1340.879, 1172.435]</code></pre>
</div>
</div>
</section>
<section id="transforms" class="level4">
<h4 class="anchored" data-anchor-id="transforms">Transforms</h4>
<p>Transforms allow you to make changes to your data on the fly before feeding them to a model for training. There are three important types of transforms:</p>
<ul>
<li>Reshaping/resizing/clipping: To get your data into the shape your model expects.</li>
<li>Data augmentations: If you don‚Äôt have a lot of training data, you can ‚Äúaugment‚Äù your dataset by presenting changed versions of your images to the model. For example, flip your images. This type of transform generally should only happen to your training data, and not your validation and testing data.</li>
<li>Make your image a tensor (the data format that GPUs can use)</li>
</ul>
<p>We can use albumentations to do any of these.</p>
<div id="cfdce899" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> albumentations</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>train_transforms <span class="op">=</span> albumentations.Compose([</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    albumentations.Resize(<span class="dv">224</span>, <span class="dv">224</span>), <span class="co"># go from 64x64 to 224x224 (Prithvi expected size)</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    albumentations.HorizontalFlip(), <span class="co"># augmentation</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    albumentations.pytorch.transforms.ToTensorV2(),</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Same transforms but without augmentations for validation and testing</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>val_test_transforms <span class="op">=</span> albumentations.Compose([</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    albumentations.Resize(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    albumentations.pytorch.transforms.ToTensorV2(),</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="define-your-datamodule" class="level3">
<h3 class="anchored" data-anchor-id="define-your-datamodule">Define your DataModule</h3>
<div id="9fd28483" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.datamodules <span class="im">import</span> GenericNonGeoClassificationDataModule</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>datamodule <span class="op">=</span> GenericNonGeoClassificationDataModule(</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">16</span>, <span class="co"># How many images to give the model at once. More = faster, but more RAM is needed</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">0</span>, <span class="co"># extra CPU threads for image loading</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Where is our data? In our case, all splits are in the same folder</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    train_data_root<span class="op">=</span>EUROSAT,</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    val_data_root<span class="op">=</span>EUROSAT,</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    test_data_root<span class="op">=</span>EUROSAT,</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Because images for all splits are in the same place, we need to specify our split files</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    train_split<span class="op">=</span>TRAIN_SPLIT,</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    val_split<span class="op">=</span>VAL_SPLIT,</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    test_split<span class="op">=</span>TEST_SPLIT,</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># means and standard deviations for the bands being input into the model</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    means<span class="op">=</span>S2L1C_prithvi_means,</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    stds<span class="op">=</span>S2L1C_prithvi_stds,</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># number of classes</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="bu">len</span>(class_names),</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tranforms, defined using albumentations</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    train_transform<span class="op">=</span>train_transforms,</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>    val_transform<span class="op">=</span>val_test_transforms,</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    test_transform<span class="op">=</span>val_test_transforms,</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bands of our dataset</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    dataset_bands<span class="op">=</span>sentinel2_bands,</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bands to input into our model</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    output_bands<span class="op">=</span>prithvi_bands,</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>datamodule.setup(<span class="st">"fit"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="check-your-datamodule" class="level3">
<h3 class="anchored" data-anchor-id="check-your-datamodule">Check your DataModule</h3>
<p>To ensure that the data are as we would expect the model to find them, we can manually iterate through a batch of the datamodule to inspect it.</p>
<div id="5184712f" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(datamodule.train_dataloader()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="aa375f02" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch keys: </span><span class="sc">{</span>batch<span class="sc">.</span>keys()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image shape: </span><span class="sc">{</span>batch[<span class="st">'image'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"Label shape: </span><span class="sc">{</span>batch[<span class="st">'label'</span>]<span class="sc">.</span>shape <span class="cf">if</span> <span class="st">'label'</span> <span class="kw">in</span> batch <span class="cf">else</span> batch[<span class="st">'labels'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the actual values</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Image dtype: </span><span class="sc">{</span>batch[<span class="st">'image'</span>]<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image min/max: </span><span class="sc">{</span>batch[<span class="st">'image'</span>]<span class="sc">.</span><span class="bu">min</span>()<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>batch[<span class="st">'image'</span>]<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Labels: </span><span class="sc">{</span>batch[<span class="st">'label'</span>] <span class="cf">if</span> <span class="st">'label'</span> <span class="kw">in</span> batch <span class="cf">else</span> batch[<span class="st">'labels'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batch keys: dict_keys(['image', 'label', 'filename'])
Image shape: torch.Size([16, 6, 224, 224])
Label shape: torch.Size([16])

Image dtype: torch.float32
Image min/max: 3.015306234359741, 6064.68359375
Labels: tensor([2, 1, 0, 6, 9, 1, 2, 9, 1, 7, 8, 5, 8, 3, 3, 0])</code></pre>
</div>
</div>
<p>As expected, we have 16 images with 6 bands and resized to the width and height the model expects.</p>
<p>Notice the min and max have not been normalized. This is because normalization occurs during every training and validation step, which happens below.</p>
</section>
</section>
<section id="step-3-get-your-task-set-up" class="level2">
<h2 class="anchored" data-anchor-id="step-3-get-your-task-set-up">Step 3: Get your Task set up</h2>
<p>TerraTorch has several Lightning Trainers to easily handle model training, and has Tasks defined for each major task type. We will be using the ClassificationTask.</p>
<p>These Tasks are where you can define your model choice and model hyperparameters.</p>
<section id="choose-your-model-from-the-model-factory" class="level3">
<h3 class="anchored" data-anchor-id="choose-your-model-from-the-model-factory">Choose your model from the model factory</h3>
<p>If you don‚Äôt know which model you would like to use, you can explore what is available in TerraTorch‚Äôs model factory.</p>
<section id="explore-encoders" class="level4">
<h4 class="anchored" data-anchor-id="explore-encoders">Explore encoders</h4>
<div id="9880b3d7" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.registry <span class="im">import</span> BACKBONE_REGISTRY</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># List all the available backbones from different sources</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, source <span class="kw">in</span> BACKBONE_REGISTRY._sources.items():</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">====</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">====="</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">list</span>(source))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
====terratorch=====
['dofa_small_patch16_224', 'dofa_base_patch16_224', 'dofa_large_patch16_224', 'prithvi_eo_tiny', 'prithvi_eo_v1_100', 'prithvi_eo_v2_tiny_tl', 'prithvi_eo_v2_100_tl', 'prithvi_eo_v2_300', 'prithvi_eo_v2_600', 'prithvi_eo_v2_300_tl', 'prithvi_eo_v2_600_tl', 'terramind_v1_base', 'terramind_v1_base_tim', 'terramind_v01_base', 'terramind_v01_base_tim', 'terramind_v1_large', 'terramind_v1_large_tim', 'terramind_v1_tiny', 'terramind_v1_tiny_tim', 'terramind_v1_small', 'terramind_v1_small_tim', 'terramind_v1_tokenizer_s2l2a', 'terramind_v1_tokenizer_s1rtc', 'terramind_v1_tokenizer_s1grd', 'terramind_v1_tokenizer_dem', 'terramind_v1_tokenizer_lulc', 'terramind_v1_tokenizer_ndvi', 'ssl4eol_resnet18_landsat_tm_toa_moco', 'ssl4eol_resnet18_landsat_tm_toa_simclr', 'ssl4eol_resnet18_landsat_etm_toa_moco', 'ssl4eol_resnet18_landsat_etm_toa_simclr', 'ssl4eol_resnet18_landsat_etm_sr_moco', 'ssl4eol_resnet18_landsat_etm_sr_simclr', 'ssl4eol_resnet18_landsat_oli_tirs_toa_moco', 'ssl4eol_resnet18_landsat_oli_tirs_toa_simclr', 'ssl4eol_resnet18_landsat_oli_sr_moco', 'ssl4eol_resnet18_landsat_oli_sr_simclr', 'ssl4eos12_resnet18_sentinel2_all_moco', 'ssl4eos12_resnet18_sentinel2_rgb_moco', 'seco_resnet18_sentinel2_rgb_seco', 'fmow_resnet50_fmow_rgb_gassl', 'ssl4eol_resnet50_landsat_tm_toa_moco', 'ssl4eol_resnet50_landsat_tm_toa_simclr', 'ssl4eol_resnet50_landsat_etm_toa_moco', 'ssl4eol_resnet50_landsat_etm_toa_simclr', 'ssl4eol_resnet50_landsat_etm_sr_moco', 'ssl4eol_resnet50_landsat_etm_sr_simclr', 'ssl4eol_resnet50_landsat_oli_tirs_toa_moco', 'ssl4eol_resnet50_landsat_oli_tirs_toa_simclr', 'ssl4eol_resnet50_landsat_oli_sr_moco', 'ssl4eol_resnet50_landsat_oli_sr_simclr', 'ssl4eos12_resnet50_sentinel1_all_decur', 'ssl4eos12_resnet50_sentinel1_all_moco', 'ssl4eos12_resnet50_sentinel2_all_decur', 'ssl4eos12_resnet50_sentinel2_all_dino', 'ssl4eos12_resnet50_sentinel2_all_moco', 'ssl4eos12_resnet50_sentinel2_rgb_moco', 'seco_resnet50_sentinel2_rgb_seco', 'satlas_resnet50_sentinel2_mi_ms_satlas', 'satlas_resnet50_sentinel2_mi_rgb_satlas', 'satlas_resnet50_sentinel2_si_ms_satlas', 'satlas_resnet50_sentinel2_si_rgb_satlas', 'satlas_resnet152_sentinel2_mi_ms', 'satlas_resnet152_sentinel2_mi_rgb', 'satlas_resnet152_sentinel2_si_ms_satlas', 'satlas_resnet152_sentinel2_si_rgb_satlas', 'satlas_swin_t_sentinel2_mi_ms', 'satlas_swin_t_sentinel2_mi_rgb', 'satlas_swin_t_sentinel2_si_ms', 'satlas_swin_t_sentinel2_si_rgb', 'satlas_swin_b_sentinel2_mi_ms', 'satlas_swin_b_sentinel2_mi_rgb', 'satlas_swin_b_sentinel2_si_ms', 'satlas_swin_b_sentinel2_si_rgb', 'satlas_swin_b_naip_mi_rgb', 'satlas_swin_b_naip_si_rgb', 'satlas_swin_b_landsat_mi_ms', 'satlas_swin_b_landsat_mi_rgb', 'satlas_swin_b_sentinel1_mi', 'satlas_swin_b_sentinel1_si', 'ssl4eol_vit_small_patch16_224_landsat_tm_toa_moco', 'ssl4eol_vit_small_patch16_224_landsat_tm_toa_simclr', 'ssl4eol_vit_small_patch16_224_landsat_etm_toa_moco', 'ssl4eol_vit_small_patch16_224_landsat_etm_toa_simclr', 'ssl4eol_vit_small_patch16_224_landsat_etm_sr_moco', 'ssl4eol_vit_small_patch16_224_landsat_etm_sr_simclr', 'ssl4eol_vit_small_patch16_224_landsat_oli_tirs_toa_simclr', 'ssl4eol_vit_small_patch16_224_landsat_oli_sr_moco', 'ssl4eol_vit_small_patch16_224_landsat_oli_sr_simclr', 'ssl4eos12_vit_small_patch16_224_sentinel2_all_dino', 'ssl4eos12_vit_small_patch16_224_sentinel2_all_moco', 'UNet']

====timm=====
['aimv2_1b_patch14_224', 'aimv2_1b_patch14_336', 'aimv2_1b_patch14_448', 'aimv2_3b_patch14_224', 'aimv2_3b_patch14_336', 'aimv2_3b_patch14_448', 'aimv2_huge_patch14_224', 'aimv2_huge_patch14_336', 'aimv2_huge_patch14_448', 'aimv2_large_patch14_224', 'aimv2_large_patch14_336', 'aimv2_large_patch14_448', 'bat_resnext26ts', 'beit3_base_patch16_224', 'beit3_giant_patch14_224', 'beit3_giant_patch14_336', 'beit3_large_patch16_224', 'beit_base_patch16_224', 'beit_base_patch16_384', 'beit_large_patch16_224', 'beit_large_patch16_384', 'beit_large_patch16_512', 'beitv2_base_patch16_224', 'beitv2_large_patch16_224', 'botnet26t_256', 'botnet50ts_256', 'caformer_b36', 'caformer_m36', 'caformer_s18', 'caformer_s36', 'cait_m36_384', 'cait_m48_448', 'cait_s24_224', 'cait_s24_384', 'cait_s36_384', 'cait_xs24_384', 'cait_xxs24_224', 'cait_xxs24_384', 'cait_xxs36_224', 'cait_xxs36_384', 'clay_v1_base', 'coat_lite_medium', 'coat_lite_medium_384', 'coat_lite_mini', 'coat_lite_small', 'coat_lite_tiny', 'coat_mini', 'coat_small', 'coat_tiny', 'coatnet_0_224', 'coatnet_0_rw_224', 'coatnet_1_224', 'coatnet_1_rw_224', 'coatnet_2_224', 'coatnet_2_rw_224', 'coatnet_3_224', 'coatnet_3_rw_224', 'coatnet_4_224', 'coatnet_5_224', 'coatnet_bn_0_rw_224', 'coatnet_nano_cc_224', 'coatnet_nano_rw_224', 'coatnet_pico_rw_224', 'coatnet_rmlp_0_rw_224', 'coatnet_rmlp_1_rw2_224', 'coatnet_rmlp_1_rw_224', 'coatnet_rmlp_2_rw_224', 'coatnet_rmlp_2_rw_384', 'coatnet_rmlp_3_rw_224', 'coatnet_rmlp_nano_rw_224', 'coatnext_nano_rw_224', 'convformer_b36', 'convformer_m36', 'convformer_s18', 'convformer_s36', 'convit_base', 'convit_small', 'convit_tiny', 'convmixer_768_32', 'convmixer_1024_20_ks9_p14', 'convmixer_1536_20', 'convnext_atto', 'convnext_atto_ols', 'convnext_atto_rms', 'convnext_base', 'convnext_femto', 'convnext_femto_ols', 'convnext_large', 'convnext_large_mlp', 'convnext_nano', 'convnext_nano_ols', 'convnext_pico', 'convnext_pico_ols', 'convnext_small', 'convnext_tiny', 'convnext_tiny_hnf', 'convnext_xlarge', 'convnext_xxlarge', 'convnext_zepto_rms', 'convnext_zepto_rms_ols', 'convnextv2_atto', 'convnextv2_base', 'convnextv2_femto', 'convnextv2_huge', 'convnextv2_large', 'convnextv2_nano', 'convnextv2_pico', 'convnextv2_small', 'convnextv2_tiny', 'crossvit_9_240', 'crossvit_9_dagger_240', 'crossvit_15_240', 'crossvit_15_dagger_240', 'crossvit_15_dagger_408', 'crossvit_18_240', 'crossvit_18_dagger_240', 'crossvit_18_dagger_408', 'crossvit_base_240', 'crossvit_small_240', 'crossvit_tiny_240', 'cs3darknet_focus_l', 'cs3darknet_focus_m', 'cs3darknet_focus_s', 'cs3darknet_focus_x', 'cs3darknet_l', 'cs3darknet_m', 'cs3darknet_s', 'cs3darknet_x', 'cs3edgenet_x', 'cs3se_edgenet_x', 'cs3sedarknet_l', 'cs3sedarknet_x', 'cs3sedarknet_xdw', 'cspdarknet53', 'cspresnet50', 'cspresnet50d', 'cspresnet50w', 'cspresnext50', 'darknet17', 'darknet21', 'darknet53', 'darknetaa53', 'davit_base', 'davit_base_fl', 'davit_giant', 'davit_huge', 'davit_huge_fl', 'davit_large', 'davit_small', 'davit_tiny', 'deit3_base_patch16_224', 'deit3_base_patch16_384', 'deit3_huge_patch14_224', 'deit3_large_patch16_224', 'deit3_large_patch16_384', 'deit3_medium_patch16_224', 'deit3_small_patch16_224', 'deit3_small_patch16_384', 'deit_base_distilled_patch16_224', 'deit_base_distilled_patch16_384', 'deit_base_patch16_224', 'deit_base_patch16_384', 'deit_small_distilled_patch16_224', 'deit_small_patch16_224', 'deit_tiny_distilled_patch16_224', 'deit_tiny_patch16_224', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'densenet264d', 'densenetblur121d', 'dla34', 'dla46_c', 'dla46x_c', 'dla60', 'dla60_res2net', 'dla60_res2next', 'dla60x', 'dla60x_c', 'dla102', 'dla102x', 'dla102x2', 'dla169', 'dm_nfnet_f0', 'dm_nfnet_f1', 'dm_nfnet_f2', 'dm_nfnet_f3', 'dm_nfnet_f4', 'dm_nfnet_f5', 'dm_nfnet_f6', 'dpn48b', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn107', 'dpn131', 'eca_botnext26ts_256', 'eca_halonext26ts', 'eca_nfnet_l0', 'eca_nfnet_l1', 'eca_nfnet_l2', 'eca_nfnet_l3', 'eca_resnet33ts', 'eca_resnext26ts', 'eca_vovnet39b', 'ecaresnet26t', 'ecaresnet50d', 'ecaresnet50d_pruned', 'ecaresnet50t', 'ecaresnet101d', 'ecaresnet101d_pruned', 'ecaresnet200d', 'ecaresnet269d', 'ecaresnetlight', 'ecaresnext26t_32x4d', 'ecaresnext50t_32x4d', 'edgenext_base', 'edgenext_small', 'edgenext_small_rw', 'edgenext_x_small', 'edgenext_xx_small', 'efficientformer_l1', 'efficientformer_l3', 'efficientformer_l7', 'efficientformerv2_l', 'efficientformerv2_s0', 'efficientformerv2_s1', 'efficientformerv2_s2', 'efficientnet_b0', 'efficientnet_b0_g8_gn', 'efficientnet_b0_g16_evos', 'efficientnet_b0_gn', 'efficientnet_b1', 'efficientnet_b1_pruned', 'efficientnet_b2', 'efficientnet_b2_pruned', 'efficientnet_b3', 'efficientnet_b3_g8_gn', 'efficientnet_b3_gn', 'efficientnet_b3_pruned', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'efficientnet_b8', 'efficientnet_blur_b0', 'efficientnet_cc_b0_4e', 'efficientnet_cc_b0_8e', 'efficientnet_cc_b1_8e', 'efficientnet_el', 'efficientnet_el_pruned', 'efficientnet_em', 'efficientnet_es', 'efficientnet_es_pruned', 'efficientnet_h_b5', 'efficientnet_l2', 'efficientnet_lite0', 'efficientnet_lite1', 'efficientnet_lite2', 'efficientnet_lite3', 'efficientnet_lite4', 'efficientnet_x_b3', 'efficientnet_x_b5', 'efficientnetv2_l', 'efficientnetv2_m', 'efficientnetv2_rw_m', 'efficientnetv2_rw_s', 'efficientnetv2_rw_t', 'efficientnetv2_s', 'efficientnetv2_xl', 'efficientvit_b0', 'efficientvit_b1', 'efficientvit_b2', 'efficientvit_b3', 'efficientvit_l1', 'efficientvit_l2', 'efficientvit_l3', 'efficientvit_m0', 'efficientvit_m1', 'efficientvit_m2', 'efficientvit_m3', 'efficientvit_m4', 'efficientvit_m5', 'ese_vovnet19b_dw', 'ese_vovnet19b_slim', 'ese_vovnet19b_slim_dw', 'ese_vovnet39b', 'ese_vovnet39b_evos', 'ese_vovnet57b', 'ese_vovnet99b', 'eva02_base_patch14_224', 'eva02_base_patch14_448', 'eva02_base_patch16_clip_224', 'eva02_enormous_patch14_clip_224', 'eva02_large_patch14_224', 'eva02_large_patch14_448', 'eva02_large_patch14_clip_224', 'eva02_large_patch14_clip_336', 'eva02_small_patch14_224', 'eva02_small_patch14_336', 'eva02_tiny_patch14_224', 'eva02_tiny_patch14_336', 'eva_giant_patch14_224', 'eva_giant_patch14_336', 'eva_giant_patch14_560', 'eva_giant_patch14_clip_224', 'eva_large_patch14_196', 'eva_large_patch14_336', 'fasternet_l', 'fasternet_m', 'fasternet_s', 'fasternet_t0', 'fasternet_t1', 'fasternet_t2', 'fastvit_ma36', 'fastvit_mci0', 'fastvit_mci1', 'fastvit_mci2', 'fastvit_mci3', 'fastvit_mci4', 'fastvit_s12', 'fastvit_sa12', 'fastvit_sa24', 'fastvit_sa36', 'fastvit_t8', 'fastvit_t12', 'fbnetc_100', 'fbnetv3_b', 'fbnetv3_d', 'fbnetv3_g', 'flexivit_base', 'flexivit_large', 'flexivit_small', 'focalnet_base_lrf', 'focalnet_base_srf', 'focalnet_huge_fl3', 'focalnet_huge_fl4', 'focalnet_large_fl3', 'focalnet_large_fl4', 'focalnet_small_lrf', 'focalnet_small_srf', 'focalnet_tiny_lrf', 'focalnet_tiny_srf', 'focalnet_xlarge_fl3', 'focalnet_xlarge_fl4', 'gc_efficientnetv2_rw_t', 'gcresnet33ts', 'gcresnet50t', 'gcresnext26ts', 'gcresnext50ts', 'gcvit_base', 'gcvit_small', 'gcvit_tiny', 'gcvit_xtiny', 'gcvit_xxtiny', 'gernet_l', 'gernet_m', 'gernet_s', 'ghostnet_050', 'ghostnet_100', 'ghostnet_130', 'ghostnetv2_100', 'ghostnetv2_130', 'ghostnetv2_160', 'ghostnetv3_050', 'ghostnetv3_100', 'ghostnetv3_130', 'ghostnetv3_160', 'gmixer_12_224', 'gmixer_24_224', 'gmlp_b16_224', 'gmlp_s16_224', 'gmlp_ti16_224', 'halo2botnet50ts_256', 'halonet26t', 'halonet50ts', 'halonet_h1', 'haloregnetz_b', 'hardcorenas_a', 'hardcorenas_b', 'hardcorenas_c', 'hardcorenas_d', 'hardcorenas_e', 'hardcorenas_f', 'hgnet_base', 'hgnet_small', 'hgnet_tiny', 'hgnetv2_b0', 'hgnetv2_b1', 'hgnetv2_b2', 'hgnetv2_b3', 'hgnetv2_b4', 'hgnetv2_b5', 'hgnetv2_b6', 'hiera_base_224', 'hiera_base_abswin_256', 'hiera_base_plus_224', 'hiera_huge_224', 'hiera_large_224', 'hiera_small_224', 'hiera_small_abswin_256', 'hiera_tiny_224', 'hieradet_small', 'hrnet_w18', 'hrnet_w18_small', 'hrnet_w18_small_v2', 'hrnet_w18_ssld', 'hrnet_w30', 'hrnet_w32', 'hrnet_w40', 'hrnet_w44', 'hrnet_w48', 'hrnet_w48_ssld', 'hrnet_w64', 'inception_next_atto', 'inception_next_base', 'inception_next_small', 'inception_next_tiny', 'inception_resnet_v2', 'inception_v3', 'inception_v4', 'lambda_resnet26rpt_256', 'lambda_resnet26t', 'lambda_resnet50ts', 'lamhalobotnet50ts_256', 'lcnet_035', 'lcnet_050', 'lcnet_075', 'lcnet_100', 'lcnet_150', 'legacy_senet154', 'legacy_seresnet18', 'legacy_seresnet34', 'legacy_seresnet50', 'legacy_seresnet101', 'legacy_seresnet152', 'legacy_seresnext26_32x4d', 'legacy_seresnext50_32x4d', 'legacy_seresnext101_32x4d', 'legacy_xception', 'levit_128', 'levit_128s', 'levit_192', 'levit_256', 'levit_256d', 'levit_384', 'levit_384_s8', 'levit_512', 'levit_512_s8', 'levit_512d', 'levit_conv_128', 'levit_conv_128s', 'levit_conv_192', 'levit_conv_256', 'levit_conv_256d', 'levit_conv_384', 'levit_conv_384_s8', 'levit_conv_512', 'levit_conv_512_s8', 'levit_conv_512d', 'mambaout_base', 'mambaout_base_plus_rw', 'mambaout_base_short_rw', 'mambaout_base_tall_rw', 'mambaout_base_wide_rw', 'mambaout_femto', 'mambaout_kobe', 'mambaout_small', 'mambaout_small_rw', 'mambaout_tiny', 'maxvit_base_tf_224', 'maxvit_base_tf_384', 'maxvit_base_tf_512', 'maxvit_large_tf_224', 'maxvit_large_tf_384', 'maxvit_large_tf_512', 'maxvit_nano_rw_256', 'maxvit_pico_rw_256', 'maxvit_rmlp_base_rw_224', 'maxvit_rmlp_base_rw_384', 'maxvit_rmlp_nano_rw_256', 'maxvit_rmlp_pico_rw_256', 'maxvit_rmlp_small_rw_224', 'maxvit_rmlp_small_rw_256', 'maxvit_rmlp_tiny_rw_256', 'maxvit_small_tf_224', 'maxvit_small_tf_384', 'maxvit_small_tf_512', 'maxvit_tiny_pm_256', 'maxvit_tiny_rw_224', 'maxvit_tiny_rw_256', 'maxvit_tiny_tf_224', 'maxvit_tiny_tf_384', 'maxvit_tiny_tf_512', 'maxvit_xlarge_tf_224', 'maxvit_xlarge_tf_384', 'maxvit_xlarge_tf_512', 'maxxvit_rmlp_nano_rw_256', 'maxxvit_rmlp_small_rw_256', 'maxxvit_rmlp_tiny_rw_256', 'maxxvitv2_nano_rw_256', 'maxxvitv2_rmlp_base_rw_224', 'maxxvitv2_rmlp_base_rw_384', 'maxxvitv2_rmlp_large_rw_224', 'mixer_b16_224', 'mixer_b32_224', 'mixer_l16_224', 'mixer_l32_224', 'mixer_s16_224', 'mixer_s32_224', 'mixnet_l', 'mixnet_m', 'mixnet_s', 'mixnet_xl', 'mixnet_xxl', 'mnasnet_050', 'mnasnet_075', 'mnasnet_100', 'mnasnet_140', 'mnasnet_small', 'mobilenet_edgetpu_100', 'mobilenet_edgetpu_v2_l', 'mobilenet_edgetpu_v2_m', 'mobilenet_edgetpu_v2_s', 'mobilenet_edgetpu_v2_xs', 'mobilenetv1_100', 'mobilenetv1_100h', 'mobilenetv1_125', 'mobilenetv2_035', 'mobilenetv2_050', 'mobilenetv2_075', 'mobilenetv2_100', 'mobilenetv2_110d', 'mobilenetv2_120d', 'mobilenetv2_140', 'mobilenetv3_large_075', 'mobilenetv3_large_100', 'mobilenetv3_large_150d', 'mobilenetv3_rw', 'mobilenetv3_small_050', 'mobilenetv3_small_075', 'mobilenetv3_small_100', 'mobilenetv4_conv_aa_large', 'mobilenetv4_conv_aa_medium', 'mobilenetv4_conv_blur_medium', 'mobilenetv4_conv_large', 'mobilenetv4_conv_medium', 'mobilenetv4_conv_small', 'mobilenetv4_conv_small_035', 'mobilenetv4_conv_small_050', 'mobilenetv4_hybrid_large', 'mobilenetv4_hybrid_large_075', 'mobilenetv4_hybrid_medium', 'mobilenetv4_hybrid_medium_075', 'mobilenetv5_300m', 'mobilenetv5_300m_enc', 'mobilenetv5_base', 'mobileone_s0', 'mobileone_s1', 'mobileone_s2', 'mobileone_s3', 'mobileone_s4', 'mobilevit_s', 'mobilevit_xs', 'mobilevit_xxs', 'mobilevitv2_050', 'mobilevitv2_075', 'mobilevitv2_100', 'mobilevitv2_125', 'mobilevitv2_150', 'mobilevitv2_175', 'mobilevitv2_200', 'mvitv2_base', 'mvitv2_base_cls', 'mvitv2_huge_cls', 'mvitv2_large', 'mvitv2_large_cls', 'mvitv2_small', 'mvitv2_small_cls', 'mvitv2_tiny', 'naflexvit_base_patch16_gap', 'naflexvit_base_patch16_map', 'naflexvit_base_patch16_par_gap', 'naflexvit_base_patch16_parfac_gap', 'naflexvit_base_patch16_siglip', 'naflexvit_so150m2_patch16_reg1_gap', 'naflexvit_so150m2_patch16_reg1_map', 'naflexvit_so400m_patch16_siglip', 'nasnetalarge', 'nest_base', 'nest_base_jx', 'nest_small', 'nest_small_jx', 'nest_tiny', 'nest_tiny_jx', 'nextvit_base', 'nextvit_large', 'nextvit_small', 'nf_ecaresnet26', 'nf_ecaresnet50', 'nf_ecaresnet101', 'nf_regnet_b0', 'nf_regnet_b1', 'nf_regnet_b2', 'nf_regnet_b3', 'nf_regnet_b4', 'nf_regnet_b5', 'nf_resnet26', 'nf_resnet50', 'nf_resnet101', 'nf_seresnet26', 'nf_seresnet50', 'nf_seresnet101', 'nfnet_f0', 'nfnet_f1', 'nfnet_f2', 'nfnet_f3', 'nfnet_f4', 'nfnet_f5', 'nfnet_f6', 'nfnet_f7', 'nfnet_l0', 'pit_b_224', 'pit_b_distilled_224', 'pit_s_224', 'pit_s_distilled_224', 'pit_ti_224', 'pit_ti_distilled_224', 'pit_xs_224', 'pit_xs_distilled_224', 'pnasnet5large', 'poolformer_m36', 'poolformer_m48', 'poolformer_s12', 'poolformer_s24', 'poolformer_s36', 'poolformerv2_m36', 'poolformerv2_m48', 'poolformerv2_s12', 'poolformerv2_s24', 'poolformerv2_s36', 'prithvi_swin_B', 'prithvi_swin_L', 'pvt_v2_b0', 'pvt_v2_b1', 'pvt_v2_b2', 'pvt_v2_b2_li', 'pvt_v2_b3', 'pvt_v2_b4', 'pvt_v2_b5', 'rdnet_base', 'rdnet_large', 'rdnet_small', 'rdnet_tiny', 'regnetv_040', 'regnetv_064', 'regnetx_002', 'regnetx_004', 'regnetx_004_tv', 'regnetx_006', 'regnetx_008', 'regnetx_016', 'regnetx_032', 'regnetx_040', 'regnetx_064', 'regnetx_080', 'regnetx_120', 'regnetx_160', 'regnetx_320', 'regnety_002', 'regnety_004', 'regnety_006', 'regnety_008', 'regnety_008_tv', 'regnety_016', 'regnety_032', 'regnety_040', 'regnety_040_sgn', 'regnety_064', 'regnety_080', 'regnety_080_tv', 'regnety_120', 'regnety_160', 'regnety_320', 'regnety_640', 'regnety_1280', 'regnety_2560', 'regnetz_005', 'regnetz_040', 'regnetz_040_h', 'regnetz_b16', 'regnetz_b16_evos', 'regnetz_c16', 'regnetz_c16_evos', 'regnetz_d8', 'regnetz_d8_evos', 'regnetz_d32', 'regnetz_e8', 'repghostnet_050', 'repghostnet_058', 'repghostnet_080', 'repghostnet_100', 'repghostnet_111', 'repghostnet_130', 'repghostnet_150', 'repghostnet_200', 'repvgg_a0', 'repvgg_a1', 'repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'repvgg_d2se', 'repvit_m0_9', 'repvit_m1', 'repvit_m1_0', 'repvit_m1_1', 'repvit_m1_5', 'repvit_m2', 'repvit_m2_3', 'repvit_m3', 'res2net50_14w_8s', 'res2net50_26w_4s', 'res2net50_26w_6s', 'res2net50_26w_8s', 'res2net50_48w_2s', 'res2net50d', 'res2net101_26w_4s', 'res2net101d', 'res2next50', 'resmlp_12_224', 'resmlp_24_224', 'resmlp_36_224', 'resmlp_big_24_224', 'resnest14d', 'resnest26d', 'resnest50d', 'resnest50d_1s4x24d', 'resnest50d_4s2x40d', 'resnest101e', 'resnest200e', 'resnest269e', 'resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34', 'resnet34d', 'resnet50', 'resnet50_clip', 'resnet50_clip_gap', 'resnet50_gn', 'resnet50_mlp', 'resnet50c', 'resnet50d', 'resnet50s', 'resnet50t', 'resnet50x4_clip', 'resnet50x4_clip_gap', 'resnet50x16_clip', 'resnet50x16_clip_gap', 'resnet50x64_clip', 'resnet50x64_clip_gap', 'resnet51q', 'resnet61q', 'resnet101', 'resnet101_clip', 'resnet101_clip_gap', 'resnet101c', 'resnet101d', 'resnet101s', 'resnet152', 'resnet152c', 'resnet152d', 'resnet152s', 'resnet200', 'resnet200d', 'resnetaa34d', 'resnetaa50', 'resnetaa50d', 'resnetaa101d', 'resnetblur18', 'resnetblur50', 'resnetblur50d', 'resnetblur101d', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420', 'resnetv2_18', 'resnetv2_18d', 'resnetv2_34', 'resnetv2_34d', 'resnetv2_50', 'resnetv2_50d', 'resnetv2_50d_evos', 'resnetv2_50d_frn', 'resnetv2_50d_gn', 'resnetv2_50t', 'resnetv2_50x1_bit', 'resnetv2_50x3_bit', 'resnetv2_101', 'resnetv2_101d', 'resnetv2_101x1_bit', 'resnetv2_101x3_bit', 'resnetv2_152', 'resnetv2_152d', 'resnetv2_152x2_bit', 'resnetv2_152x4_bit', 'resnext26ts', 'resnext50_32x4d', 'resnext50d_32x4d', 'resnext101_32x4d', 'resnext101_32x8d', 'resnext101_32x16d', 'resnext101_32x32d', 'resnext101_64x4d', 'rexnet_100', 'rexnet_130', 'rexnet_150', 'rexnet_200', 'rexnet_300', 'rexnetr_100', 'rexnetr_130', 'rexnetr_150', 'rexnetr_200', 'rexnetr_300', 'sam2_hiera_base_plus', 'sam2_hiera_large', 'sam2_hiera_small', 'sam2_hiera_tiny', 'samvit_base_patch16', 'samvit_base_patch16_224', 'samvit_huge_patch16', 'samvit_large_patch16', 'sebotnet33ts_256', 'sedarknet21', 'sehalonet33ts', 'selecsls42', 'selecsls42b', 'selecsls60', 'selecsls60b', 'selecsls84', 'semnasnet_050', 'semnasnet_075', 'semnasnet_100', 'semnasnet_140', 'senet154', 'sequencer2d_l', 'sequencer2d_m', 'sequencer2d_s', 'seresnet18', 'seresnet33ts', 'seresnet34', 'seresnet50', 'seresnet50t', 'seresnet101', 'seresnet152', 'seresnet152d', 'seresnet200d', 'seresnet269d', 'seresnetaa50d', 'seresnext26d_32x4d', 'seresnext26t_32x4d', 'seresnext26ts', 'seresnext50_32x4d', 'seresnext101_32x4d', 'seresnext101_32x8d', 'seresnext101_64x4d', 'seresnext101d_32x8d', 'seresnextaa101d_32x8d', 'seresnextaa201d_32x8d', 'shvit_s1', 'shvit_s2', 'shvit_s3', 'shvit_s4', 'skresnet18', 'skresnet34', 'skresnet50', 'skresnet50d', 'skresnext50_32x4d', 'spnasnet_100', 'starnet_s1', 'starnet_s2', 'starnet_s3', 'starnet_s4', 'starnet_s050', 'starnet_s100', 'starnet_s150', 'swiftformer_l1', 'swiftformer_l3', 'swiftformer_s', 'swiftformer_xs', 'swin_base_patch4_window7_224', 'swin_base_patch4_window12_384', 'swin_large_patch4_window7_224', 'swin_large_patch4_window12_384', 'swin_s3_base_224', 'swin_s3_small_224', 'swin_s3_tiny_224', 'swin_small_patch4_window7_224', 'swin_tiny_patch4_window7_224', 'swinv2_base_window8_256', 'swinv2_base_window12_192', 'swinv2_base_window12to16_192to256', 'swinv2_base_window12to24_192to384', 'swinv2_base_window16_256', 'swinv2_cr_base_224', 'swinv2_cr_base_384', 'swinv2_cr_base_ns_224', 'swinv2_cr_giant_224', 'swinv2_cr_giant_384', 'swinv2_cr_huge_224', 'swinv2_cr_huge_384', 'swinv2_cr_large_224', 'swinv2_cr_large_384', 'swinv2_cr_small_224', 'swinv2_cr_small_384', 'swinv2_cr_small_ns_224', 'swinv2_cr_small_ns_256', 'swinv2_cr_tiny_224', 'swinv2_cr_tiny_384', 'swinv2_cr_tiny_ns_224', 'swinv2_large_window12_192', 'swinv2_large_window12to16_192to256', 'swinv2_large_window12to24_192to384', 'swinv2_small_window8_256', 'swinv2_small_window16_256', 'swinv2_tiny_window8_256', 'swinv2_tiny_window16_256', 'test_byobnet', 'test_convnext', 'test_convnext2', 'test_convnext3', 'test_efficientnet', 'test_efficientnet_evos', 'test_efficientnet_gn', 'test_efficientnet_ln', 'test_mambaout', 'test_nfnet', 'test_resnet', 'test_vit', 'test_vit2', 'test_vit3', 'test_vit4', 'tf_efficientnet_b0', 'tf_efficientnet_b1', 'tf_efficientnet_b2', 'tf_efficientnet_b3', 'tf_efficientnet_b4', 'tf_efficientnet_b5', 'tf_efficientnet_b6', 'tf_efficientnet_b7', 'tf_efficientnet_b8', 'tf_efficientnet_cc_b0_4e', 'tf_efficientnet_cc_b0_8e', 'tf_efficientnet_cc_b1_8e', 'tf_efficientnet_el', 'tf_efficientnet_em', 'tf_efficientnet_es', 'tf_efficientnet_l2', 'tf_efficientnet_lite0', 'tf_efficientnet_lite1', 'tf_efficientnet_lite2', 'tf_efficientnet_lite3', 'tf_efficientnet_lite4', 'tf_efficientnetv2_b0', 'tf_efficientnetv2_b1', 'tf_efficientnetv2_b2', 'tf_efficientnetv2_b3', 'tf_efficientnetv2_l', 'tf_efficientnetv2_m', 'tf_efficientnetv2_s', 'tf_efficientnetv2_xl', 'tf_mixnet_l', 'tf_mixnet_m', 'tf_mixnet_s', 'tf_mobilenetv3_large_075', 'tf_mobilenetv3_large_100', 'tf_mobilenetv3_large_minimal_100', 'tf_mobilenetv3_small_075', 'tf_mobilenetv3_small_100', 'tf_mobilenetv3_small_minimal_100', 'tiny_vit_5m_224', 'tiny_vit_11m_224', 'tiny_vit_21m_224', 'tiny_vit_21m_384', 'tiny_vit_21m_512', 'tinynet_a', 'tinynet_b', 'tinynet_c', 'tinynet_d', 'tinynet_e', 'tnt_b_patch16_224', 'tnt_s_legacy_patch16_224', 'tnt_s_patch16_224', 'tresnet_l', 'tresnet_m', 'tresnet_v2_l', 'tresnet_xl', 'twins_pcpvt_base', 'twins_pcpvt_large', 'twins_pcpvt_small', 'twins_svt_base', 'twins_svt_large', 'twins_svt_small', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'visformer_small', 'visformer_tiny', 'vit_7b_patch16_dinov3', 'vit_base_mci_224', 'vit_base_patch8_224', 'vit_base_patch14_dinov2', 'vit_base_patch14_reg4_dinov2', 'vit_base_patch16_18x2_224', 'vit_base_patch16_224', 'vit_base_patch16_224_miil', 'vit_base_patch16_384', 'vit_base_patch16_clip_224', 'vit_base_patch16_clip_384', 'vit_base_patch16_clip_quickgelu_224', 'vit_base_patch16_dinov3', 'vit_base_patch16_dinov3_qkvb', 'vit_base_patch16_gap_224', 'vit_base_patch16_plus_240', 'vit_base_patch16_plus_clip_240', 'vit_base_patch16_reg4_gap_256', 'vit_base_patch16_rope_224', 'vit_base_patch16_rope_ape_224', 'vit_base_patch16_rope_mixed_224', 'vit_base_patch16_rope_mixed_ape_224', 'vit_base_patch16_rope_reg1_gap_256', 'vit_base_patch16_rpn_224', 'vit_base_patch16_siglip_224', 'vit_base_patch16_siglip_256', 'vit_base_patch16_siglip_384', 'vit_base_patch16_siglip_512', 'vit_base_patch16_siglip_gap_224', 'vit_base_patch16_siglip_gap_256', 'vit_base_patch16_siglip_gap_384', 'vit_base_patch16_siglip_gap_512', 'vit_base_patch16_xp_224', 'vit_base_patch32_224', 'vit_base_patch32_384', 'vit_base_patch32_clip_224', 'vit_base_patch32_clip_256', 'vit_base_patch32_clip_384', 'vit_base_patch32_clip_448', 'vit_base_patch32_clip_quickgelu_224', 'vit_base_patch32_plus_256', 'vit_base_patch32_siglip_256', 'vit_base_patch32_siglip_gap_256', 'vit_base_r26_s32_224', 'vit_base_r50_s16_224', 'vit_base_r50_s16_384', 'vit_base_resnet26d_224', 'vit_base_resnet50d_224', 'vit_betwixt_patch16_gap_256', 'vit_betwixt_patch16_reg1_gap_256', 'vit_betwixt_patch16_reg4_gap_256', 'vit_betwixt_patch16_reg4_gap_384', 'vit_betwixt_patch16_rope_reg4_gap_256', 'vit_betwixt_patch32_clip_224', 'vit_giant_patch14_224', 'vit_giant_patch14_clip_224', 'vit_giant_patch14_dinov2', 'vit_giant_patch14_reg4_dinov2', 'vit_giant_patch16_gap_224', 'vit_giantopt_patch16_siglip_256', 'vit_giantopt_patch16_siglip_384', 'vit_giantopt_patch16_siglip_gap_256', 'vit_giantopt_patch16_siglip_gap_384', 'vit_gigantic_patch14_224', 'vit_gigantic_patch14_clip_224', 'vit_gigantic_patch14_clip_378', 'vit_gigantic_patch14_clip_quickgelu_224', 'vit_huge_patch14_224', 'vit_huge_patch14_clip_224', 'vit_huge_patch14_clip_336', 'vit_huge_patch14_clip_378', 'vit_huge_patch14_clip_quickgelu_224', 'vit_huge_patch14_clip_quickgelu_378', 'vit_huge_patch14_gap_224', 'vit_huge_patch14_xp_224', 'vit_huge_patch16_gap_448', 'vit_huge_plus_patch16_dinov3', 'vit_huge_plus_patch16_dinov3_qkvb', 'vit_intern300m_patch14_448', 'vit_large_patch14_224', 'vit_large_patch14_clip_224', 'vit_large_patch14_clip_336', 'vit_large_patch14_clip_quickgelu_224', 'vit_large_patch14_clip_quickgelu_336', 'vit_large_patch14_dinov2', 'vit_large_patch14_reg4_dinov2', 'vit_large_patch14_xp_224', 'vit_large_patch16_224', 'vit_large_patch16_384', 'vit_large_patch16_dinov3', 'vit_large_patch16_dinov3_qkvb', 'vit_large_patch16_rope_224', 'vit_large_patch16_rope_ape_224', 'vit_large_patch16_rope_mixed_224', 'vit_large_patch16_rope_mixed_ape_224', 'vit_large_patch16_siglip_256', 'vit_large_patch16_siglip_384', 'vit_large_patch16_siglip_512', 'vit_large_patch16_siglip_gap_256', 'vit_large_patch16_siglip_gap_384', 'vit_large_patch16_siglip_gap_512', 'vit_large_patch32_224', 'vit_large_patch32_384', 'vit_large_r50_s32_224', 'vit_large_r50_s32_384', 'vit_little_patch16_reg1_gap_256', 'vit_little_patch16_reg4_gap_256', 'vit_medium_patch16_clip_224', 'vit_medium_patch16_gap_240', 'vit_medium_patch16_gap_256', 'vit_medium_patch16_gap_384', 'vit_medium_patch16_reg1_gap_256', 'vit_medium_patch16_reg4_gap_256', 'vit_medium_patch16_rope_reg1_gap_256', 'vit_medium_patch32_clip_224', 'vit_mediumd_patch16_reg4_gap_256', 'vit_mediumd_patch16_reg4_gap_384', 'vit_mediumd_patch16_rope_reg1_gap_256', 'vit_pe_core_base_patch16_224', 'vit_pe_core_gigantic_patch14_448', 'vit_pe_core_large_patch14_336', 'vit_pe_core_small_patch16_384', 'vit_pe_core_tiny_patch16_384', 'vit_pe_lang_gigantic_patch14_448', 'vit_pe_lang_large_patch14_448', 'vit_pe_spatial_base_patch16_512', 'vit_pe_spatial_gigantic_patch14_448', 'vit_pe_spatial_large_patch14_448', 'vit_pe_spatial_small_patch16_512', 'vit_pe_spatial_tiny_patch16_512', 'vit_pwee_patch16_reg1_gap_256', 'vit_relpos_base_patch16_224', 'vit_relpos_base_patch16_cls_224', 'vit_relpos_base_patch16_clsgap_224', 'vit_relpos_base_patch16_plus_240', 'vit_relpos_base_patch16_rpn_224', 'vit_relpos_base_patch32_plus_rpn_256', 'vit_relpos_medium_patch16_224', 'vit_relpos_medium_patch16_cls_224', 'vit_relpos_medium_patch16_rpn_224', 'vit_relpos_small_patch16_224', 'vit_relpos_small_patch16_rpn_224', 'vit_small_patch8_224', 'vit_small_patch14_dinov2', 'vit_small_patch14_reg4_dinov2', 'vit_small_patch16_18x2_224', 'vit_small_patch16_36x1_224', 'vit_small_patch16_224', 'vit_small_patch16_384', 'vit_small_patch16_dinov3', 'vit_small_patch16_dinov3_qkvb', 'vit_small_patch16_rope_224', 'vit_small_patch16_rope_ape_224', 'vit_small_patch16_rope_mixed_224', 'vit_small_patch16_rope_mixed_ape_224', 'vit_small_patch32_224', 'vit_small_patch32_384', 'vit_small_plus_patch16_dinov3', 'vit_small_plus_patch16_dinov3_qkvb', 'vit_small_r26_s32_224', 'vit_small_r26_s32_384', 'vit_small_resnet26d_224', 'vit_small_resnet50d_s16_224', 'vit_so150m2_patch16_reg1_gap_256', 'vit_so150m2_patch16_reg1_gap_384', 'vit_so150m2_patch16_reg1_gap_448', 'vit_so150m_patch16_reg4_gap_256', 'vit_so150m_patch16_reg4_gap_384', 'vit_so150m_patch16_reg4_map_256', 'vit_so400m_patch14_siglip_224', 'vit_so400m_patch14_siglip_378', 'vit_so400m_patch14_siglip_384', 'vit_so400m_patch14_siglip_gap_224', 'vit_so400m_patch14_siglip_gap_378', 'vit_so400m_patch14_siglip_gap_384', 'vit_so400m_patch14_siglip_gap_448', 'vit_so400m_patch14_siglip_gap_896', 'vit_so400m_patch16_siglip_256', 'vit_so400m_patch16_siglip_384', 'vit_so400m_patch16_siglip_512', 'vit_so400m_patch16_siglip_gap_256', 'vit_so400m_patch16_siglip_gap_384', 'vit_so400m_patch16_siglip_gap_512', 'vit_srelpos_medium_patch16_224', 'vit_srelpos_small_patch16_224', 'vit_tiny_patch16_224', 'vit_tiny_patch16_384', 'vit_tiny_r_s16_p8_224', 'vit_tiny_r_s16_p8_384', 'vit_wee_patch16_reg1_gap_256', 'vit_xsmall_patch16_clip_224', 'vitamin_base_224', 'vitamin_large2_224', 'vitamin_large2_256', 'vitamin_large2_336', 'vitamin_large2_384', 'vitamin_large_224', 'vitamin_large_256', 'vitamin_large_336', 'vitamin_large_384', 'vitamin_small_224', 'vitamin_xlarge_256', 'vitamin_xlarge_336', 'vitamin_xlarge_384', 'volo_d1_224', 'volo_d1_384', 'volo_d2_224', 'volo_d2_384', 'volo_d3_224', 'volo_d3_448', 'volo_d4_224', 'volo_d4_448', 'volo_d5_224', 'volo_d5_448', 'volo_d5_512', 'vovnet39a', 'vovnet57a', 'wide_resnet50_2', 'wide_resnet101_2', 'xception41', 'xception41p', 'xception65', 'xception65p', 'xception71', 'xcit_large_24_p8_224', 'xcit_large_24_p8_384', 'xcit_large_24_p16_224', 'xcit_large_24_p16_384', 'xcit_medium_24_p8_224', 'xcit_medium_24_p8_384', 'xcit_medium_24_p16_224', 'xcit_medium_24_p16_384', 'xcit_nano_12_p8_224', 'xcit_nano_12_p8_384', 'xcit_nano_12_p16_224', 'xcit_nano_12_p16_384', 'xcit_small_12_p8_224', 'xcit_small_12_p8_384', 'xcit_small_12_p16_224', 'xcit_small_12_p16_384', 'xcit_small_24_p8_224', 'xcit_small_24_p8_384', 'xcit_small_24_p16_224', 'xcit_small_24_p16_384', 'xcit_tiny_12_p8_224', 'xcit_tiny_12_p8_384', 'xcit_tiny_12_p16_224', 'xcit_tiny_12_p16_384', 'xcit_tiny_24_p8_224', 'xcit_tiny_24_p8_384', 'xcit_tiny_24_p16_224', 'xcit_tiny_24_p16_384']</code></pre>
</div>
</div>
<div id="cef10b90" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's just list out the Prithvi models in the terratorch source</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([mod <span class="cf">for</span> mod <span class="kw">in</span> BACKBONE_REGISTRY._sources[<span class="st">'terratorch'</span>] <span class="cf">if</span> <span class="st">'prithvi'</span> <span class="kw">in</span> mod])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['prithvi_eo_tiny', 'prithvi_eo_v1_100', 'prithvi_eo_v2_tiny_tl', 'prithvi_eo_v2_100_tl', 'prithvi_eo_v2_300', 'prithvi_eo_v2_600', 'prithvi_eo_v2_300_tl', 'prithvi_eo_v2_600_tl']</code></pre>
</div>
</div>
<p>There are many different prithvi versions to select from ‚Äì for this demo we will choose the smaller <code>prithvi_eo_v1_100</code>.</p>
</section>
<section id="explore-decoders" class="level4">
<h4 class="anchored" data-anchor-id="explore-decoders">Explore decoders</h4>
<div id="3facf519" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.registry <span class="im">import</span> DECODER_REGISTRY</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Check available decoders</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, source <span class="kw">in</span> DECODER_REGISTRY._sources.items():</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">====</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">====="</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">list</span>(source))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
====terratorch=====
['ASPPModule', 'ASPPHead', 'ASPPSegmentationHead', 'ASPPRegressionHead', 'FCNDecoder', 'IdentityDecoder', 'LinearDecoder', 'MLPDecoder', 'SatMAEHead', 'UNetDecoder', 'UperNetDecoder']

====smp=====
['DeepLabV3Plus', 'FPN', 'PSPNet', 'UPerNet', 'Segformer', 'Linknet', 'PAN', 'DeepLabV3', 'Unet', 'UnetPlusPlus', 'MAnet']</code></pre>
</div>
</div>
<p>We choose the FCNDecoder because it is good for Classification tasks.</p>
</section>
</section>
<section id="define-your-task" class="level3">
<h3 class="anchored" data-anchor-id="define-your-task">Define your Task</h3>
<div id="2823af93" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.tasks <span class="im">import</span> ClassificationTask</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> ClassificationTask(</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define your model</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    model_factory<span class="op">=</span><span class="st">"EncoderDecoderFactory"</span>, <span class="co"># TerraTorch's EncoderDecoderFactory, where we found our models</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    model_args<span class="op">=</span>{</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">'backbone'</span>: <span class="st">'prithvi_eo_v1_100'</span>, <span class="co"># Smaller Prithvi model for demo</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">'backbone_pretrained'</span>: <span class="va">True</span>, <span class="co"># Train from scratch or use pre-trained weights?</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">'decoder'</span>: <span class="st">'FCNDecoder'</span>, <span class="co"># Chosen decoder for classification</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'num_classes'</span>: <span class="bu">len</span>(class_names),</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># What would you like to train?</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    freeze_backbone<span class="op">=</span><span class="va">True</span>, <span class="co"># Do not update prithvi weights for demo</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    freeze_decoder<span class="op">=</span><span class="va">False</span>, <span class="co"># Train the decoder</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optionally, change hyperparameters from the defaults</span></span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'ce'</span>,</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">1e-4</span></span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:root:Decoder FCNDecoder does not have an `includes_head` attribute. Falling back to the value of the registry.</code></pre>
</div>
</div>
</section>
</section>
<section id="step-4-train-your-model" class="level2">
<h2 class="anchored" data-anchor-id="step-4-train-your-model">Step 4: Train your model</h2>
<p>Now that we have our Task and DataModule, we can easily use PyTorch Lightning to train our model.</p>
<p>We also use Weights and Biases to check our progress during training.</p>
<div id="c0a851c1" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightning.pytorch.loggers <span class="im">import</span> WandbLogger</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightning.pytorch <span class="im">import</span> Trainer</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightning.pytorch.callbacks <span class="im">import</span> EarlyStopping, ModelCheckpoint, RichProgressBar, LearningRateMonitor</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Auto-detect best precision based on available hardware</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> <span class="st">"16-mixed"</span>  <span class="co"># CUDA supports mixed precision well</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> torch.backends.mps.is_available():</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> <span class="st">"32"</span>  <span class="co"># MPS has limited 16-bit support</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> <span class="st">"32"</span>  <span class="co"># CPU doesn't benefit from mixed precision</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize callbacks</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the best and the last model</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>checkpoint_callback <span class="op">=</span> ModelCheckpoint(</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span>task.monitor, save_top_k<span class="op">=</span><span class="dv">1</span>, save_last<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Stop training if validation metrics stop improving for a certain number of epochs</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>early_stopping_callback <span class="op">=</span> EarlyStopping(</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span>task.monitor, min_delta<span class="op">=</span><span class="fl">0.00</span>, patience<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set this to None so we can load the best model from checkpoint later</span></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>checkpoint_callback.best_model_path <span class="op">=</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can define our Trainer.</p>
<div id="0f765e7d" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check your progress and results during training using Weights and Biases.</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># You'll have to make an account but it's very useful.</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Just click the link it gives you to watch your training progress plotted in real time.</span></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>wandb_logger <span class="op">=</span> WandbLogger(log_model<span class="op">=</span><span class="st">"all"</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define your Trainer</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    accelerator<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    devices<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Number of GPUs. Interactive mode recommended with 1 device</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>    precision<span class="op">=</span>precision,  <span class="co"># Mac MPS has limited support for 16-bit floats</span></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># precision="16-mixed", # Use 16-bit floats as opposed to 32-bit (higher precision) when it's safe to save on time and memory</span></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        RichProgressBar(),</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        checkpoint_callback,</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        early_stopping_callback,</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        LearningRateMonitor(logging_interval<span class="op">=</span><span class="st">"epoch"</span>),</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    logger<span class="op">=</span>wandb_logger,</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    max_epochs<span class="op">=</span><span class="dv">100</span>,  <span class="co"># Train for up to 100 epochs</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a>    default_root_dir<span class="op">=</span><span class="st">'output/tutorial'</span>,  <span class="co"># where checkpoints/logs go.</span></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>    log_every_n_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    check_val_every_n_epoch<span class="op">=</span><span class="dv">1</span>  <span class="co"># How frequently to calcualte validation performance</span></span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit your model!</span></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a><span class="co"># _ = trainer.fit(model=task, datamodule=datamodule)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-5-run-inference-with-your-trained-model" class="level2">
<h2 class="anchored" data-anchor-id="step-5-run-inference-with-your-trained-model">Step 5: Run inference with your trained model</h2>
<p>Now that we have a trained model, let‚Äôs see how to use it for predictions on new images.</p>
<section id="load-the-best-checkpoint" class="level3">
<h3 class="anchored" data-anchor-id="load-the-best-checkpoint">Load the best checkpoint</h3>
<p>After training, the best model is saved. Let‚Äôs load it.</p>
<p>Note: If you just trained your model, the checkpoint_callback.best_model_path should already be set to the path of the best model. If you didn‚Äôt run the training step, you will need to set the best_model_path to the path of the best model.</p>
<div id="6a1af626" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the best model from checkpoint</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> checkpoint_callback.best_model_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If the best model path is not None, use it</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    best_model_path <span class="op">=</span> checkpoint_callback.best_model_path</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If the best model path is None, use a default path (must already exist!)</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    best_model_path <span class="op">=</span> <span class="st">'lightning_logs/ym9pbv87/checkpoints/epoch=51-step=156.ckpt'</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best model saved at: </span><span class="sc">{</span>best_model_path<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best model saved at: lightning_logs/ym9pbv87/checkpoints/epoch=51-step=156.ckpt</code></pre>
</div>
</div>
<div id="e85a01fc" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model for inference</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.tasks <span class="im">import</span> ClassificationTask</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model for inference using the ClassificationTask class</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the same class we used to train the model</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>trained_task <span class="op">=</span> ClassificationTask.load_from_checkpoint(best_model_path)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the model to evaluation mode</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="co"># This is important to ensure the model is in the correct mode for inference</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>trained_task.<span class="bu">eval</span>() <span class="co"># set to evaluation mode</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:root:Decoder FCNDecoder does not have an `includes_head` attribute. Falling back to the value of the registry.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>ClassificationTask(
  (model): ScalarOutputModel(
    (encoder): PrithviViT(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(6, 768, kernel_size=(1, 16, 16), stride=(1, 16, 16))
        (norm): Identity()
      )
      (blocks): ModuleList(
        (0-11): 12 x Block(
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): FCNDecoder(
      (convs): Sequential(
        (0): Sequential(
          (0): ConvTranspose2d(768, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): Norm2d(
            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          )
          (3): GELU(approximate='none')
        )
        (1): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): Norm2d(
            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          )
          (3): GELU(approximate='none')
        )
        (2): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): Norm2d(
            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          )
          (3): GELU(approximate='none')
        )
        (3): Sequential(
          (0): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (2): Norm2d(
            (ln): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          )
          (3): GELU(approximate='none')
        )
      )
    )
    (head): ClassificationHead(
      (head): Sequential(
        (0): Identity()
        (1): Identity()
        (2): Linear(in_features=256, out_features=10, bias=True)
      )
    )
    (aux_heads): ModuleDict()
  )
  (criterion): CrossEntropyLoss()
  (train_metrics): MetricCollection(
    (Accuracy): MulticlassAccuracy()
    (Accuracy_Micro): MulticlassAccuracy()
    (Class_Accuracy): ClasswiseWrapper(
      (metric): MulticlassAccuracy()
    )
    (Class_F1): ClasswiseWrapper(
      (metric): MulticlassF1Score()
    )
    (F1_Score): MulticlassF1Score()
    (Precision): MulticlassPrecision()
    (Recall): MulticlassRecall(),
    prefix=train/
  )
  (val_metrics): MetricCollection(
    (Accuracy): MulticlassAccuracy()
    (Accuracy_Micro): MulticlassAccuracy()
    (Class_Accuracy): ClasswiseWrapper(
      (metric): MulticlassAccuracy()
    )
    (Class_F1): ClasswiseWrapper(
      (metric): MulticlassF1Score()
    )
    (F1_Score): MulticlassF1Score()
    (Precision): MulticlassPrecision()
    (Recall): MulticlassRecall(),
    prefix=val/
  )
  (test_metrics): ModuleList(
    (0): MetricCollection(
      (Accuracy): MulticlassAccuracy()
      (Accuracy_Micro): MulticlassAccuracy()
      (Class_Accuracy): ClasswiseWrapper(
        (metric): MulticlassAccuracy()
      )
      (Class_F1): ClasswiseWrapper(
        (metric): MulticlassF1Score()
      )
      (F1_Score): MulticlassF1Score()
      (Precision): MulticlassPrecision()
      (Recall): MulticlassRecall(),
      prefix=test/
    )
  )
)</code></pre>
</div>
</div>
</section>
<section id="get-some-test-images" class="level3">
<h3 class="anchored" data-anchor-id="get-some-test-images">Get some test images</h3>
</section>
<section id="run-predictions" class="level3">
<h3 class="anchored" data-anchor-id="run-predictions">Run predictions</h3>
<div id="13c0d61e" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a single batch from the validation dataloader (not yet transformed)</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>val_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(datamodule.val_dataloader()))</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Clone the images to avoid modifying the original data</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We will use these unnormalized images to visualize the predictions later</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>unnormalized_images <span class="op">=</span> val_batch[<span class="st">'image'</span>].clone()</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the same transforms used during training/validation</span></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="co"># This is important to ensure the images are in the same format as during training</span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>val_images <span class="op">=</span> datamodule.aug(val_batch)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the labels for the batch, which are the ground truth labels</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>val_labels <span class="op">=</span> val_batch[<span class="st">'label'</span>]</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="co"># CHeck stats on images to see if they are normalized</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image min: </span><span class="sc">{</span>val_images[<span class="st">'image'</span>]<span class="sc">.</span><span class="bu">min</span>()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image max: </span><span class="sc">{</span>val_images[<span class="st">'image'</span>]<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image mean: </span><span class="sc">{</span>val_images[<span class="st">'image'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image std: </span><span class="sc">{</span>val_images[<span class="st">'image'</span>]<span class="sc">.</span>std()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Now conduct inference using the transformed images:</span></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>trained_task.<span class="bu">eval</span>()</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> trained_task(val_images[<span class="st">'image'</span>])</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># outputs.output is a tensor of shape (batch_size, num_classes)</span></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We want to get the class with the highest probability</span></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># So we take the argmax of the output tensor</span></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This gives us the predicted class for each image in the batch</span></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> outputs.output.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Image min: -1.7845385074615479
Image max: 3.436638355255127
Image mean: -0.5142390131950378
Image std: 0.8566247820854187</code></pre>
</div>
</div>
<div id="eb3febd4" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Check accuracy on this batch</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> (predictions <span class="op">==</span> val_labels).<span class="bu">sum</span>().item()</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> correct <span class="op">/</span> <span class="bu">len</span>(val_labels)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted: </span><span class="sc">{</span>predictions<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"True:      </span><span class="sc">{</span>val_labels<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batch accuracy: 31.25%
Predicted: tensor([0, 5, 5, 1, 7, 2, 7, 7, 3, 3, 5, 0, 1, 6, 4, 8])
True:      tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7])</code></pre>
</div>
</div>
</section>
<section id="visualize-predictions" class="level3">
<h3 class="anchored" data-anchor-id="visualize-predictions">Visualize predictions</h3>
<div id="82f39084" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Pick first 4 images to visualize</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>n_images <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-define class names to match the order of the predictions and ground truth labels:</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"AnnualCrop"</span>,</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Forest"</span>,</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"HerbaceousVegetation"</span>,</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Highway"</span>,</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Industrial"</span>,</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Pasture"</span>,</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"PermanentCrop"</span>,</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Residential"</span>,</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"River"</span>,</span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"SeaLake"</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, n_images, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_images):</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># val_images['images'] is now a [batch, 6, 224, 224] tensor</span></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the i-th 6-channel image</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>    img_numpy <span class="op">=</span> unnormalized_images[i].cpu().numpy()</span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to_rgb expects a 6-channel CHW numpy image as input</span></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> to_rgb(img_numpy, bands<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>    axes[i].imshow(rgb)</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>    true_label <span class="op">=</span> class_names[val_labels[i]]</span>
<span id="cb43-33"><a href="#cb43-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># predictions is used for predicted classes</span></span>
<span id="cb43-34"><a href="#cb43-34" aria-hidden="true" tabindex="-1"></a>    pred_label <span class="op">=</span> class_names[predictions[i]]</span>
<span id="cb43-35"><a href="#cb43-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-36"><a href="#cb43-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Color code: green if correct, red if wrong</span></span>
<span id="cb43-37"><a href="#cb43-37" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> <span class="st">'green'</span> <span class="cf">if</span> val_labels[i] <span class="op">==</span> predictions[i] <span class="cf">else</span> <span class="st">'red'</span></span>
<span id="cb43-38"><a href="#cb43-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-39"><a href="#cb43-39" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(<span class="ss">f"True: </span><span class="sc">{</span>true_label<span class="sc">}</span><span class="ch">\n</span><span class="ss">Pred: </span><span class="sc">{</span>pred_label<span class="sc">}</span><span class="ss">"</span>, color<span class="op">=</span>color)</span>
<span id="cb43-40"><a href="#cb43-40" aria-hidden="true" tabindex="-1"></a>    axes[i].axis(<span class="st">'off'</span>)</span>
<span id="cb43-41"><a href="#cb43-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-42"><a href="#cb43-42" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb43-43"><a href="#cb43-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="terratorch_finetuning_workflow_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="get-prediction-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="get-prediction-probabilities">Get prediction probabilities</h3>
<div id="7ed627cd" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the `predict` function to get predictions and probabilities</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Make sure to apply the same transforms to the image as during training</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the validation dataloader to get predictions and probabilities for a batch</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> datamodule.val_dataloader()</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get a single batch from the validation dataloader (not yet transformed)</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    val_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(val_loader))</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply the same transforms used during training/validation</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>    transformed_batch <span class="op">=</span> datamodule.aug(val_batch)</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Take first image, keep batch dimension [1, C, H, W]</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    transformed_image <span class="op">=</span> transformed_batch[<span class="st">'image'</span>][<span class="dv">0</span>:<span class="dv">1</span>]  <span class="co"># Now properly normalized</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    true_label <span class="op">=</span> val_batch[<span class="st">'label'</span>][<span class="dv">0</span>]</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions using forward pass</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>    trained_task.<span class="bu">eval</span>()</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> trained_task(transformed_image)</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> output.output</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predicted class</span></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> logits.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get probabilities using softmax</span></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a><span class="co"># `probs` is a tensor of shape (batch_size, num_classes)</span></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Look at the probabilities for the first image in the batch</span></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First image probabilities:"</span>)</span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, class_name <span class="kw">in</span> <span class="bu">enumerate</span>(class_names):</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>probs[<span class="dv">0</span>, i]<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>First image probabilities:
AnnualCrop: 0.752
Forest: 0.004
HerbaceousVegetation: 0.004
Highway: 0.134
Industrial: 0.004
Pasture: 0.010
PermanentCrop: 0.076
Residential: 0.004
River: 0.010
SeaLake: 0.003</code></pre>
</div>
</div>
</section>
</section>
<section id="bonus-exploring-model-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="bonus-exploring-model-embeddings">Bonus: Exploring model embeddings</h2>
<p>Let‚Äôs visualize the embeddings of the pretrained and fine-tuned models to see how the classification head might have learned to separate classes, even though the backbone embeddings remain unchanged.</p>
<section id="visualize-embedding-space-with-pca" class="level3">
<h3 class="anchored" data-anchor-id="visualize-embedding-space-with-pca">Visualize embedding space with PCA</h3>
<div id="706c209b" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Increase the number of points by using both validation and test images</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>all_embeddings <span class="op">=</span> []</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>all_labels <span class="op">=</span> []</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set up test dataset if available</span></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    datamodule.setup(<span class="st">'test'</span>)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    loaders <span class="op">=</span> [datamodule.val_dataloader(), datamodule.test_dataloader()]</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use validation (and test if available) sets for more points in PCA</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> loader <span class="kw">in</span> loaders:      </span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch <span class="kw">in</span> loader:</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Apply the same transforms used during training/validation</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>            transformed_batch <span class="op">=</span> datamodule.aug(batch)</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> transformed_batch[<span class="st">'image'</span>]  <span class="co"># Now properly normalized</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> batch[<span class="st">'label'</span>]</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a>            emb <span class="op">=</span> trained_task.model.encoder(images)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>            emb_flat <span class="op">=</span> emb.mean(dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Global average pooling</span></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a>            all_embeddings.append(emb_flat.cpu())</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>            all_labels.append(labels.cpu())</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a>all_embeddings <span class="op">=</span> torch.cat(all_embeddings).numpy()</span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>all_labels <span class="op">=</span> torch.cat(all_labels).numpy()</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA to 2D</span></span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a>embeddings_2d <span class="op">=</span> pca.fit_transform(all_embeddings)</span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot with class names as legend entries</span></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> plt.scatter(</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a>    embeddings_2d[:, <span class="dv">0</span>], embeddings_2d[:, <span class="dv">1</span>],</span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span>all_labels, cmap<span class="op">=</span><span class="st">'tab10'</span>, alpha<span class="op">=</span><span class="fl">0.6</span></span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Build legend mapping colors back to class names</span></span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.lines <span class="im">import</span> Line2D</span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a>handles <span class="op">=</span> []</span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a>unique_labels <span class="op">=</span> np.unique(all_labels)</span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> unique_labels:</span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> scatter.cmap(scatter.norm(label))</span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a>    handles.append(Line2D(</span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>        [<span class="dv">0</span>], [<span class="dv">0</span>], marker<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'w'</span>, label<span class="op">=</span>class_names[label],</span>
<span id="cb46-51"><a href="#cb46-51" aria-hidden="true" tabindex="-1"></a>        markerfacecolor<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">10</span>, alpha<span class="op">=</span><span class="fl">0.6</span></span>
<span id="cb46-52"><a href="#cb46-52" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb46-53"><a href="#cb46-53" aria-hidden="true" tabindex="-1"></a>plt.legend(handles<span class="op">=</span>handles, title<span class="op">=</span><span class="st">"Class"</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb46-54"><a href="#cb46-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-55"><a href="#cb46-55" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Validation &amp; Test Set Embeddings (PCA)</span><span class="ch">\n</span><span class="st">(same for pretrained &amp; fine-tuned)'</span>)</span>
<span id="cb46-56"><a href="#cb46-56" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'PC1'</span>)</span>
<span id="cb46-57"><a href="#cb46-57" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'PC2'</span>)</span>
<span id="cb46-58"><a href="#cb46-58" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="terratorch_finetuning_workflow_files/figure-html/cell-30-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kcaylor\.github\.io\/GEOG-288KC-geospatial-foundation-models");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb47" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Getting started with fine tuning with TerraTorch"</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> geoai</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>TerraTorch is a package that allows you to easily use and fine tune geospatial foundation models (GeoFMs). This tutorial walks through how to get started with TerraTorch to fine tune a GeoFM on your own data and task, whatever that may be.</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>Achieving fine tuning and inference using TerraTorch can be achieved in five steps:</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Organize your data:** Make sure your inputs, labels, and splits are organized in a standardized way that TerraTorch generic DataModules can interpret</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Set up your DataModule:** Use TerraTorch generic DataModules to create an object which reads in your data, makes any necessary adjustments, and passes it to your model in the format it expects for training.</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Set up your Task:** Identify your chosen task, choose your model and define hyperparameters</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Train your model:** Use PyTorch Lightning to handle training and logging for a set number of epochs.</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Run inference:** Once training is complete, use your fine-tuned model to make predictions on new data and inspect embeddings.</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>In this demo, we will show how to fine tune Prithvi on a subset of the EuroSAT benchmark, which classifies 64x64 pixel Sentinel 2 L1C images into one of 10 classes. The demo is written in a general way to help users adapt this code to their specific use case.</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## Step 0: Setup</span></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### HuggingFace cache</span></span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a>Because we are on a shared server with shared conda environment, we need to reorient where huggingface downloads things like model weights and datasets</span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pathlib</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> logging</span>
<span id="cb47-34"><a href="#cb47-34" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb47-35"><a href="#cb47-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-36"><a href="#cb47-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppress PIL warnings about multi-band TIFFs (common with satellite imagery)</span></span>
<span id="cb47-37"><a href="#cb47-37" aria-hidden="true" tabindex="-1"></a>logging.getLogger(<span class="st">'PIL.TiffImagePlugin'</span>).setLevel(logging.CRITICAL)</span>
<span id="cb47-38"><a href="#cb47-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-39"><a href="#cb47-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppress other common warnings</span></span>
<span id="cb47-40"><a href="#cb47-40" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>, category<span class="op">=</span><span class="pp">UserWarning</span>)</span>
<span id="cb47-41"><a href="#cb47-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-42"><a href="#cb47-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify where you want model weights/datasets to be saved</span></span>
<span id="cb47-43"><a href="#cb47-43" aria-hidden="true" tabindex="-1"></a>DATA_PATH <span class="op">=</span> <span class="st">'/Users/kellycaylor/dev/geoAI/data'</span></span>
<span id="cb47-44"><a href="#cb47-44" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-45"><a href="#cb47-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-48"><a href="#cb47-48" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-49"><a href="#cb47-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up environment variables that are used by HuggingFace</span></span>
<span id="cb47-50"><a href="#cb47-50" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"HF_HOME"</span>] <span class="op">=</span> os.path.join(DATA_PATH, <span class="st">"hfhome"</span>)</span>
<span id="cb47-51"><a href="#cb47-51" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"HF_HUB_CACHE"</span>] <span class="op">=</span> os.path.join(DATA_PATH, <span class="st">"hub"</span>)</span>
<span id="cb47-52"><a href="#cb47-52" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"HF_DATASETS_CACHE"</span>] <span class="op">=</span> os.path.join(DATA_PATH, <span class="st">"datasets"</span>)</span>
<span id="cb47-53"><a href="#cb47-53" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"TRANSFORMERS_CACHE"</span>] <span class="op">=</span> os.path.join(DATA_PATH, <span class="st">"transformers"</span>)</span>
<span id="cb47-54"><a href="#cb47-54" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-55"><a href="#cb47-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-56"><a href="#cb47-56" aria-hidden="true" tabindex="-1"></a><span class="fu">## Step 1: Get your data in the right format for TerraTorch</span></span>
<span id="cb47-57"><a href="#cb47-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-58"><a href="#cb47-58" aria-hidden="true" tabindex="-1"></a>TerraTorch makes your life easier if you organize your data in standard ways. Specifically, following these conventions will allow you to use their generic DataModules and save your the trouble of writing your own.</span>
<span id="cb47-59"><a href="#cb47-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-60"><a href="#cb47-60" aria-hidden="true" tabindex="-1"></a><span class="fu">### File organization</span></span>
<span id="cb47-61"><a href="#cb47-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-62"><a href="#cb47-62" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Classification:** Organize your images into different folders depending on their class (this is how EuroSAT is organized). Alternatively, have a metadata file listing</span>
<span id="cb47-63"><a href="#cb47-63" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Segmentation/regression:** Have images and their labels (masks or continuous outputs) have the same name but followed by an image vs. mask or label identifier (e.g. image: <span class="in">`Im_1_img.tif`</span> and <span class="in">`Im_1_mask.tif`</span></span>
<span id="cb47-64"><a href="#cb47-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-65"><a href="#cb47-65" aria-hidden="true" tabindex="-1"></a><span class="fu">#### EuroSAT classification example</span></span>
<span id="cb47-66"><a href="#cb47-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-67"><a href="#cb47-67" aria-hidden="true" tabindex="-1"></a>We can retrieve a subsample of the EuroSAT dataset using torchgeo. Luckily, we will show it is already in the format TerraTorch expects for a classification task.</span>
<span id="cb47-68"><a href="#cb47-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-71"><a href="#cb47-71" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-72"><a href="#cb47-72" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchgeo.datasets <span class="im">import</span> EuroSAT100 <span class="co"># 100 image subset from EuroSAT</span></span>
<span id="cb47-73"><a href="#cb47-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-74"><a href="#cb47-74" aria-hidden="true" tabindex="-1"></a>EuroSAT100(</span>
<span id="cb47-75"><a href="#cb47-75" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span>DATA_PATH, <span class="co"># make sure you have defined the path where you want data saved</span></span>
<span id="cb47-76"><a href="#cb47-76" aria-hidden="true" tabindex="-1"></a>    split<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb47-77"><a href="#cb47-77" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb47-78"><a href="#cb47-78" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-79"><a href="#cb47-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-80"><a href="#cb47-80" aria-hidden="true" tabindex="-1"></a>EuroSAT100(</span>
<span id="cb47-81"><a href="#cb47-81" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span>DATA_PATH,</span>
<span id="cb47-82"><a href="#cb47-82" aria-hidden="true" tabindex="-1"></a>    split<span class="op">=</span><span class="st">'val'</span>,</span>
<span id="cb47-83"><a href="#cb47-83" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb47-84"><a href="#cb47-84" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-85"><a href="#cb47-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-86"><a href="#cb47-86" aria-hidden="true" tabindex="-1"></a>EuroSAT100(</span>
<span id="cb47-87"><a href="#cb47-87" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span>DATA_PATH,</span>
<span id="cb47-88"><a href="#cb47-88" aria-hidden="true" tabindex="-1"></a>    split<span class="op">=</span><span class="st">'test'</span>,</span>
<span id="cb47-89"><a href="#cb47-89" aria-hidden="true" tabindex="-1"></a>    download<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb47-90"><a href="#cb47-90" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-91"><a href="#cb47-91" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-92"><a href="#cb47-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-93"><a href="#cb47-93" aria-hidden="true" tabindex="-1"></a>When opening inspecting the downloaded dataset, you will find that the data will be stored at the following path:</span>
<span id="cb47-94"><a href="#cb47-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-97"><a href="#cb47-97" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-98"><a href="#cb47-98" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the location of the saved EUROSAT data</span></span>
<span id="cb47-99"><a href="#cb47-99" aria-hidden="true" tabindex="-1"></a>EUROSAT <span class="op">=</span> DATA_PATH <span class="op">+</span> <span class="st">"/ds/images/remote_sensing/otherDatasets/sentinel_2/tif/"</span></span>
<span id="cb47-100"><a href="#cb47-100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-101"><a href="#cb47-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-102"><a href="#cb47-102" aria-hidden="true" tabindex="-1"></a>There is one folder for each class, each containing <span class="in">`.tif`</span> files with the images to be classified.</span>
<span id="cb47-103"><a href="#cb47-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-106"><a href="#cb47-106" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-107"><a href="#cb47-107" aria-hidden="true" tabindex="-1"></a><span class="co"># List the folders each containing images for a given EuroSAT class</span></span>
<span id="cb47-108"><a href="#cb47-108" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb47-109"><a href="#cb47-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-110"><a href="#cb47-110" aria-hidden="true" tabindex="-1"></a>class_paths <span class="op">=</span> [item <span class="cf">for</span> item <span class="kw">in</span> Path(EUROSAT).iterdir()]</span>
<span id="cb47-111"><a href="#cb47-111" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [item.name <span class="cf">for</span> item <span class="kw">in</span> class_paths]</span>
<span id="cb47-112"><a href="#cb47-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(class_names)</span>
<span id="cb47-113"><a href="#cb47-113" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-114"><a href="#cb47-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-115"><a href="#cb47-115" aria-hidden="true" tabindex="-1"></a><span class="fu">### Specify splits</span></span>
<span id="cb47-116"><a href="#cb47-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-117"><a href="#cb47-117" aria-hidden="true" tabindex="-1"></a>To specify which images belong to which split, you have two options:</span>
<span id="cb47-118"><a href="#cb47-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-119"><a href="#cb47-119" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>provide <span class="in">`.txt`</span> files that have the names of the images in each split in them</span>
<span id="cb47-120"><a href="#cb47-120" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>organize the splits into different folders</span>
<span id="cb47-121"><a href="#cb47-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-122"><a href="#cb47-122" aria-hidden="true" tabindex="-1"></a><span class="fu">#### EuroSAT example</span></span>
<span id="cb47-123"><a href="#cb47-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-124"><a href="#cb47-124" aria-hidden="true" tabindex="-1"></a>In the case of EuroSAT, the names of the images belonging to each split are saved as <span class="in">`.txt`</span> files.</span>
<span id="cb47-125"><a href="#cb47-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-128"><a href="#cb47-128" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-129"><a href="#cb47-129" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the paths to each split for use in the DataModule setup</span></span>
<span id="cb47-130"><a href="#cb47-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-131"><a href="#cb47-131" aria-hidden="true" tabindex="-1"></a>TRAIN_SPLIT <span class="op">=</span> DATA_PATH <span class="op">+</span> <span class="st">'/eurosat-100-train.txt'</span></span>
<span id="cb47-132"><a href="#cb47-132" aria-hidden="true" tabindex="-1"></a>VAL_SPLIT <span class="op">=</span> DATA_PATH <span class="op">+</span> <span class="st">'/eurosat-100-val.txt'</span></span>
<span id="cb47-133"><a href="#cb47-133" aria-hidden="true" tabindex="-1"></a>TEST_SPLIT <span class="op">=</span> DATA_PATH <span class="op">+</span> <span class="st">'/eurosat-100-test.txt'</span></span>
<span id="cb47-134"><a href="#cb47-134" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-135"><a href="#cb47-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-136"><a href="#cb47-136" aria-hidden="true" tabindex="-1"></a><span class="fu">## Step 2: Get your DataModule set up</span></span>
<span id="cb47-137"><a href="#cb47-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-138"><a href="#cb47-138" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inspect your data</span></span>
<span id="cb47-139"><a href="#cb47-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-140"><a href="#cb47-140" aria-hidden="true" tabindex="-1"></a>It is important to have a good grasp of the contents of your data in order to set up the DataModule properly. It can therefore be useful to do a bit of exploration and visualization.</span>
<span id="cb47-141"><a href="#cb47-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-144"><a href="#cb47-144" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-145"><a href="#cb47-145" aria-hidden="true" tabindex="-1"></a><span class="co"># Read in the first image in each class for inspection</span></span>
<span id="cb47-146"><a href="#cb47-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-147"><a href="#cb47-147" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rasterio</span>
<span id="cb47-148"><a href="#cb47-148" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-149"><a href="#cb47-149" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb47-150"><a href="#cb47-150" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb47-151"><a href="#cb47-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-152"><a href="#cb47-152" aria-hidden="true" tabindex="-1"></a><span class="co"># get the first image in each class</span></span>
<span id="cb47-153"><a href="#cb47-153" aria-hidden="true" tabindex="-1"></a>image_sample <span class="op">=</span> [<span class="bu">next</span>(folder.iterdir()) <span class="cf">for</span> folder <span class="kw">in</span> class_paths] <span class="co"># list of 10 images</span></span>
<span id="cb47-154"><a href="#cb47-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-155"><a href="#cb47-155" aria-hidden="true" tabindex="-1"></a><span class="co"># read all 10 sample images</span></span>
<span id="cb47-156"><a href="#cb47-156" aria-hidden="true" tabindex="-1"></a>ims <span class="op">=</span> [rasterio.<span class="bu">open</span>(image).read() <span class="cf">for</span> image <span class="kw">in</span> image_sample]</span>
<span id="cb47-157"><a href="#cb47-157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-158"><a href="#cb47-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-159"><a href="#cb47-159" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Image statistics</span></span>
<span id="cb47-160"><a href="#cb47-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-161"><a href="#cb47-161" aria-hidden="true" tabindex="-1"></a>By inspecting the first image, we find that we have 13 bands per image (corresponding to the 13 Sentinel-2 L1C bands) and images are 64x64 pixels. Neither of these is what Prithvi expects, so it's important to know this so we can adjust it in the DataLoader.</span>
<span id="cb47-162"><a href="#cb47-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-163"><a href="#cb47-163" aria-hidden="true" tabindex="-1"></a>We also note that the units are in reflectance values.</span>
<span id="cb47-164"><a href="#cb47-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-167"><a href="#cb47-167" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-168"><a href="#cb47-168" aria-hidden="true" tabindex="-1"></a><span class="co"># print some statistics for the first image</span></span>
<span id="cb47-169"><a href="#cb47-169" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Shape:"</span>, ims[<span class="dv">1</span>].shape)</span>
<span id="cb47-170"><a href="#cb47-170" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Means by band:"</span>, ims[<span class="dv">1</span>].mean(axis<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>)))</span>
<span id="cb47-171"><a href="#cb47-171" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stds by band:"</span>, ims[<span class="dv">1</span>].std(axis<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>)))</span>
<span id="cb47-172"><a href="#cb47-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-173"><a href="#cb47-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-174"><a href="#cb47-174" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualization</span></span>
<span id="cb47-175"><a href="#cb47-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-178"><a href="#cb47-178" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-179"><a href="#cb47-179" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_rgb(img, bands<span class="op">=</span>[<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>]):</span>
<span id="cb47-180"><a href="#cb47-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-181"><a href="#cb47-181" aria-hidden="true" tabindex="-1"></a>    <span class="co"># select the r,g,b bands</span></span>
<span id="cb47-182"><a href="#cb47-182" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> img[bands].astype(<span class="bu">float</span>)</span>
<span id="cb47-183"><a href="#cb47-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-184"><a href="#cb47-184" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Follow EuroSAT guidelines for visualization</span></span>
<span id="cb47-185"><a href="#cb47-185" aria-hidden="true" tabindex="-1"></a>    <span class="co"># clip to a max of 2750</span></span>
<span id="cb47-186"><a href="#cb47-186" aria-hidden="true" tabindex="-1"></a>    <span class="co"># normalize to 0-1</span></span>
<span id="cb47-187"><a href="#cb47-187" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> np.clip(rgb, <span class="dv">0</span>, <span class="dv">2750</span>) <span class="op">/</span> <span class="dv">2750</span></span>
<span id="cb47-188"><a href="#cb47-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-189"><a href="#cb47-189" aria-hidden="true" tabindex="-1"></a>    <span class="co"># go from (CxWxH) to (WxHxC)</span></span>
<span id="cb47-190"><a href="#cb47-190" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> np.transpose(rgb, (<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>))</span>
<span id="cb47-191"><a href="#cb47-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-192"><a href="#cb47-192" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rgb</span>
<span id="cb47-193"><a href="#cb47-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-194"><a href="#cb47-194" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">6</span>))</span>
<span id="cb47-195"><a href="#cb47-195" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.ravel()</span>
<span id="cb47-196"><a href="#cb47-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-197"><a href="#cb47-197" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (ax, img, name) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(axes, ims, class_names)):</span>
<span id="cb47-198"><a href="#cb47-198" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> to_rgb(img)</span>
<span id="cb47-199"><a href="#cb47-199" aria-hidden="true" tabindex="-1"></a>    ax.imshow(rgb)</span>
<span id="cb47-200"><a href="#cb47-200" aria-hidden="true" tabindex="-1"></a>    ax.set_title(name)</span>
<span id="cb47-201"><a href="#cb47-201" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">"off"</span>)</span>
<span id="cb47-202"><a href="#cb47-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-203"><a href="#cb47-203" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb47-204"><a href="#cb47-204" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb47-205"><a href="#cb47-205" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-206"><a href="#cb47-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-207"><a href="#cb47-207" aria-hidden="true" tabindex="-1"></a><span class="fu">### Define bands, statistics, and transforms</span></span>
<span id="cb47-208"><a href="#cb47-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-209"><a href="#cb47-209" aria-hidden="true" tabindex="-1"></a>There are a few important parameters we will need to define when setting up our DataModule in addition to the location of the data and splits that we defined in Step 1.</span>
<span id="cb47-210"><a href="#cb47-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-211"><a href="#cb47-211" aria-hidden="true" tabindex="-1"></a>We specifically use the GenericNonGeoClassificationDataModule, though all generic DataModules will need these inputs. You can always learn more about the arguments for a given DataModule by adapting the following code:</span>
<span id="cb47-212"><a href="#cb47-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-213"><a href="#cb47-213" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb47-214"><a href="#cb47-214" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.datamodules <span class="im">import</span> GenericNonGeoClassificationDataModule</span>
<span id="cb47-215"><a href="#cb47-215" aria-hidden="true" tabindex="-1"></a>?GenericNonGeoClassificationDataModule</span>
<span id="cb47-216"><a href="#cb47-216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-217"><a href="#cb47-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-218"><a href="#cb47-218" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Band names</span></span>
<span id="cb47-219"><a href="#cb47-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-220"><a href="#cb47-220" aria-hidden="true" tabindex="-1"></a>We need to tell the datamodule which bands we actually want to go into the model.</span>
<span id="cb47-221"><a href="#cb47-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-222"><a href="#cb47-222" aria-hidden="true" tabindex="-1"></a>In our case, we have 13 sentinel 2 bands, but Prithvi is trained on 6 landsay bands, so we need to subset them.</span>
<span id="cb47-223"><a href="#cb47-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-226"><a href="#cb47-226" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-227"><a href="#cb47-227" aria-hidden="true" tabindex="-1"></a>sentinel2_bands <span class="op">=</span> [</span>
<span id="cb47-228"><a href="#cb47-228" aria-hidden="true" tabindex="-1"></a>    <span class="st">"B1"</span>, <span class="st">"B2"</span>, <span class="st">"B3"</span>, <span class="st">"B4"</span>, <span class="st">"B5"</span>, <span class="st">"B6"</span>, <span class="st">"B7"</span>,</span>
<span id="cb47-229"><a href="#cb47-229" aria-hidden="true" tabindex="-1"></a>    <span class="st">"B8"</span>, <span class="st">"B8A"</span>, <span class="st">"B9"</span>, <span class="st">"B10"</span>, <span class="st">"B11"</span>, <span class="st">"B12"</span></span>
<span id="cb47-230"><a href="#cb47-230" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb47-231"><a href="#cb47-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-232"><a href="#cb47-232" aria-hidden="true" tabindex="-1"></a><span class="co"># The subset of our bands Prithvi will take</span></span>
<span id="cb47-233"><a href="#cb47-233" aria-hidden="true" tabindex="-1"></a>prithvi_subset <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">8</span>, <span class="dv">11</span>, <span class="dv">12</span>]</span>
<span id="cb47-234"><a href="#cb47-234" aria-hidden="true" tabindex="-1"></a>prithvi_bands <span class="op">=</span> [sentinel2_bands[i] <span class="cf">for</span> i <span class="kw">in</span> prithvi_subset]</span>
<span id="cb47-235"><a href="#cb47-235" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prithvi_bands)</span>
<span id="cb47-236"><a href="#cb47-236" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-237"><a href="#cb47-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-238"><a href="#cb47-238" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Means and standard deviations</span></span>
<span id="cb47-239"><a href="#cb47-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-240"><a href="#cb47-240" aria-hidden="true" tabindex="-1"></a>We need band-level means and standard deviations. Either compute them yourself over your entire dataset, or get general numbers that work well for your data type.</span>
<span id="cb47-241"><a href="#cb47-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-242"><a href="#cb47-242" aria-hidden="true" tabindex="-1"></a>We have S2 L1C data. We can use TerraMesh statistics I grabbed from their huggingface: https://huggingface.co/api/resolve-cache/datasets/ibm-esa-geospatial/TerraMesh/6c548cdacdd70e98a236de9f5b708d4b9dadf253/terramesh.py</span>
<span id="cb47-243"><a href="#cb47-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-246"><a href="#cb47-246" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-247"><a href="#cb47-247" aria-hidden="true" tabindex="-1"></a>statistics <span class="op">=</span> {</span>
<span id="cb47-248"><a href="#cb47-248" aria-hidden="true" tabindex="-1"></a>    <span class="st">"mean"</span>: {</span>
<span id="cb47-249"><a href="#cb47-249" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2L1C"</span>: [<span class="fl">2357.090</span>, <span class="fl">2137.398</span>, <span class="fl">2018.799</span>, <span class="fl">2082.998</span>, <span class="fl">2295.663</span>, <span class="fl">2854.548</span>, <span class="fl">3122.860</span>, <span class="fl">3040.571</span>, <span class="fl">3306.491</span>, <span class="fl">1473.849</span>,</span>
<span id="cb47-250"><a href="#cb47-250" aria-hidden="true" tabindex="-1"></a>                  <span class="fl">506.072</span>, <span class="fl">2472.840</span>, <span class="fl">1838.943</span>],</span>
<span id="cb47-251"><a href="#cb47-251" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2L2A"</span>: [<span class="fl">1390.461</span>, <span class="fl">1503.332</span>, <span class="fl">1718.211</span>, <span class="fl">1853.926</span>, <span class="fl">2199.116</span>, <span class="fl">2779.989</span>, <span class="fl">2987.025</span>, <span class="fl">3083.248</span>, <span class="fl">3132.235</span>, <span class="fl">3162.989</span>,</span>
<span id="cb47-252"><a href="#cb47-252" aria-hidden="true" tabindex="-1"></a>                  <span class="fl">2424.902</span>, <span class="fl">1857.665</span>],</span>
<span id="cb47-253"><a href="#cb47-253" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2RGB"</span>: [<span class="fl">110.349</span>, <span class="fl">99.507</span>, <span class="fl">75.843</span>],</span>
<span id="cb47-254"><a href="#cb47-254" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S1GRD"</span>: [<span class="op">-</span><span class="fl">12.577</span>, <span class="op">-</span><span class="fl">20.265</span>],</span>
<span id="cb47-255"><a href="#cb47-255" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S1RTC"</span>: [<span class="op">-</span><span class="fl">10.93</span>, <span class="op">-</span><span class="fl">17.329</span>],</span>
<span id="cb47-256"><a href="#cb47-256" aria-hidden="true" tabindex="-1"></a>        <span class="st">"NDVI"</span>: [<span class="fl">0.327</span>],</span>
<span id="cb47-257"><a href="#cb47-257" aria-hidden="true" tabindex="-1"></a>        <span class="st">"DEM"</span>: [<span class="fl">651.663</span>],</span>
<span id="cb47-258"><a href="#cb47-258" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb47-259"><a href="#cb47-259" aria-hidden="true" tabindex="-1"></a>    <span class="st">"std"</span>: {</span>
<span id="cb47-260"><a href="#cb47-260" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2L1C"</span>: [<span class="fl">1673.639</span>, <span class="fl">1722.641</span>, <span class="fl">1602.205</span>, <span class="fl">1873.138</span>, <span class="fl">1866.055</span>, <span class="fl">1779.839</span>, <span class="fl">1776.496</span>, <span class="fl">1724.114</span>, <span class="fl">1771.041</span>, <span class="fl">1079.786</span>,</span>
<span id="cb47-261"><a href="#cb47-261" aria-hidden="true" tabindex="-1"></a>                  <span class="fl">512.404</span>, <span class="fl">1340.879</span>, <span class="fl">1172.435</span>],</span>
<span id="cb47-262"><a href="#cb47-262" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2L2A"</span>: [<span class="fl">2131.157</span>, <span class="fl">2163.666</span>, <span class="fl">2059.311</span>, <span class="fl">2152.477</span>, <span class="fl">2105.179</span>, <span class="fl">1912.773</span>, <span class="fl">1842.326</span>, <span class="fl">1893.568</span>, <span class="fl">1775.656</span>, <span class="fl">1814.907</span>,</span>
<span id="cb47-263"><a href="#cb47-263" aria-hidden="true" tabindex="-1"></a>                  <span class="fl">1436.282</span>, <span class="fl">1336.155</span>],</span>
<span id="cb47-264"><a href="#cb47-264" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S2RGB"</span>: [<span class="fl">69.905</span>, <span class="fl">53.708</span>, <span class="fl">53.378</span>],</span>
<span id="cb47-265"><a href="#cb47-265" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S1GRD"</span>: [<span class="fl">5.179</span>, <span class="fl">5.872</span>],</span>
<span id="cb47-266"><a href="#cb47-266" aria-hidden="true" tabindex="-1"></a>        <span class="st">"S1RTC"</span>: [<span class="fl">4.391</span>, <span class="fl">4.459</span>],</span>
<span id="cb47-267"><a href="#cb47-267" aria-hidden="true" tabindex="-1"></a>        <span class="st">"NDVI"</span>: [<span class="fl">0.322</span>],</span>
<span id="cb47-268"><a href="#cb47-268" aria-hidden="true" tabindex="-1"></a>        <span class="st">"DEM"</span>: [<span class="fl">928.168</span>]</span>
<span id="cb47-269"><a href="#cb47-269" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb47-270"><a href="#cb47-270" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb47-271"><a href="#cb47-271" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-272"><a href="#cb47-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-273"><a href="#cb47-273" aria-hidden="true" tabindex="-1"></a>We specifically need to pass Sentinel 2 L1C means and stds, but only for the 6 bands we are subsetting for input into prithvi.</span>
<span id="cb47-274"><a href="#cb47-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-277"><a href="#cb47-277" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-278"><a href="#cb47-278" aria-hidden="true" tabindex="-1"></a>S2L1C_prithvi_means <span class="op">=</span> [statistics[<span class="st">"mean"</span>][<span class="st">"S2L1C"</span>][i] <span class="cf">for</span> i <span class="kw">in</span> prithvi_subset]</span>
<span id="cb47-279"><a href="#cb47-279" aria-hidden="true" tabindex="-1"></a>S2L1C_prithvi_stds <span class="op">=</span> [statistics[<span class="st">"std"</span>][<span class="st">"S2L1C"</span>][i] <span class="cf">for</span> i <span class="kw">in</span> prithvi_subset]</span>
<span id="cb47-280"><a href="#cb47-280" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(S2L1C_prithvi_means)</span>
<span id="cb47-281"><a href="#cb47-281" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(S2L1C_prithvi_stds)</span>
<span id="cb47-282"><a href="#cb47-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-283"><a href="#cb47-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-284"><a href="#cb47-284" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Transforms</span></span>
<span id="cb47-285"><a href="#cb47-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-286"><a href="#cb47-286" aria-hidden="true" tabindex="-1"></a>Transforms allow you to make changes to your data on the fly before feeding them to a model for training. There are three important types of transforms:</span>
<span id="cb47-287"><a href="#cb47-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-288"><a href="#cb47-288" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Reshaping/resizing/clipping: To get your data into the shape your model expects.</span>
<span id="cb47-289"><a href="#cb47-289" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Data augmentations: If you don't have a lot of training data, you can "augment" your dataset by presenting changed versions of your images to the model. For example, flip your images. This type of transform generally should only happen to your training data, and not your validation and testing data.</span>
<span id="cb47-290"><a href="#cb47-290" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Make your image a tensor (the data format that GPUs can use)</span>
<span id="cb47-291"><a href="#cb47-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-292"><a href="#cb47-292" aria-hidden="true" tabindex="-1"></a>We can use albumentations to do any of these.</span>
<span id="cb47-293"><a href="#cb47-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-296"><a href="#cb47-296" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-297"><a href="#cb47-297" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> albumentations</span>
<span id="cb47-298"><a href="#cb47-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-299"><a href="#cb47-299" aria-hidden="true" tabindex="-1"></a>train_transforms <span class="op">=</span> albumentations.Compose([</span>
<span id="cb47-300"><a href="#cb47-300" aria-hidden="true" tabindex="-1"></a>    albumentations.Resize(<span class="dv">224</span>, <span class="dv">224</span>), <span class="co"># go from 64x64 to 224x224 (Prithvi expected size)</span></span>
<span id="cb47-301"><a href="#cb47-301" aria-hidden="true" tabindex="-1"></a>    albumentations.HorizontalFlip(), <span class="co"># augmentation</span></span>
<span id="cb47-302"><a href="#cb47-302" aria-hidden="true" tabindex="-1"></a>    albumentations.pytorch.transforms.ToTensorV2(),</span>
<span id="cb47-303"><a href="#cb47-303" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb47-304"><a href="#cb47-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-305"><a href="#cb47-305" aria-hidden="true" tabindex="-1"></a><span class="co"># Same transforms but without augmentations for validation and testing</span></span>
<span id="cb47-306"><a href="#cb47-306" aria-hidden="true" tabindex="-1"></a>val_test_transforms <span class="op">=</span> albumentations.Compose([</span>
<span id="cb47-307"><a href="#cb47-307" aria-hidden="true" tabindex="-1"></a>    albumentations.Resize(<span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb47-308"><a href="#cb47-308" aria-hidden="true" tabindex="-1"></a>    albumentations.pytorch.transforms.ToTensorV2(),</span>
<span id="cb47-309"><a href="#cb47-309" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb47-310"><a href="#cb47-310" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-311"><a href="#cb47-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-312"><a href="#cb47-312" aria-hidden="true" tabindex="-1"></a><span class="fu">### Define your DataModule</span></span>
<span id="cb47-313"><a href="#cb47-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-316"><a href="#cb47-316" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-317"><a href="#cb47-317" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.datamodules <span class="im">import</span> GenericNonGeoClassificationDataModule</span>
<span id="cb47-318"><a href="#cb47-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-319"><a href="#cb47-319" aria-hidden="true" tabindex="-1"></a>datamodule <span class="op">=</span> GenericNonGeoClassificationDataModule(</span>
<span id="cb47-320"><a href="#cb47-320" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">16</span>, <span class="co"># How many images to give the model at once. More = faster, but more RAM is needed</span></span>
<span id="cb47-321"><a href="#cb47-321" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">0</span>, <span class="co"># extra CPU threads for image loading</span></span>
<span id="cb47-322"><a href="#cb47-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-323"><a href="#cb47-323" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Where is our data? In our case, all splits are in the same folder</span></span>
<span id="cb47-324"><a href="#cb47-324" aria-hidden="true" tabindex="-1"></a>    train_data_root<span class="op">=</span>EUROSAT,</span>
<span id="cb47-325"><a href="#cb47-325" aria-hidden="true" tabindex="-1"></a>    val_data_root<span class="op">=</span>EUROSAT,</span>
<span id="cb47-326"><a href="#cb47-326" aria-hidden="true" tabindex="-1"></a>    test_data_root<span class="op">=</span>EUROSAT,</span>
<span id="cb47-327"><a href="#cb47-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-328"><a href="#cb47-328" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Because images for all splits are in the same place, we need to specify our split files</span></span>
<span id="cb47-329"><a href="#cb47-329" aria-hidden="true" tabindex="-1"></a>    train_split<span class="op">=</span>TRAIN_SPLIT,</span>
<span id="cb47-330"><a href="#cb47-330" aria-hidden="true" tabindex="-1"></a>    val_split<span class="op">=</span>VAL_SPLIT,</span>
<span id="cb47-331"><a href="#cb47-331" aria-hidden="true" tabindex="-1"></a>    test_split<span class="op">=</span>TEST_SPLIT,</span>
<span id="cb47-332"><a href="#cb47-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-333"><a href="#cb47-333" aria-hidden="true" tabindex="-1"></a>    <span class="co"># means and standard deviations for the bands being input into the model</span></span>
<span id="cb47-334"><a href="#cb47-334" aria-hidden="true" tabindex="-1"></a>    means<span class="op">=</span>S2L1C_prithvi_means,</span>
<span id="cb47-335"><a href="#cb47-335" aria-hidden="true" tabindex="-1"></a>    stds<span class="op">=</span>S2L1C_prithvi_stds,</span>
<span id="cb47-336"><a href="#cb47-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-337"><a href="#cb47-337" aria-hidden="true" tabindex="-1"></a>    <span class="co"># number of classes</span></span>
<span id="cb47-338"><a href="#cb47-338" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="bu">len</span>(class_names),</span>
<span id="cb47-339"><a href="#cb47-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-340"><a href="#cb47-340" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tranforms, defined using albumentations</span></span>
<span id="cb47-341"><a href="#cb47-341" aria-hidden="true" tabindex="-1"></a>    train_transform<span class="op">=</span>train_transforms,</span>
<span id="cb47-342"><a href="#cb47-342" aria-hidden="true" tabindex="-1"></a>    val_transform<span class="op">=</span>val_test_transforms,</span>
<span id="cb47-343"><a href="#cb47-343" aria-hidden="true" tabindex="-1"></a>    test_transform<span class="op">=</span>val_test_transforms,</span>
<span id="cb47-344"><a href="#cb47-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-345"><a href="#cb47-345" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bands of our dataset</span></span>
<span id="cb47-346"><a href="#cb47-346" aria-hidden="true" tabindex="-1"></a>    dataset_bands<span class="op">=</span>sentinel2_bands,</span>
<span id="cb47-347"><a href="#cb47-347" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Bands to input into our model</span></span>
<span id="cb47-348"><a href="#cb47-348" aria-hidden="true" tabindex="-1"></a>    output_bands<span class="op">=</span>prithvi_bands,</span>
<span id="cb47-349"><a href="#cb47-349" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-350"><a href="#cb47-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-351"><a href="#cb47-351" aria-hidden="true" tabindex="-1"></a>datamodule.setup(<span class="st">"fit"</span>)</span>
<span id="cb47-352"><a href="#cb47-352" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-353"><a href="#cb47-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-354"><a href="#cb47-354" aria-hidden="true" tabindex="-1"></a><span class="fu">### Check your DataModule</span></span>
<span id="cb47-355"><a href="#cb47-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-356"><a href="#cb47-356" aria-hidden="true" tabindex="-1"></a>To ensure that the data are as we would expect the model to find them, we can manually iterate through a batch of the datamodule to inspect it.</span>
<span id="cb47-357"><a href="#cb47-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-360"><a href="#cb47-360" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-361"><a href="#cb47-361" aria-hidden="true" tabindex="-1"></a>batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(datamodule.train_dataloader()))</span>
<span id="cb47-362"><a href="#cb47-362" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-363"><a href="#cb47-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-366"><a href="#cb47-366" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-367"><a href="#cb47-367" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch keys: </span><span class="sc">{</span>batch<span class="sc">.</span>keys()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-368"><a href="#cb47-368" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image shape: </span><span class="sc">{</span>batch[<span class="st">'image'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-369"><a href="#cb47-369" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb47-370"><a href="#cb47-370" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"Label shape: </span><span class="sc">{</span>batch[<span class="st">'label'</span>]<span class="sc">.</span>shape <span class="cf">if</span> <span class="st">'label'</span> <span class="kw">in</span> batch <span class="cf">else</span> batch[<span class="st">'labels'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-371"><a href="#cb47-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-372"><a href="#cb47-372" aria-hidden="true" tabindex="-1"></a><span class="co"># Check the actual values</span></span>
<span id="cb47-373"><a href="#cb47-373" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Image dtype: </span><span class="sc">{</span>batch[<span class="st">'image'</span>]<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-374"><a href="#cb47-374" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image min/max: </span><span class="sc">{</span>batch[<span class="st">'image'</span>]<span class="sc">.</span><span class="bu">min</span>()<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>batch[<span class="st">'image'</span>]<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-375"><a href="#cb47-375" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Labels: </span><span class="sc">{</span>batch[<span class="st">'label'</span>] <span class="cf">if</span> <span class="st">'label'</span> <span class="kw">in</span> batch <span class="cf">else</span> batch[<span class="st">'labels'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-376"><a href="#cb47-376" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-377"><a href="#cb47-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-378"><a href="#cb47-378" aria-hidden="true" tabindex="-1"></a>As expected, we have 16 images with 6 bands and resized to the width and height the model expects.</span>
<span id="cb47-379"><a href="#cb47-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-380"><a href="#cb47-380" aria-hidden="true" tabindex="-1"></a>Notice the min and max have not been normalized. This is because normalization occurs during every training and validation step, which happens below.</span>
<span id="cb47-381"><a href="#cb47-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-382"><a href="#cb47-382" aria-hidden="true" tabindex="-1"></a><span class="fu">## Step 3: Get your Task set up</span></span>
<span id="cb47-383"><a href="#cb47-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-384"><a href="#cb47-384" aria-hidden="true" tabindex="-1"></a>TerraTorch has several Lightning Trainers to easily handle model training, and has Tasks defined for each major task type. We will be using the ClassificationTask.</span>
<span id="cb47-385"><a href="#cb47-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-386"><a href="#cb47-386" aria-hidden="true" tabindex="-1"></a>These Tasks are where you can define your model choice and model hyperparameters.</span>
<span id="cb47-387"><a href="#cb47-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-388"><a href="#cb47-388" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choose your model from the model factory</span></span>
<span id="cb47-389"><a href="#cb47-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-390"><a href="#cb47-390" aria-hidden="true" tabindex="-1"></a>If you don't know which model you would like to use, you can explore what is available in TerraTorch's model factory.</span>
<span id="cb47-391"><a href="#cb47-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-392"><a href="#cb47-392" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Explore encoders</span></span>
<span id="cb47-393"><a href="#cb47-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-396"><a href="#cb47-396" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-397"><a href="#cb47-397" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.registry <span class="im">import</span> BACKBONE_REGISTRY</span>
<span id="cb47-398"><a href="#cb47-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-399"><a href="#cb47-399" aria-hidden="true" tabindex="-1"></a><span class="co"># List all the available backbones from different sources</span></span>
<span id="cb47-400"><a href="#cb47-400" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, source <span class="kw">in</span> BACKBONE_REGISTRY._sources.items():</span>
<span id="cb47-401"><a href="#cb47-401" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">====</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">====="</span>)</span>
<span id="cb47-402"><a href="#cb47-402" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">list</span>(source))</span>
<span id="cb47-403"><a href="#cb47-403" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-404"><a href="#cb47-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-407"><a href="#cb47-407" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-408"><a href="#cb47-408" aria-hidden="true" tabindex="-1"></a><span class="co"># let's just list out the Prithvi models in the terratorch source</span></span>
<span id="cb47-409"><a href="#cb47-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-410"><a href="#cb47-410" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([mod <span class="cf">for</span> mod <span class="kw">in</span> BACKBONE_REGISTRY._sources[<span class="st">'terratorch'</span>] <span class="cf">if</span> <span class="st">'prithvi'</span> <span class="kw">in</span> mod])</span>
<span id="cb47-411"><a href="#cb47-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-412"><a href="#cb47-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-413"><a href="#cb47-413" aria-hidden="true" tabindex="-1"></a>There are many different prithvi versions to select from -- for this demo we will choose the smaller <span class="in">`prithvi_eo_v1_100`</span>.</span>
<span id="cb47-414"><a href="#cb47-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-415"><a href="#cb47-415" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Explore decoders</span></span>
<span id="cb47-416"><a href="#cb47-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-419"><a href="#cb47-419" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-420"><a href="#cb47-420" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.registry <span class="im">import</span> DECODER_REGISTRY</span>
<span id="cb47-421"><a href="#cb47-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-422"><a href="#cb47-422" aria-hidden="true" tabindex="-1"></a><span class="co"># Check available decoders</span></span>
<span id="cb47-423"><a href="#cb47-423" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, source <span class="kw">in</span> DECODER_REGISTRY._sources.items():</span>
<span id="cb47-424"><a href="#cb47-424" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">====</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">====="</span>)</span>
<span id="cb47-425"><a href="#cb47-425" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">list</span>(source))</span>
<span id="cb47-426"><a href="#cb47-426" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-427"><a href="#cb47-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-428"><a href="#cb47-428" aria-hidden="true" tabindex="-1"></a>We choose the FCNDecoder because it is good for Classification tasks.</span>
<span id="cb47-429"><a href="#cb47-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-430"><a href="#cb47-430" aria-hidden="true" tabindex="-1"></a><span class="fu">### Define your Task</span></span>
<span id="cb47-431"><a href="#cb47-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-434"><a href="#cb47-434" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-435"><a href="#cb47-435" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.tasks <span class="im">import</span> ClassificationTask</span>
<span id="cb47-436"><a href="#cb47-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-437"><a href="#cb47-437" aria-hidden="true" tabindex="-1"></a>task <span class="op">=</span> ClassificationTask(</span>
<span id="cb47-438"><a href="#cb47-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-439"><a href="#cb47-439" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define your model</span></span>
<span id="cb47-440"><a href="#cb47-440" aria-hidden="true" tabindex="-1"></a>    model_factory<span class="op">=</span><span class="st">"EncoderDecoderFactory"</span>, <span class="co"># TerraTorch's EncoderDecoderFactory, where we found our models</span></span>
<span id="cb47-441"><a href="#cb47-441" aria-hidden="true" tabindex="-1"></a>    model_args<span class="op">=</span>{</span>
<span id="cb47-442"><a href="#cb47-442" aria-hidden="true" tabindex="-1"></a>        <span class="st">'backbone'</span>: <span class="st">'prithvi_eo_v1_100'</span>, <span class="co"># Smaller Prithvi model for demo</span></span>
<span id="cb47-443"><a href="#cb47-443" aria-hidden="true" tabindex="-1"></a>        <span class="st">'backbone_pretrained'</span>: <span class="va">True</span>, <span class="co"># Train from scratch or use pre-trained weights?</span></span>
<span id="cb47-444"><a href="#cb47-444" aria-hidden="true" tabindex="-1"></a>        <span class="st">'decoder'</span>: <span class="st">'FCNDecoder'</span>, <span class="co"># Chosen decoder for classification</span></span>
<span id="cb47-445"><a href="#cb47-445" aria-hidden="true" tabindex="-1"></a>        <span class="st">'num_classes'</span>: <span class="bu">len</span>(class_names),</span>
<span id="cb47-446"><a href="#cb47-446" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb47-447"><a href="#cb47-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-448"><a href="#cb47-448" aria-hidden="true" tabindex="-1"></a>    <span class="co"># What would you like to train?</span></span>
<span id="cb47-449"><a href="#cb47-449" aria-hidden="true" tabindex="-1"></a>    freeze_backbone<span class="op">=</span><span class="va">True</span>, <span class="co"># Do not update prithvi weights for demo</span></span>
<span id="cb47-450"><a href="#cb47-450" aria-hidden="true" tabindex="-1"></a>    freeze_decoder<span class="op">=</span><span class="va">False</span>, <span class="co"># Train the decoder</span></span>
<span id="cb47-451"><a href="#cb47-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-452"><a href="#cb47-452" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Optionally, change hyperparameters from the defaults</span></span>
<span id="cb47-453"><a href="#cb47-453" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'ce'</span>,</span>
<span id="cb47-454"><a href="#cb47-454" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">1e-4</span></span>
<span id="cb47-455"><a href="#cb47-455" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-456"><a href="#cb47-456" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-457"><a href="#cb47-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-458"><a href="#cb47-458" aria-hidden="true" tabindex="-1"></a><span class="fu">## Step 4: Train your model</span></span>
<span id="cb47-459"><a href="#cb47-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-460"><a href="#cb47-460" aria-hidden="true" tabindex="-1"></a>Now that we have our Task and DataModule, we can easily use PyTorch Lightning to train our model.</span>
<span id="cb47-461"><a href="#cb47-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-462"><a href="#cb47-462" aria-hidden="true" tabindex="-1"></a>We also use Weights and Biases to check our progress during training.</span>
<span id="cb47-463"><a href="#cb47-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-464"><a href="#cb47-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-467"><a href="#cb47-467" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-468"><a href="#cb47-468" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightning.pytorch.loggers <span class="im">import</span> WandbLogger</span>
<span id="cb47-469"><a href="#cb47-469" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightning.pytorch <span class="im">import</span> Trainer</span>
<span id="cb47-470"><a href="#cb47-470" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lightning.pytorch.callbacks <span class="im">import</span> EarlyStopping, ModelCheckpoint, RichProgressBar, LearningRateMonitor</span>
<span id="cb47-471"><a href="#cb47-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-472"><a href="#cb47-472" aria-hidden="true" tabindex="-1"></a><span class="co"># Auto-detect best precision based on available hardware</span></span>
<span id="cb47-473"><a href="#cb47-473" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb47-474"><a href="#cb47-474" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> <span class="st">"16-mixed"</span>  <span class="co"># CUDA supports mixed precision well</span></span>
<span id="cb47-475"><a href="#cb47-475" aria-hidden="true" tabindex="-1"></a><span class="cf">elif</span> torch.backends.mps.is_available():</span>
<span id="cb47-476"><a href="#cb47-476" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> <span class="st">"32"</span>  <span class="co"># MPS has limited 16-bit support</span></span>
<span id="cb47-477"><a href="#cb47-477" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb47-478"><a href="#cb47-478" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> <span class="st">"32"</span>  <span class="co"># CPU doesn't benefit from mixed precision</span></span>
<span id="cb47-479"><a href="#cb47-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-480"><a href="#cb47-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-481"><a href="#cb47-481" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize callbacks</span></span>
<span id="cb47-482"><a href="#cb47-482" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the best and the last model</span></span>
<span id="cb47-483"><a href="#cb47-483" aria-hidden="true" tabindex="-1"></a>checkpoint_callback <span class="op">=</span> ModelCheckpoint(</span>
<span id="cb47-484"><a href="#cb47-484" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span>task.monitor, save_top_k<span class="op">=</span><span class="dv">1</span>, save_last<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-485"><a href="#cb47-485" aria-hidden="true" tabindex="-1"></a><span class="co"># Stop training if validation metrics stop improving for a certain number of epochs</span></span>
<span id="cb47-486"><a href="#cb47-486" aria-hidden="true" tabindex="-1"></a>early_stopping_callback <span class="op">=</span> EarlyStopping(</span>
<span id="cb47-487"><a href="#cb47-487" aria-hidden="true" tabindex="-1"></a>    monitor<span class="op">=</span>task.monitor, min_delta<span class="op">=</span><span class="fl">0.00</span>, patience<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb47-488"><a href="#cb47-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-489"><a href="#cb47-489" aria-hidden="true" tabindex="-1"></a><span class="co"># Set this to None so we can load the best model from checkpoint later</span></span>
<span id="cb47-490"><a href="#cb47-490" aria-hidden="true" tabindex="-1"></a>checkpoint_callback.best_model_path <span class="op">=</span> <span class="va">None</span></span>
<span id="cb47-491"><a href="#cb47-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-492"><a href="#cb47-492" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-493"><a href="#cb47-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-494"><a href="#cb47-494" aria-hidden="true" tabindex="-1"></a>Now we can define our Trainer.</span>
<span id="cb47-495"><a href="#cb47-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-498"><a href="#cb47-498" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-499"><a href="#cb47-499" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb47-500"><a href="#cb47-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-501"><a href="#cb47-501" aria-hidden="true" tabindex="-1"></a><span class="co"># Check your progress and results during training using Weights and Biases.</span></span>
<span id="cb47-502"><a href="#cb47-502" aria-hidden="true" tabindex="-1"></a><span class="co"># You'll have to make an account but it's very useful.</span></span>
<span id="cb47-503"><a href="#cb47-503" aria-hidden="true" tabindex="-1"></a><span class="co"># Just click the link it gives you to watch your training progress plotted in real time.</span></span>
<span id="cb47-504"><a href="#cb47-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-505"><a href="#cb47-505" aria-hidden="true" tabindex="-1"></a>wandb_logger <span class="op">=</span> WandbLogger(log_model<span class="op">=</span><span class="st">"all"</span>)</span>
<span id="cb47-506"><a href="#cb47-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-507"><a href="#cb47-507" aria-hidden="true" tabindex="-1"></a><span class="co"># Define your Trainer</span></span>
<span id="cb47-508"><a href="#cb47-508" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb47-509"><a href="#cb47-509" aria-hidden="true" tabindex="-1"></a>    accelerator<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb47-510"><a href="#cb47-510" aria-hidden="true" tabindex="-1"></a>    devices<span class="op">=</span><span class="dv">1</span>,  <span class="co"># Number of GPUs. Interactive mode recommended with 1 device</span></span>
<span id="cb47-511"><a href="#cb47-511" aria-hidden="true" tabindex="-1"></a>    precision<span class="op">=</span>precision,  <span class="co"># Mac MPS has limited support for 16-bit floats</span></span>
<span id="cb47-512"><a href="#cb47-512" aria-hidden="true" tabindex="-1"></a>    <span class="co"># precision="16-mixed", # Use 16-bit floats as opposed to 32-bit (higher precision) when it's safe to save on time and memory</span></span>
<span id="cb47-513"><a href="#cb47-513" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb47-514"><a href="#cb47-514" aria-hidden="true" tabindex="-1"></a>        RichProgressBar(),</span>
<span id="cb47-515"><a href="#cb47-515" aria-hidden="true" tabindex="-1"></a>        checkpoint_callback,</span>
<span id="cb47-516"><a href="#cb47-516" aria-hidden="true" tabindex="-1"></a>        early_stopping_callback,</span>
<span id="cb47-517"><a href="#cb47-517" aria-hidden="true" tabindex="-1"></a>        LearningRateMonitor(logging_interval<span class="op">=</span><span class="st">"epoch"</span>),</span>
<span id="cb47-518"><a href="#cb47-518" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb47-519"><a href="#cb47-519" aria-hidden="true" tabindex="-1"></a>    logger<span class="op">=</span>wandb_logger,</span>
<span id="cb47-520"><a href="#cb47-520" aria-hidden="true" tabindex="-1"></a>    max_epochs<span class="op">=</span><span class="dv">100</span>,  <span class="co"># Train for up to 100 epochs</span></span>
<span id="cb47-521"><a href="#cb47-521" aria-hidden="true" tabindex="-1"></a>    default_root_dir<span class="op">=</span><span class="st">'output/tutorial'</span>,  <span class="co"># where checkpoints/logs go.</span></span>
<span id="cb47-522"><a href="#cb47-522" aria-hidden="true" tabindex="-1"></a>    log_every_n_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb47-523"><a href="#cb47-523" aria-hidden="true" tabindex="-1"></a>    check_val_every_n_epoch<span class="op">=</span><span class="dv">1</span>  <span class="co"># How frequently to calcualte validation performance</span></span>
<span id="cb47-524"><a href="#cb47-524" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-525"><a href="#cb47-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-526"><a href="#cb47-526" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit your model!</span></span>
<span id="cb47-527"><a href="#cb47-527" aria-hidden="true" tabindex="-1"></a><span class="co"># _ = trainer.fit(model=task, datamodule=datamodule)</span></span>
<span id="cb47-528"><a href="#cb47-528" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-529"><a href="#cb47-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-530"><a href="#cb47-530" aria-hidden="true" tabindex="-1"></a><span class="fu">## Step 5: Run inference with your trained model</span></span>
<span id="cb47-531"><a href="#cb47-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-532"><a href="#cb47-532" aria-hidden="true" tabindex="-1"></a>Now that we have a trained model, let's see how to use it for predictions on new images.</span>
<span id="cb47-533"><a href="#cb47-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-534"><a href="#cb47-534" aria-hidden="true" tabindex="-1"></a><span class="fu">### Load the best checkpoint</span></span>
<span id="cb47-535"><a href="#cb47-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-536"><a href="#cb47-536" aria-hidden="true" tabindex="-1"></a>After training, the best model is saved. Let's load it.</span>
<span id="cb47-537"><a href="#cb47-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-538"><a href="#cb47-538" aria-hidden="true" tabindex="-1"></a>Note: If you just trained your model, the checkpoint_callback.best_model_path should already be set to the path of the best model. If you didn't run the training step, you will need to set the best_model_path to the path of the best model.</span>
<span id="cb47-539"><a href="#cb47-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-542"><a href="#cb47-542" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-543"><a href="#cb47-543" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the best model from checkpoint</span></span>
<span id="cb47-544"><a href="#cb47-544" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> checkpoint_callback.best_model_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb47-545"><a href="#cb47-545" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If the best model path is not None, use it</span></span>
<span id="cb47-546"><a href="#cb47-546" aria-hidden="true" tabindex="-1"></a>    best_model_path <span class="op">=</span> checkpoint_callback.best_model_path</span>
<span id="cb47-547"><a href="#cb47-547" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb47-548"><a href="#cb47-548" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If the best model path is None, use a default path (must already exist!)</span></span>
<span id="cb47-549"><a href="#cb47-549" aria-hidden="true" tabindex="-1"></a>    best_model_path <span class="op">=</span> <span class="st">'lightning_logs/ym9pbv87/checkpoints/epoch=51-step=156.ckpt'</span></span>
<span id="cb47-550"><a href="#cb47-550" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best model saved at: </span><span class="sc">{</span>best_model_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-551"><a href="#cb47-551" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-552"><a href="#cb47-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-555"><a href="#cb47-555" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-556"><a href="#cb47-556" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model for inference</span></span>
<span id="cb47-557"><a href="#cb47-557" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.tasks <span class="im">import</span> ClassificationTask</span>
<span id="cb47-558"><a href="#cb47-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-559"><a href="#cb47-559" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the model for inference using the ClassificationTask class</span></span>
<span id="cb47-560"><a href="#cb47-560" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the same class we used to train the model</span></span>
<span id="cb47-561"><a href="#cb47-561" aria-hidden="true" tabindex="-1"></a>trained_task <span class="op">=</span> ClassificationTask.load_from_checkpoint(best_model_path)</span>
<span id="cb47-562"><a href="#cb47-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-563"><a href="#cb47-563" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the model to evaluation mode</span></span>
<span id="cb47-564"><a href="#cb47-564" aria-hidden="true" tabindex="-1"></a><span class="co"># This is important to ensure the model is in the correct mode for inference</span></span>
<span id="cb47-565"><a href="#cb47-565" aria-hidden="true" tabindex="-1"></a>trained_task.<span class="bu">eval</span>() <span class="co"># set to evaluation mode</span></span>
<span id="cb47-566"><a href="#cb47-566" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-567"><a href="#cb47-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-568"><a href="#cb47-568" aria-hidden="true" tabindex="-1"></a><span class="fu">### Get some test images</span></span>
<span id="cb47-569"><a href="#cb47-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-570"><a href="#cb47-570" aria-hidden="true" tabindex="-1"></a><span class="fu">### Run predictions</span></span>
<span id="cb47-573"><a href="#cb47-573" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-574"><a href="#cb47-574" aria-hidden="true" tabindex="-1"></a><span class="co"># Get a single batch from the validation dataloader (not yet transformed)</span></span>
<span id="cb47-575"><a href="#cb47-575" aria-hidden="true" tabindex="-1"></a>val_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(datamodule.val_dataloader()))</span>
<span id="cb47-576"><a href="#cb47-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-577"><a href="#cb47-577" aria-hidden="true" tabindex="-1"></a><span class="co"># Clone the images to avoid modifying the original data</span></span>
<span id="cb47-578"><a href="#cb47-578" aria-hidden="true" tabindex="-1"></a><span class="co"># We will use these unnormalized images to visualize the predictions later</span></span>
<span id="cb47-579"><a href="#cb47-579" aria-hidden="true" tabindex="-1"></a>unnormalized_images <span class="op">=</span> val_batch[<span class="st">'image'</span>].clone()</span>
<span id="cb47-580"><a href="#cb47-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-581"><a href="#cb47-581" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the same transforms used during training/validation</span></span>
<span id="cb47-582"><a href="#cb47-582" aria-hidden="true" tabindex="-1"></a><span class="co"># This is important to ensure the images are in the same format as during training</span></span>
<span id="cb47-583"><a href="#cb47-583" aria-hidden="true" tabindex="-1"></a>val_images <span class="op">=</span> datamodule.aug(val_batch)</span>
<span id="cb47-584"><a href="#cb47-584" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the labels for the batch, which are the ground truth labels</span></span>
<span id="cb47-585"><a href="#cb47-585" aria-hidden="true" tabindex="-1"></a>val_labels <span class="op">=</span> val_batch[<span class="st">'label'</span>]</span>
<span id="cb47-586"><a href="#cb47-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-587"><a href="#cb47-587" aria-hidden="true" tabindex="-1"></a><span class="co"># CHeck stats on images to see if they are normalized</span></span>
<span id="cb47-588"><a href="#cb47-588" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image min: </span><span class="sc">{</span>val_images[<span class="st">'image'</span>]<span class="sc">.</span><span class="bu">min</span>()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-589"><a href="#cb47-589" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image max: </span><span class="sc">{</span>val_images[<span class="st">'image'</span>]<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-590"><a href="#cb47-590" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image mean: </span><span class="sc">{</span>val_images[<span class="st">'image'</span>]<span class="sc">.</span>mean()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-591"><a href="#cb47-591" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Image std: </span><span class="sc">{</span>val_images[<span class="st">'image'</span>]<span class="sc">.</span>std()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-592"><a href="#cb47-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-593"><a href="#cb47-593" aria-hidden="true" tabindex="-1"></a><span class="co"># Now conduct inference using the transformed images:</span></span>
<span id="cb47-594"><a href="#cb47-594" aria-hidden="true" tabindex="-1"></a>trained_task.<span class="bu">eval</span>()</span>
<span id="cb47-595"><a href="#cb47-595" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb47-596"><a href="#cb47-596" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> trained_task(val_images[<span class="st">'image'</span>])</span>
<span id="cb47-597"><a href="#cb47-597" aria-hidden="true" tabindex="-1"></a>    <span class="co"># outputs.output is a tensor of shape (batch_size, num_classes)</span></span>
<span id="cb47-598"><a href="#cb47-598" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We want to get the class with the highest probability</span></span>
<span id="cb47-599"><a href="#cb47-599" aria-hidden="true" tabindex="-1"></a>    <span class="co"># So we take the argmax of the output tensor</span></span>
<span id="cb47-600"><a href="#cb47-600" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This gives us the predicted class for each image in the batch</span></span>
<span id="cb47-601"><a href="#cb47-601" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> outputs.output.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb47-602"><a href="#cb47-602" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-603"><a href="#cb47-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-606"><a href="#cb47-606" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-607"><a href="#cb47-607" aria-hidden="true" tabindex="-1"></a><span class="co"># Check accuracy on this batch</span></span>
<span id="cb47-608"><a href="#cb47-608" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> (predictions <span class="op">==</span> val_labels).<span class="bu">sum</span>().item()</span>
<span id="cb47-609"><a href="#cb47-609" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> correct <span class="op">/</span> <span class="bu">len</span>(val_labels)</span>
<span id="cb47-610"><a href="#cb47-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-611"><a href="#cb47-611" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2%}</span><span class="ss">"</span>)</span>
<span id="cb47-612"><a href="#cb47-612" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted: </span><span class="sc">{</span>predictions<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-613"><a href="#cb47-613" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"True:      </span><span class="sc">{</span>val_labels<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-614"><a href="#cb47-614" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-615"><a href="#cb47-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-616"><a href="#cb47-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-617"><a href="#cb47-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-618"><a href="#cb47-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-619"><a href="#cb47-619" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualize predictions</span></span>
<span id="cb47-620"><a href="#cb47-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-623"><a href="#cb47-623" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-624"><a href="#cb47-624" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb47-625"><a href="#cb47-625" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-626"><a href="#cb47-626" aria-hidden="true" tabindex="-1"></a><span class="co"># Pick first 4 images to visualize</span></span>
<span id="cb47-627"><a href="#cb47-627" aria-hidden="true" tabindex="-1"></a>n_images <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb47-628"><a href="#cb47-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-629"><a href="#cb47-629" aria-hidden="true" tabindex="-1"></a><span class="co"># Re-define class names to match the order of the predictions and ground truth labels:</span></span>
<span id="cb47-630"><a href="#cb47-630" aria-hidden="true" tabindex="-1"></a>class_names <span class="op">=</span> [</span>
<span id="cb47-631"><a href="#cb47-631" aria-hidden="true" tabindex="-1"></a>    <span class="st">"AnnualCrop"</span>,</span>
<span id="cb47-632"><a href="#cb47-632" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Forest"</span>,</span>
<span id="cb47-633"><a href="#cb47-633" aria-hidden="true" tabindex="-1"></a>    <span class="st">"HerbaceousVegetation"</span>,</span>
<span id="cb47-634"><a href="#cb47-634" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Highway"</span>,</span>
<span id="cb47-635"><a href="#cb47-635" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Industrial"</span>,</span>
<span id="cb47-636"><a href="#cb47-636" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Pasture"</span>,</span>
<span id="cb47-637"><a href="#cb47-637" aria-hidden="true" tabindex="-1"></a>    <span class="st">"PermanentCrop"</span>,</span>
<span id="cb47-638"><a href="#cb47-638" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Residential"</span>,</span>
<span id="cb47-639"><a href="#cb47-639" aria-hidden="true" tabindex="-1"></a>    <span class="st">"River"</span>,</span>
<span id="cb47-640"><a href="#cb47-640" aria-hidden="true" tabindex="-1"></a>    <span class="st">"SeaLake"</span></span>
<span id="cb47-641"><a href="#cb47-641" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb47-642"><a href="#cb47-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-643"><a href="#cb47-643" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, n_images, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb47-644"><a href="#cb47-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-645"><a href="#cb47-645" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_images):</span>
<span id="cb47-646"><a href="#cb47-646" aria-hidden="true" tabindex="-1"></a>    <span class="co"># val_images['images'] is now a [batch, 6, 224, 224] tensor</span></span>
<span id="cb47-647"><a href="#cb47-647" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the i-th 6-channel image</span></span>
<span id="cb47-648"><a href="#cb47-648" aria-hidden="true" tabindex="-1"></a>    img_numpy <span class="op">=</span> unnormalized_images[i].cpu().numpy()</span>
<span id="cb47-649"><a href="#cb47-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-650"><a href="#cb47-650" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to_rgb expects a 6-channel CHW numpy image as input</span></span>
<span id="cb47-651"><a href="#cb47-651" aria-hidden="true" tabindex="-1"></a>    rgb <span class="op">=</span> to_rgb(img_numpy, bands<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb47-652"><a href="#cb47-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-653"><a href="#cb47-653" aria-hidden="true" tabindex="-1"></a>    axes[i].imshow(rgb)</span>
<span id="cb47-654"><a href="#cb47-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-655"><a href="#cb47-655" aria-hidden="true" tabindex="-1"></a>    true_label <span class="op">=</span> class_names[val_labels[i]]</span>
<span id="cb47-656"><a href="#cb47-656" aria-hidden="true" tabindex="-1"></a>    <span class="co"># predictions is used for predicted classes</span></span>
<span id="cb47-657"><a href="#cb47-657" aria-hidden="true" tabindex="-1"></a>    pred_label <span class="op">=</span> class_names[predictions[i]]</span>
<span id="cb47-658"><a href="#cb47-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-659"><a href="#cb47-659" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Color code: green if correct, red if wrong</span></span>
<span id="cb47-660"><a href="#cb47-660" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> <span class="st">'green'</span> <span class="cf">if</span> val_labels[i] <span class="op">==</span> predictions[i] <span class="cf">else</span> <span class="st">'red'</span></span>
<span id="cb47-661"><a href="#cb47-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-662"><a href="#cb47-662" aria-hidden="true" tabindex="-1"></a>    axes[i].set_title(<span class="ss">f"True: </span><span class="sc">{</span>true_label<span class="sc">}</span><span class="ch">\n</span><span class="ss">Pred: </span><span class="sc">{</span>pred_label<span class="sc">}</span><span class="ss">"</span>, color<span class="op">=</span>color)</span>
<span id="cb47-663"><a href="#cb47-663" aria-hidden="true" tabindex="-1"></a>    axes[i].axis(<span class="st">'off'</span>)</span>
<span id="cb47-664"><a href="#cb47-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-665"><a href="#cb47-665" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb47-666"><a href="#cb47-666" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb47-667"><a href="#cb47-667" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-668"><a href="#cb47-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-669"><a href="#cb47-669" aria-hidden="true" tabindex="-1"></a><span class="fu">### Get prediction probabilities</span></span>
<span id="cb47-670"><a href="#cb47-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-673"><a href="#cb47-673" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-674"><a href="#cb47-674" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the `predict` function to get predictions and probabilities</span></span>
<span id="cb47-675"><a href="#cb47-675" aria-hidden="true" tabindex="-1"></a><span class="co"># Make sure to apply the same transforms to the image as during training</span></span>
<span id="cb47-676"><a href="#cb47-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-677"><a href="#cb47-677" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb47-678"><a href="#cb47-678" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use the validation dataloader to get predictions and probabilities for a batch</span></span>
<span id="cb47-679"><a href="#cb47-679" aria-hidden="true" tabindex="-1"></a>    val_loader <span class="op">=</span> datamodule.val_dataloader()</span>
<span id="cb47-680"><a href="#cb47-680" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get a single batch from the validation dataloader (not yet transformed)</span></span>
<span id="cb47-681"><a href="#cb47-681" aria-hidden="true" tabindex="-1"></a>    val_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(val_loader))</span>
<span id="cb47-682"><a href="#cb47-682" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply the same transforms used during training/validation</span></span>
<span id="cb47-683"><a href="#cb47-683" aria-hidden="true" tabindex="-1"></a>    transformed_batch <span class="op">=</span> datamodule.aug(val_batch)</span>
<span id="cb47-684"><a href="#cb47-684" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Take first image, keep batch dimension [1, C, H, W]</span></span>
<span id="cb47-685"><a href="#cb47-685" aria-hidden="true" tabindex="-1"></a>    transformed_image <span class="op">=</span> transformed_batch[<span class="st">'image'</span>][<span class="dv">0</span>:<span class="dv">1</span>]  <span class="co"># Now properly normalized</span></span>
<span id="cb47-686"><a href="#cb47-686" aria-hidden="true" tabindex="-1"></a>    true_label <span class="op">=</span> val_batch[<span class="st">'label'</span>][<span class="dv">0</span>]</span>
<span id="cb47-687"><a href="#cb47-687" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-688"><a href="#cb47-688" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions using forward pass</span></span>
<span id="cb47-689"><a href="#cb47-689" aria-hidden="true" tabindex="-1"></a>    trained_task.<span class="bu">eval</span>()</span>
<span id="cb47-690"><a href="#cb47-690" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> trained_task(transformed_image)</span>
<span id="cb47-691"><a href="#cb47-691" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> output.output</span>
<span id="cb47-692"><a href="#cb47-692" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-693"><a href="#cb47-693" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predicted class</span></span>
<span id="cb47-694"><a href="#cb47-694" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> logits.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb47-695"><a href="#cb47-695" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb47-696"><a href="#cb47-696" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get probabilities using softmax</span></span>
<span id="cb47-697"><a href="#cb47-697" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb47-698"><a href="#cb47-698" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb47-699"><a href="#cb47-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-700"><a href="#cb47-700" aria-hidden="true" tabindex="-1"></a><span class="co"># `probs` is a tensor of shape (batch_size, num_classes)</span></span>
<span id="cb47-701"><a href="#cb47-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-702"><a href="#cb47-702" aria-hidden="true" tabindex="-1"></a><span class="co"># Look at the probabilities for the first image in the batch</span></span>
<span id="cb47-703"><a href="#cb47-703" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"First image probabilities:"</span>)</span>
<span id="cb47-704"><a href="#cb47-704" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, class_name <span class="kw">in</span> <span class="bu">enumerate</span>(class_names):</span>
<span id="cb47-705"><a href="#cb47-705" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>probs[<span class="dv">0</span>, i]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb47-706"><a href="#cb47-706" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-707"><a href="#cb47-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-708"><a href="#cb47-708" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bonus: Exploring model embeddings</span></span>
<span id="cb47-709"><a href="#cb47-709" aria-hidden="true" tabindex="-1"></a>Let's visualize the embeddings of the pretrained and fine-tuned models to see how the classification head might have learned to separate classes, even though the backbone embeddings remain unchanged.</span>
<span id="cb47-710"><a href="#cb47-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-711"><a href="#cb47-711" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualize embedding space with PCA</span></span>
<span id="cb47-712"><a href="#cb47-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-715"><a href="#cb47-715" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb47-716"><a href="#cb47-716" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb47-717"><a href="#cb47-717" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb47-718"><a href="#cb47-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-719"><a href="#cb47-719" aria-hidden="true" tabindex="-1"></a><span class="co"># Increase the number of points by using both validation and test images</span></span>
<span id="cb47-720"><a href="#cb47-720" aria-hidden="true" tabindex="-1"></a>all_embeddings <span class="op">=</span> []</span>
<span id="cb47-721"><a href="#cb47-721" aria-hidden="true" tabindex="-1"></a>all_labels <span class="op">=</span> []</span>
<span id="cb47-722"><a href="#cb47-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-723"><a href="#cb47-723" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb47-724"><a href="#cb47-724" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set up test dataset if available</span></span>
<span id="cb47-725"><a href="#cb47-725" aria-hidden="true" tabindex="-1"></a>    datamodule.setup(<span class="st">'test'</span>)</span>
<span id="cb47-726"><a href="#cb47-726" aria-hidden="true" tabindex="-1"></a>    loaders <span class="op">=</span> [datamodule.val_dataloader(), datamodule.test_dataloader()]</span>
<span id="cb47-727"><a href="#cb47-727" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb47-728"><a href="#cb47-728" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use validation (and test if available) sets for more points in PCA</span></span>
<span id="cb47-729"><a href="#cb47-729" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> loader <span class="kw">in</span> loaders:      </span>
<span id="cb47-730"><a href="#cb47-730" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch <span class="kw">in</span> loader:</span>
<span id="cb47-731"><a href="#cb47-731" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Apply the same transforms used during training/validation</span></span>
<span id="cb47-732"><a href="#cb47-732" aria-hidden="true" tabindex="-1"></a>            transformed_batch <span class="op">=</span> datamodule.aug(batch)</span>
<span id="cb47-733"><a href="#cb47-733" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> transformed_batch[<span class="st">'image'</span>]  <span class="co"># Now properly normalized</span></span>
<span id="cb47-734"><a href="#cb47-734" aria-hidden="true" tabindex="-1"></a>            labels <span class="op">=</span> batch[<span class="st">'label'</span>]</span>
<span id="cb47-735"><a href="#cb47-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-736"><a href="#cb47-736" aria-hidden="true" tabindex="-1"></a>            emb <span class="op">=</span> trained_task.model.encoder(images)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb47-737"><a href="#cb47-737" aria-hidden="true" tabindex="-1"></a>            emb_flat <span class="op">=</span> emb.mean(dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Global average pooling</span></span>
<span id="cb47-738"><a href="#cb47-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-739"><a href="#cb47-739" aria-hidden="true" tabindex="-1"></a>            all_embeddings.append(emb_flat.cpu())</span>
<span id="cb47-740"><a href="#cb47-740" aria-hidden="true" tabindex="-1"></a>            all_labels.append(labels.cpu())</span>
<span id="cb47-741"><a href="#cb47-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-742"><a href="#cb47-742" aria-hidden="true" tabindex="-1"></a>all_embeddings <span class="op">=</span> torch.cat(all_embeddings).numpy()</span>
<span id="cb47-743"><a href="#cb47-743" aria-hidden="true" tabindex="-1"></a>all_labels <span class="op">=</span> torch.cat(all_labels).numpy()</span>
<span id="cb47-744"><a href="#cb47-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-745"><a href="#cb47-745" aria-hidden="true" tabindex="-1"></a><span class="co"># PCA to 2D</span></span>
<span id="cb47-746"><a href="#cb47-746" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb47-747"><a href="#cb47-747" aria-hidden="true" tabindex="-1"></a>embeddings_2d <span class="op">=</span> pca.fit_transform(all_embeddings)</span>
<span id="cb47-748"><a href="#cb47-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-749"><a href="#cb47-749" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot with class names as legend entries</span></span>
<span id="cb47-750"><a href="#cb47-750" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb47-751"><a href="#cb47-751" aria-hidden="true" tabindex="-1"></a>scatter <span class="op">=</span> plt.scatter(</span>
<span id="cb47-752"><a href="#cb47-752" aria-hidden="true" tabindex="-1"></a>    embeddings_2d[:, <span class="dv">0</span>], embeddings_2d[:, <span class="dv">1</span>],</span>
<span id="cb47-753"><a href="#cb47-753" aria-hidden="true" tabindex="-1"></a>    c<span class="op">=</span>all_labels, cmap<span class="op">=</span><span class="st">'tab10'</span>, alpha<span class="op">=</span><span class="fl">0.6</span></span>
<span id="cb47-754"><a href="#cb47-754" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb47-755"><a href="#cb47-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-756"><a href="#cb47-756" aria-hidden="true" tabindex="-1"></a><span class="co"># Build legend mapping colors back to class names</span></span>
<span id="cb47-757"><a href="#cb47-757" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.lines <span class="im">import</span> Line2D</span>
<span id="cb47-758"><a href="#cb47-758" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb47-759"><a href="#cb47-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-760"><a href="#cb47-760" aria-hidden="true" tabindex="-1"></a>handles <span class="op">=</span> []</span>
<span id="cb47-761"><a href="#cb47-761" aria-hidden="true" tabindex="-1"></a>unique_labels <span class="op">=</span> np.unique(all_labels)</span>
<span id="cb47-762"><a href="#cb47-762" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> unique_labels:</span>
<span id="cb47-763"><a href="#cb47-763" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> scatter.cmap(scatter.norm(label))</span>
<span id="cb47-764"><a href="#cb47-764" aria-hidden="true" tabindex="-1"></a>    handles.append(Line2D(</span>
<span id="cb47-765"><a href="#cb47-765" aria-hidden="true" tabindex="-1"></a>        [<span class="dv">0</span>], [<span class="dv">0</span>], marker<span class="op">=</span><span class="st">'o'</span>, color<span class="op">=</span><span class="st">'w'</span>, label<span class="op">=</span>class_names[label],</span>
<span id="cb47-766"><a href="#cb47-766" aria-hidden="true" tabindex="-1"></a>        markerfacecolor<span class="op">=</span>color, markersize<span class="op">=</span><span class="dv">10</span>, alpha<span class="op">=</span><span class="fl">0.6</span></span>
<span id="cb47-767"><a href="#cb47-767" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb47-768"><a href="#cb47-768" aria-hidden="true" tabindex="-1"></a>plt.legend(handles<span class="op">=</span>handles, title<span class="op">=</span><span class="st">"Class"</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>), loc<span class="op">=</span><span class="st">'upper left'</span>)</span>
<span id="cb47-769"><a href="#cb47-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-770"><a href="#cb47-770" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Validation &amp; Test Set Embeddings (PCA)</span><span class="ch">\n</span><span class="st">(same for pretrained &amp; fine-tuned)'</span>)</span>
<span id="cb47-771"><a href="#cb47-771" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'PC1'</span>)</span>
<span id="cb47-772"><a href="#cb47-772" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'PC2'</span>)</span>
<span id="cb47-773"><a href="#cb47-773" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb47-774"><a href="#cb47-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-775"><a href="#cb47-775" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb47-776"><a href="#cb47-776" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/geog-logo.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Department of Geography logo</figcaption>
</figure>
</div>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is built with <a href="https://github.com/kcaylor/GEOG-288KC-geospatial-foundation-models"><i class="fa-brands fa-github" title="the github octocat logo" aria-label="github"></i></a> and <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>