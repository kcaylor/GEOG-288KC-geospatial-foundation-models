<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Object Detection with Prithvi</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">GEOG 288KC</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">üè† home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Syllabus.html"> 
<span class="menu-text">üìã syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-weekly-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üíª weekly sessions</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-weekly-sessions">    
        <li>
    <a class="dropdown-item" href="../../chapters/c01-geospatial-data-foundations.html">
 <span class="dropdown-text">Week 1 - üöÄ Core Tools and Data Access</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c02-spatial-temporal-attention-mechanisms.html">
 <span class="dropdown-text">Week 2 - ‚ö° Rapid Remote Sensing Preprocessing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c03a-terratorch-foundations.html">
 <span class="dropdown-text">Week 3a - üåç TerraTorch Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c03-complete-gfm-architecture.html">
 <span class="dropdown-text">Week 3b - ü§ñ Machine Learning on Remote Sensing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c04-pretraining-implementation.html">
 <span class="dropdown-text">Week 4 - üèóÔ∏è Foundation Models in Practice</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c05-training-loop-optimization.html">
 <span class="dropdown-text">Week 5 - üîß Fine-Tuning &amp; Transfer Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c06-model-evaluation-analysis.html">
 <span class="dropdown-text">Week 6 - ‚è∞ Spatiotemporal Modeling &amp; Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-cheatsheets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üëÄ cheatsheets</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-cheatsheets">    
        <li>
    <a class="dropdown-item" href="../../cheatsheets.html">
 <span class="dropdown-text">üìã All Cheatsheets</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">‚ö° Quick Starts</li>
        <li>
    <a class="dropdown-item" href="../../extras/cheatsheets/week01_imports.html">
 <span class="dropdown-text">Week 01: Import Guide</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-explainers" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üß© explainers</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-explainers">    
        <li class="dropdown-header">1Ô∏è‚É£ Week 1</li>
        <li>
    <a class="dropdown-item" href="../../extras/ai-ml-dl-fm-hierarchy.html">
 <span class="dropdown-text">ü§ñ AI/ML/DL/FM Hierarchy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-foundation-model-predictions-standalone.html">
 <span class="dropdown-text">üéØ GFM Predictions (Standalone)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-prediction-hierarchy.html">
 <span class="dropdown-text">‚úÖ Geospatial Task/Prediction Types</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/neural_networks_explainer.html">
 <span class="dropdown-text">üß† Neural Networks: Neurons to Transformers</span></a>
  </li>  
        <li class="dropdown-header">2Ô∏è‚É£ Week 2</li>
        <li>
    <a class="dropdown-item" href="../../chapters/c00a-foundation_model_architectures.html">
 <span class="dropdown-text">üèóÔ∏è Foundation Model Architectures</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c00b-introduction-to-deeplearning-architecture.html">
 <span class="dropdown-text">üéì Introduction to Deep Learning Architecture</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-extras" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">üìñ extras</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-extras">    
        <li class="dropdown-header">üéØ Practical Examples</li>
        <li>
    <a class="dropdown-item" href="../../extras/examples/normalization_comparison.html">
 <span class="dropdown-text">Normalization Comparison</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/resnet.html">
 <span class="dropdown-text">ResNet Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/segmentation_finetuning.html">
 <span class="dropdown-text">Segmentation Fine-Tuning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/Terramind_EuroSAT.html">
 <span class="dropdown-text">TerraMind EuroSAT Classification</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/text_encoder.html">
 <span class="dropdown-text">Text Encoder</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/tiling-and-patches.html">
 <span class="dropdown-text">Tiling and Patches</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/terratorch_workflows.html">
 <span class="dropdown-text">TerraTorch Workflows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/resources/course_resources.html">
 <span class="dropdown-text">üìö Reference Materials</span></a>
  </li>  
        <li class="dropdown-header">üìÅ Project Templates</li>
        <li>
    <a class="dropdown-item" href="../../extras/projects/project-proposal-template.html">
 <span class="dropdown-text">Project Proposal Template</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/projects/mvp-template.html">
 <span class="dropdown-text">Project Results Template</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gfms-from-scratch/gfms-from-scratch.github.io" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Object Detection with Prithvi</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Fine-tuning Prithvi GFM on the DIOR aerial imagery dataset</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a>
  <ul class="collapse">
  <li><a href="#load-the-dior-dataset" id="toc-load-the-dior-dataset" class="nav-link" data-scroll-target="#load-the-dior-dataset">Load the DIOR Dataset</a></li>
  <li><a href="#understanding-object-detection-architecture" id="toc-understanding-object-detection-architecture" class="nav-link" data-scroll-target="#understanding-object-detection-architecture">Understanding Object Detection Architecture</a></li>
  <li><a href="#setting-up-object-detection-with-terratorch" id="toc-setting-up-object-detection-with-terratorch" class="nav-link" data-scroll-target="#setting-up-object-detection-with-terratorch">Setting Up Object Detection with TerraTorch</a></li>
  </ul></li>
  <li><a href="#prepare-data-for-training" id="toc-prepare-data-for-training" class="nav-link" data-scroll-target="#prepare-data-for-training">Prepare Data for Training</a></li>
  <li><a href="#train-the-object-detection-model" id="toc-train-the-object-detection-model" class="nav-link" data-scroll-target="#train-the-object-detection-model">Train the Object Detection Model</a></li>
  <li><a href="#debug-model-predictions" id="toc-debug-model-predictions" class="nav-link" data-scroll-target="#debug-model-predictions">Debug Model Predictions</a></li>
  <li><a href="#visualize-predictions-vs-ground-truth" id="toc-visualize-predictions-vs-ground-truth" class="nav-link" data-scroll-target="#visualize-predictions-vs-ground-truth">Visualize Predictions vs Ground Truth</a></li>
  <li><a href="#summary-and-key-takeaways" id="toc-summary-and-key-takeaways" class="nav-link" data-scroll-target="#summary-and-key-takeaways">Summary and Key Takeaways</a>
  <ul class="collapse">
  <li><a href="#complete-pipeline-built" id="toc-complete-pipeline-built" class="nav-link" data-scroll-target="#complete-pipeline-built">‚úÖ Complete Pipeline Built</a></li>
  <li><a href="#key-lessons" id="toc-key-lessons" class="nav-link" data-scroll-target="#key-lessons">üéì Key Lessons</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">üöÄ Next Steps</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">üìö Further Reading</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>This tutorial demonstrates object detection using the <strong>Prithvi geospatial foundation model</strong> on the DIOR aerial imagery dataset. You‚Äôll learn how to adapt a Vision Transformer (ViT) backbone for object detection, solving the architectural challenges that arise when integrating ViTs with traditional detection frameworks.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What You‚Äôll Learn
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Dataset &amp; Pipeline:</strong> - Load DIOR (23,463 aerial images, 20 classes) from Hugging Face - Build complete detection pipeline: <strong>Prithvi ViT ‚Üí Faster R-CNN</strong> (single-scale) - Create custom data collation for ViT-friendly 512√ó512 images</p>
<p><strong>ViT Detection Challenges &amp; Solutions:</strong> - <strong>Challenge</strong>: Faster R-CNN expects multi-scale CNN features (FPN), ViT gives single-scale sequences - <strong>Solution 1</strong>: Skip FPN, work directly on ViT‚Äôs 32√ó32 feature map - <strong>Solution 2</strong>: Custom single-scale RPN with 15,360 anchors tuned for aerial imagery - <strong>Solution 3</strong>: Replace MultiScaleRoIAlign with single-scale RoIAlign - <strong>Solution 4</strong>: Custom forward pass to reshape ViT sequences ‚Üí spatial features</p>
<p><strong>Training &amp; Evaluation:</strong> - Train object detection model with automatic GPU/CPU support - Monitor RPN and ROI losses separately - Debug predictions with confidence score analysis - Visualize detections vs ground truth</p>
<p>By the end, you‚Äôll understand how to integrate ANY ViT-based GFM with object detection frameworks!</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why DIOR Is Perfect for Prithvi
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>The VHR10 Problem:</strong> - Variable image sizes (586√ó716 to 958√ó1024) - ViTs need consistent input sizes - Required complex padding/resizing logic</p>
<p><strong>The DIOR Solution:</strong> - ‚úÖ <strong>Fixed 800√ó800 pixel images</strong> - Every single image! - ‚úÖ <strong>23,463 images</strong> - Much larger dataset - ‚úÖ <strong>20 object classes</strong> - More diversity - ‚úÖ <strong>Simple resize to 512√ó512 or 224√ó224</strong> - No padding needed</p>
<p>This makes DIOR the ideal dataset for testing Prithvi-based object detection!</p>
</div>
</div>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p><strong>Object Detection vs.&nbsp;Segmentation:</strong> - <strong>Object Detection</strong>: Localizes and classifies objects in an image - <strong>Segmentation</strong>: Assigns a label to each pixel independently</p>
<section id="load-the-dior-dataset" class="level3">
<h3 class="anchored" data-anchor-id="load-the-dior-dataset">Load the DIOR Dataset</h3>
<p>We are using the HichTala/dior dataset from Hugging Face, which has proper bounding box annotations.</p>
<p>Citation: <span class="citation" data-cites="article">@article</span>{Li_2020, title={Object detection in optical remote sensing images: A survey and a new benchmark}, volume={159}, ISSN={0924-2716}, url={http://dx.doi.org/10.1016/j.isprsjprs.2019.11.023}, DOI={10.1016/j.isprsjprs.2019.11.023}, journal={ISPRS Journal of Photogrammetry and Remote Sensing}, publisher={Elsevier BV}, author={Li, Ke and Wan, Gang and Cheng, Gong and Meng, Liqiu and Han, Junwei}, year={2020}, month=jan, pages={296‚Äì307}}</p>
<p>The <strong>DIOR (Dataset for Object deteCtIon in aerial Images)</strong> is an excellent dataset for object detection in aerial imagery, hosted on <a href="https://huggingface.co/datasets/torchgeo/dior">Hugging Face</a>.</p>
<p><strong>Key Features:</strong> - <strong>23,463 total images</strong> (19,000 train, 4,463 test) - <strong>All images are 800√ó800 pixels</strong> (consistent size!) - <strong>20 object classes</strong> common in aerial imagery - <strong>Standard train/test splits</strong></p>
<p><strong>Classes (20 total):</strong> airplane, airport, baseball field, basketball court, bridge, chimney, dam, expressway service area, expressway toll station, golf course, ground track field, harbor, overpass, ship, stadium, storage tank, tennis court, train station, vehicle, wind mill</p>
<p>Objects are annotated with bounding boxes in <code>[xmin, ymin, xmax, ymax]</code> format.</p>
<div id="69c89f4e" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load DIOR from Hugging Face (COCO format with annotations!)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Using HichTala/dior which has proper bounding box annotations</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading DIOR from Hugging Face (with annotations)..."</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>hf_train <span class="op">=</span> load_dataset(<span class="st">"HichTala/dior"</span>, split<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Training samples: </span><span class="sc">{</span><span class="bu">len</span>(hf_train)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì All images are 800√ó800 pixels (perfect for ViT!)"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Examine structure</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> hf_train[<span class="dv">0</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">‚úì Sample keys: </span><span class="sc">{</span><span class="bu">list</span>(sample.keys())<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  image_id: </span><span class="sc">{</span>sample[<span class="st">'image_id'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  width: </span><span class="sc">{</span>sample[<span class="st">'width'</span>]<span class="sc">}</span><span class="ss">, height: </span><span class="sc">{</span>sample[<span class="st">'height'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  num objects: </span><span class="sc">{</span><span class="bu">len</span>(sample[<span class="st">'objects'</span>][<span class="st">'category'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Show first object</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(sample[<span class="st">'objects'</span>][<span class="st">'category'</span>]) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">‚úì First object:"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  category: </span><span class="sc">{</span>sample[<span class="st">'objects'</span>][<span class="st">'category'</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  bbox: </span><span class="sc">{</span>sample[<span class="st">'objects'</span>][<span class="st">'bbox'</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># [x, y, w, h] COCO format</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  area: </span><span class="sc">{</span>sample[<span class="st">'objects'</span>][<span class="st">'area'</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let‚Äôs create a PyTorch dataset wrapper and visualize DIOR samples:</p>
<div id="43708819" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># DIOR class names (20 classes)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>DIOR_CLASSES <span class="op">=</span> [</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'airplane'</span>, <span class="st">'airport'</span>, <span class="st">'baseball field'</span>, <span class="st">'basketball court'</span>, <span class="st">'bridge'</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'chimney'</span>, <span class="st">'dam'</span>, <span class="st">'expressway service area'</span>, <span class="st">'expressway toll station'</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'golf course'</span>, <span class="st">'ground track field'</span>, <span class="st">'harbor'</span>, <span class="st">'overpass'</span>, <span class="st">'ship'</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'stadium'</span>, <span class="st">'storage tank'</span>, <span class="st">'tennis court'</span>, <span class="st">'train station'</span>, <span class="st">'vehicle'</span>, <span class="st">'wind mill'</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># First, inspect what keys HuggingFace DIOR has</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> hf_train[<span class="dv">0</span>]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DIOR sample structure:"</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key <span class="kw">in</span> sample.keys():</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    val <span class="op">=</span> sample[key]</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">type</span>(val)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(val, (<span class="bu">list</span>, <span class="bu">dict</span>)):</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"    Content: </span><span class="sc">{</span>val<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let‚Äôs create a proper PyTorch dataset wrapper:</p>
<div id="c834a17b" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DIORPyTorchDataset(torch.utils.data.Dataset):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert Hugging Face DIOR (COCO format) to PyTorch detection format.</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Input (COCO format):</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - bbox: [x, y, width, height]</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">    - category: class index</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Output (torchvision format):</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    - image: Tensor [3, 800, 800]</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - bbox_xyxy: Tensor [N, 4] in [xmin, ymin, xmax, ymax] format</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">    - label: Tensor [N]</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hf_dataset):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hf_dataset <span class="op">=</span> hf_dataset</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.hf_dataset)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> <span class="va">self</span>.hf_dataset[idx]</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert PIL image to tensor [C, H, W]</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> sample[<span class="st">'image'</span>]</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(image, Image.Image):</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> torch.from_numpy(np.array(image)).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>).<span class="bu">float</span>() <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract bounding boxes (COCO format: [x, y, w, h])</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        bboxes_coco <span class="op">=</span> sample[<span class="st">'objects'</span>][<span class="st">'bbox'</span>]  <span class="co"># List of [x, y, w, h]</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> sample[<span class="st">'objects'</span>][<span class="st">'category'</span>]   <span class="co"># List of class indices</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert COCO [x, y, w, h] to xyxy [xmin, ymin, xmax, ymax]</span></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        bboxes_xyxy <span class="op">=</span> []</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> bbox <span class="kw">in</span> bboxes_coco:</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            x, y, w, h <span class="op">=</span> bbox</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>            xmin, ymin <span class="op">=</span> x, y</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>            xmax, ymax <span class="op">=</span> x <span class="op">+</span> w, y <span class="op">+</span> h</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>            bboxes_xyxy.append([xmin, ymin, xmax, ymax])</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert to tensors</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">=</span> torch.tensor(bboxes_xyxy, dtype<span class="op">=</span>torch.float32) <span class="cf">if</span> bboxes_xyxy <span class="cf">else</span> torch.zeros((<span class="dv">0</span>, <span class="dv">4</span>), dtype<span class="op">=</span>torch.float32)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.tensor(labels, dtype<span class="op">=</span>torch.int64) <span class="cf">if</span> labels <span class="cf">else</span> torch.zeros((<span class="dv">0</span>,), dtype<span class="op">=</span>torch.int64)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>            <span class="st">'image'</span>: image,</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>            <span class="st">'bbox_xyxy'</span>: boxes,</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>            <span class="st">'label'</span>: labels</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataset</span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>dataset_train <span class="op">=</span> DIORPyTorchDataset(hf_train)</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Test it</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a>test_sample <span class="op">=</span> dataset_train[<span class="dv">0</span>]</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">‚úì PyTorch dataset created"</span>)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Image shape: </span><span class="sc">{</span>test_sample[<span class="st">'image'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Bounding boxes: </span><span class="sc">{</span>test_sample[<span class="st">'bbox_xyxy'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Labels: </span><span class="sc">{</span>test_sample[<span class="st">'label'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Expected: [3, 800, 800] images with variable number of objects"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now let‚Äôs visualize some DIOR samples with their annotations:</p>
<div id="4fb6303f" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> patches</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># DIOR class names</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>DIOR_CLASSES <span class="op">=</span> [</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Airplane'</span>, <span class="st">'Airport'</span>, <span class="st">'Baseball field'</span>, <span class="st">'Basketball court'</span>, <span class="st">'Bridge'</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Chimney'</span>, <span class="st">'Dam'</span>, <span class="st">'Expressway service area'</span>, <span class="st">'Expressway toll station'</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Golf course'</span>, <span class="st">'Ground track field'</span>, <span class="st">'Harbor'</span>, <span class="st">'Overpass'</span>, <span class="st">'Ship'</span>,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Stadium'</span>, <span class="st">'Storage tank'</span>, <span class="st">'Tennis court'</span>, <span class="st">'Train station'</span>, <span class="st">'Vehicle'</span>, <span class="st">'Wind mill'</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize 4 samples</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> dataset_train[idx]</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get image</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> sample[<span class="st">'image'</span>].permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy()  <span class="co"># [C,H,W] ‚Üí [H,W,C]</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get boxes and labels  </span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    boxes <span class="op">=</span> sample[<span class="st">'bbox_xyxy'</span>]</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> sample[<span class="st">'label'</span>]</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[idx]</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    ax.imshow(image)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw bounding boxes</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> box, label <span class="kw">in</span> <span class="bu">zip</span>(boxes, labels):</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        xmin, ymin, xmax, ymax <span class="op">=</span> box.numpy()</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        width <span class="op">=</span> xmax <span class="op">-</span> xmin</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        height <span class="op">=</span> ymax <span class="op">-</span> ymin</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rectangle</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        rect <span class="op">=</span> patches.Rectangle(</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>            (xmin, ymin), width, height,</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>            linewidth<span class="op">=</span><span class="dv">2</span>, edgecolor<span class="op">=</span><span class="st">'red'</span>, facecolor<span class="op">=</span><span class="st">'none'</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        ax.add_patch(rect)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Label</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> DIOR_CLASSES[<span class="bu">int</span>(label)]</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        ax.text(</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>            xmin, ymin <span class="op">-</span> <span class="dv">5</span>,</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>            class_name,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span><span class="st">'white'</span>,</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">9</span>,</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>            weight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>            bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'off'</span>)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"Sample </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">len</span>(boxes)<span class="sc">}</span><span class="ss"> object(s)"</span>, fontsize<span class="op">=</span><span class="dv">12</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">"DIOR Dataset Examples (800√ó800 pixels)"</span>, fontsize<span class="op">=</span><span class="dv">16</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="understanding-object-detection-architecture" class="level3">
<h3 class="anchored" data-anchor-id="understanding-object-detection-architecture">Understanding Object Detection Architecture</h3>
<p>Object detection requires more than just a backbone model - we need a complete detection pipeline:</p>
<ol type="1">
<li><strong>Backbone</strong>: Extract features from the image (e.g., Prithvi ViT)</li>
<li><strong>Neck</strong> (optional): Process features at multiple scales (e.g., FPN)</li>
<li><strong>Detection Head</strong>: Predict bounding boxes and classes (e.g., Faster R-CNN, RetinaNet)</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Object Detection is Complex
</div>
</div>
<div class="callout-body-container callout-body">
<p>Unlike classification or segmentation, object detection must: - Predict variable numbers of objects per image - Localize objects with bounding boxes - Handle objects at different scales - Deal with overlapping predictions (Non-Maximum Suppression)</p>
<p>This complexity means we typically need specialized frameworks like <strong>Detectron2</strong>, <strong>MMDetection</strong>, or <strong>TerraTorch</strong> that provide pre-built detection pipelines.</p>
</div>
</div>
</section>
<section id="setting-up-object-detection-with-terratorch" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-object-detection-with-terratorch">Setting Up Object Detection with TerraTorch</h3>
<p>Use the TerraTorch Object Detection Model Factory to create a model for object detection. The complete pipeline has three components:</p>
<ol type="1">
<li><strong>Backbone (Prithvi)</strong>: Extracts features from input images</li>
<li><strong>Neck (Optional - FPN)</strong>: Can transform features into multi-scale pyramid</li>
<li><strong>Head (Faster R-CNN)</strong>: Predicts bounding boxes and classes from features</li>
</ol>
<p>For ViT, we skip the neck and use single-scale detection directly.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why Skip the Neck for ViT?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Traditional CNN detectors use FPN to create multi-scale features. However:</p>
<ul>
<li><strong>Prithvi ViT</strong>: Outputs single-scale features (32√ó32)</li>
<li><strong>FPN Complexity</strong>: Feature map filtering and multi-scale expectations</li>
<li><strong>DIOR Dataset</strong>: Consistent image sizes and object scales</li>
</ul>
<p><strong>Our approach</strong>: Skip FPN entirely and use <strong>single-scale detection</strong> directly on ViT features. This is simpler and avoids compatibility issues.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Working with TerraTorch Object Detection Models
</div>
</div>
<div class="callout-body-container callout-body">
<p>TerraTorch‚Äôs <code>ObjectDetectionModel</code> is a wrapper around torchvision‚Äôs detection models. While the wrapper is useful for model construction, it has some quirks during training:</p>
<p><strong>The Issue</strong>: The wrapper‚Äôs <code>forward()</code> method doesn‚Äôt properly pass <code>targets</code> to the underlying model during training mode.</p>
<p><strong>The Solution</strong>: Extract the underlying torchvision model using <code>model.torchvision_model</code> and train that directly.</p>
<p>This is a common pattern when working with model wrappers - use them for construction, but train the underlying model directly.</p>
</div>
</div>
<div id="ed36889a" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.models.object_detection_model_factory <span class="im">import</span> ObjectDetectionModelFactory</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.anchor_utils <span class="im">import</span> AnchorGenerator</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.rpn <span class="im">import</span> RegionProposalNetwork, RPNHead</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> types</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>model_factory <span class="op">=</span> ObjectDetectionModelFactory()</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model_factory.build_model(</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    task<span class="op">=</span><span class="st">"object_detection"</span>,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    backbone<span class="op">=</span><span class="st">"prithvi_eo_v1_100"</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    backbone_bands<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    backbone_num_frames<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    backbone_pretrained<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    neck<span class="op">=</span><span class="va">None</span>,  <span class="co"># No FPN - simpler single-scale detection</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    framework<span class="op">=</span><span class="st">"faster-rcnn"</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(model, <span class="st">'torchvision_model'</span>):</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    detection_model <span class="op">=</span> model.torchvision_model</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    detection_model <span class="op">=</span> model</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix image transform to keep 512√ó512 size (don't resize)</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>detection_model.transform.min_size <span class="op">=</span> (<span class="dv">512</span>,)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>detection_model.transform.max_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model created with Prithvi backbone"</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image transform configured: 512√ó512 (no resizing)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we need to configure custom anchors for ViT‚Äôs single-scale features:</p>
<div id="06aa1a97" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Anchor Generator and Custom Region Proposal Network for ViT Single-Scale Detection ---</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_vit_anchor_generator():</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Create an AnchorGenerator specifically for Prithvi ViT's single-scale feature map.</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    In the classic Faster R-CNN pipeline, anchors are generated over multiple feature map scales (FPN).</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Since our ViT backbone outputs just a single feature map (no FPN), we use a single set of anchor sizes and ratios:</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co">    For 512x512 input with patch_size=16:</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - ViT outputs a single [32x32] feature map (stride 16).</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">    - Each feature location has anchors of sizes: 32, 64, 128, 256, 512 pixels.</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co">    - Each anchor size is combined with aspect ratios: 0.5 (tall), 1.0 (square), 2.0 (wide).</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co">    This ensures the RPN can propose boxes covering a wide range of object sizes/shapes</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co">    despite only a single feature resolution.</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Anchor sizes tuned for objects in 512√ó512 images (DIOR resized from 800√ó800)</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cover range: small vehicles (~20px) to large buildings/airports (~200px)</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    anchor_sizes <span class="op">=</span> ((<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>),)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    aspect_ratios <span class="op">=</span> ((<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>),)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> AnchorGenerator(sizes<span class="op">=</span>anchor_sizes, aspect_ratios<span class="op">=</span>aspect_ratios)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> replace_rpn(model):</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Replace the model's RPN (Region Proposal Network) with a version </span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co">    compatible with ViT's single-scale features.</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="co">    - in_channels: Set to Prithvi's hidden dimension (768 for ViT-base)</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="co">                   because that's the channel count of the feature map.</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co">    - anchor_generator: Single-scale anchor generator from above.</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="co">    - RPNHead: Computes objectness and box regression deltas per anchor.</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="co">    - RegionProposalNetwork: Uses anchors and predictions to generate </span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="co">                            object proposal regions. All RPN parameters</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="co">                            are tuned to standard values but operate only</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="co">                            on the single ViT feature map (not FPN).</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="co">    This customization is required because the default RPN expects FPN-style input,</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="co">    but our model has just one feature map level (from ViT).</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    in_channels <span class="op">=</span> <span class="dv">768</span>  <span class="co"># Prithvi ViT-Base hidden dimension</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    anchor_generator <span class="op">=</span> create_vit_anchor_generator()</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># RPNHead = conv layers predicting objectness &amp; bbox offsets for each anchor at each location.</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    rpn_head <span class="op">=</span> RPNHead(</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>        in_channels<span class="op">=</span>in_channels,</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>        num_anchors<span class="op">=</span>anchor_generator.num_anchors_per_location()[<span class="dv">0</span>]</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The actual RPN module, configured for single-scale input.</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    rpn <span class="op">=</span> RegionProposalNetwork(</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>        anchor_generator<span class="op">=</span>anchor_generator,</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>        head<span class="op">=</span>rpn_head,</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>        fg_iou_thresh<span class="op">=</span><span class="fl">0.7</span>,    <span class="co"># IoU threshold for matching anchors to GT boxes (foreground)</span></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>        bg_iou_thresh<span class="op">=</span><span class="fl">0.3</span>,    <span class="co"># IoU threshold for matching anchors as background</span></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>        batch_size_per_image<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>        positive_fraction<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>        pre_nms_top_n<span class="op">=</span><span class="bu">dict</span>(training<span class="op">=</span><span class="dv">2000</span>, testing<span class="op">=</span><span class="dv">1000</span>),</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>        post_nms_top_n<span class="op">=</span><span class="bu">dict</span>(training<span class="op">=</span><span class="dv">2000</span>, testing<span class="op">=</span><span class="dv">1000</span>),</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>        nms_thresh<span class="op">=</span><span class="fl">0.7</span>         <span class="co"># Non-max suppression threshold for RPN proposals</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Override the model's RPN with our custom single-scale RPN.</span></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>    model.rpn <span class="op">=</span> rpn</span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Insert our custom single-scale RPN into the model pipeline.</span></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>detection_model <span class="op">=</span> replace_rpn(detection_model)</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Custom RPN configured for ViT features"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now replace the ROI pooler to work with single-scale features:</p>
<div id="4ee40c40" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops <span class="im">import</span> RoIAlign</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> replace_roi_pooler(model):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Replace the default multi-scale ROI pooler in Faster R-CNN with a</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">    custom single-scale ROIAlign module suitable for ViT-style (single feature map) backbones.</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Why:</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Faster R-CNN defaults to MultiScaleRoIAlign, which expects a dict of feature maps at </span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">        multiple pyramid levels (as with FPN). Vision Transformers output a single feature map,</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">        so we need a simpler pooling module that operates on just one map.</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co">    What this does:</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">      - Instantiates a single-scale RoIAlign set to a 7x7 output, which is standard for the detection head.</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">      - Uses a spatial scale of 1/16, as Prithvi/ViT feature stride is 16 pixels (512 input -&gt; 32x32 features).</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">      - Defines a custom nn.Module that handles both dict and tensor feature inputs, always pulling out</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">        the only feature map available, and ignoring FPN-style logic.</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">      - Replaces model.roi_heads.box_roi_pool with this new single-scale implementation.</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co">        The modified model, ready for use with a ViT backbone.</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create single-scale RoIAlign; output size 7x7 is default for detection heads</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    roi_align <span class="op">=</span> RoIAlign(</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        output_size<span class="op">=</span><span class="dv">7</span>,</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        spatial_scale<span class="op">=</span><span class="fl">1.0</span><span class="op">/</span><span class="dv">16</span>,  <span class="co"># ViT stride=16 (e.g., input 512 -&gt; 32x32)</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        sampling_ratio<span class="op">=</span><span class="dv">2</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">class</span> SingleScaleRoIPool(nn.Module):</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="co">        A wrapper for single-scale RoIAlign to mimic expected interface of MultiScaleRoIAlign,</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="co">        allowing seamless plug-in to torchvision's Faster R-CNN code.</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, roi_align):</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.roi_align <span class="op">=</span> roi_align</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> forward(<span class="va">self</span>, features, proposals, image_shapes):</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>            <span class="co">"""</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="co">            Apply RoIAlign to a single feature map.</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="co">            - features: Either a dict (assumed to contain only one entry) or a Tensor.</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="co">            - proposals: List of proposal boxes per image (as required by RoIAlign).</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a><span class="co">            - image_shapes: (Ignored) present for API compatibility only.</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="co">            Returns: Tensor of pooled RoI features.</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="co">            """</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If features is a dict (from model), extract its sole value; otherwise, use it as-is</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(features, <span class="bu">dict</span>):</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> <span class="bu">list</span>(features.values())[<span class="dv">0</span>]</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> features</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>            <span class="co"># image_shapes argument is not used, as RoIAlign doesn't require it for single-scale</span></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.roi_align(x, proposals)</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Swap out multi-scale RoI pooler for single-scale version</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    model.roi_heads.box_roi_pool <span class="op">=</span> SingleScaleRoIPool(roi_align)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch the model to use the new single-scale ROI pooler</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>detection_model <span class="op">=</span> replace_roi_pooler(detection_model)</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Custom ROI pooler configured for single-scale features"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Four Key Adaptations for ViT Detection
</div>
</div>
<div class="callout-body-container callout-body">
<p>To make Prithvi ViT work with Faster R-CNN, we made four critical changes:</p>
<ol type="1">
<li><strong>Skip FPN (neck=None)</strong>: Avoid multi-scale complexity, work directly on single 32√ó32 feature map</li>
<li><strong>Custom RPN</strong>: Single-scale anchor generator with 15,360 anchors tuned for 512√ó512 images</li>
<li><strong>Custom ROI Pooler</strong>: Replace MultiScaleRoIAlign with single-scale RoIAlign at 1/16 spatial scale</li>
<li><strong>Custom Forward Pass</strong>: Reshape ViT sequence [B,1024,768] ‚Üí spatial [B,768,32,32] with MPS compatibility</li>
</ol>
<p>These adaptations allow single-scale detection without FPN complexity while maintaining full compatibility with Faster R-CNN‚Äôs architecture.</p>
</div>
</div>
<p>Apply the compatibility patch for ViT features:</p>
<div id="26bc1fcf" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fixed_fasterrcnn_forward(<span class="va">self</span>, images, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Custom forward pass to enable Faster R-CNN with Prithvi ViT backbone.</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    This replaces the original forward to handle single-scale ViT features, which differ</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">    from typical multi-scale CNN backbones (like FPN).</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Steps performed:</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Converts ViT sequence output from shape [B, N, C] into a spatial feature map [B, C, H, W], </span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">        where:</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">         B=batch size, </span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">         N=number of tokens (patches), </span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">         C=channels, and </span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co">         H and W are the height and width of the feature map (typically recovered from N as H √ó W = N).</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co">       - Handles [CLS] token (N==1025) by removing the first token.</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co">       - Reshapes flat sequence into square map (ViT patch output).</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Wraps features in an OrderedDict, preserving required interface.</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co">       - Ensures tensor data is contiguous for MPS (Apple Silicon) and general backend compatibility.</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Proceeds with proposal, ROI, and loss computation as usual.</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co">    4. Returns post-processed detections or losses depending on training mode.</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co">        images: List of input images, already as Tensors.</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co">        targets: List of ground truth targets (boxes/labels). Required in training mode.</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co">        Loss dict (training mode) or list of detection outputs (eval mode).</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training mode sanity check‚Äîtargets required for loss computation</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.training <span class="kw">and</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        torch._assert(<span class="va">False</span>, <span class="st">"targets should not be none when in training mode"</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Record original sizes before transforms (needed for inverse mapping in postprocess)</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    original_image_sizes <span class="op">=</span> []</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> img <span class="kw">in</span> images:</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        val <span class="op">=</span> img.shape[<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        original_image_sizes.append((val[<span class="dv">0</span>], val[<span class="dv">1</span>]))</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply input transforms (e.g. resizing, normalization)</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    images, targets <span class="op">=</span> <span class="va">self</span>.transform(images, targets)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward through the ViT backbone</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> <span class="va">self</span>.backbone(images.tensors)  <span class="co"># Could be Tensor, list, or OrderedDict</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If backbone outputs a list, use the last element (common for some backbones)</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(features, <span class="bu">list</span>):</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> features[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If features are [B, N, C] (ViT style: sequence length), reshape to [B, C, H, W]</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(features, torch.Tensor) <span class="kw">and</span> features.ndim <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        B, N, C <span class="op">=</span> features.shape</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Some ViT models output CLS token: strip if N==1025 ([B, 1025, C])</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> N <span class="op">==</span> <span class="dv">1025</span>:</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>            features <span class="op">=</span> features[:, <span class="dv">1</span>:, :]</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>            N <span class="op">=</span> <span class="dv">1024</span>  <span class="co"># Now we have only patch tokens</span></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Recover H and W; for ViT this is usually 32x32</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> W <span class="op">=</span> <span class="bu">int</span>(math.sqrt(N))</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rearrange to [B, C, H, W] for convolutional-style heads</span></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> features.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>).reshape(B, C, H, W)</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to OrderedDict if not already, as expected by torchvision detection components.</span></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Always call .contiguous() for MPS (Metal) backend compatibility and for safety.</span></span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(features, OrderedDict):</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Only one feature map ('0' key)</span></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> OrderedDict([(<span class="st">"0"</span>, features.contiguous())])</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make sure all tensors in the dict are contiguous</span></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> OrderedDict([(k, v.contiguous()) <span class="cf">for</span> k, v <span class="kw">in</span> features.items()])</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate proposals with Region Proposal Network (RPN), then detect boxes/labels/sores</span></span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>    proposals, proposal_losses <span class="op">=</span> <span class="va">self</span>.rpn(images, features, targets)</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>    detections, detector_losses <span class="op">=</span> <span class="va">self</span>.roi_heads(</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>        features, proposals, images.image_sizes, targets</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>    detections <span class="op">=</span> <span class="va">self</span>.transform.postprocess(</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>        detections, images.image_sizes, original_image_sizes</span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aggregate all computed losses for training, or return detections in eval mode</span></span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> {}</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>    losses.update(detector_losses)</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>    losses.update(proposal_losses)</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> losses</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> detections</span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a><span class="co"># Overwrite the model's forward method with our ViT-compatible implementation</span></span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>detection_model.forward <span class="op">=</span> types.MethodType(fixed_fasterrcnn_forward, detection_model)</span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Compatibility patch applied for ViT backbone (single-scale features)"</span>)</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model ready for training or inference"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Complete Solution: Three Adaptations for ViT Detection
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>The Challenge:</strong> - Default Faster R-CNN expects multi-scale CNN features (FPN pyramid) - Prithvi ViT produces single-scale features (32√ó32 at stride 16) - Multi-scale components (RPN, ROI pooler) fail with single-scale input</p>
<p><strong>Our Three-Part Solution:</strong></p>
<ol type="1">
<li><strong>Skip FPN</strong> (<code>neck=None</code>)
<ul>
<li>Avoid multi-scale complexity entirely</li>
<li>Direct single-scale detection</li>
<li>Works directly on ViT‚Äôs 32√ó32 feature map</li>
</ul></li>
<li><strong>Custom RPN</strong>
<ul>
<li>Single anchor tuple for 32√ó32 feature map</li>
<li>Anchor sizes: 16, 32, 64, 128, 256 pixels (tuned for 512√ó512 input)</li>
<li>Aspect ratios: 0.5 (tall), 1.0 (square), 2.0 (wide)</li>
<li>Total: 5 sizes √ó 3 ratios √ó 32√ó32 locations = 15,360 anchors per image</li>
</ul></li>
<li><strong>Custom ROI Pooler</strong>
<ul>
<li>Replace <code>MultiScaleRoIAlign</code> with simple <code>RoIAlign</code></li>
<li>Spatial scale: 1/16 (matches ViT stride)</li>
<li>Output: 7√ó7 features per proposal (standard for detection head)</li>
<li>Bypass FPN-style feature map filtering</li>
</ul></li>
<li><strong>Custom Forward Pass</strong>
<ul>
<li>Reshape ViT sequence [B, 1024, 768] ‚Üí spatial [B, 768, 32, 32]</li>
<li>Handle [CLS] token removal if present</li>
<li>Ensure tensor contiguity for MPS (Apple Silicon) compatibility</li>
<li>Wrap features in OrderedDict for torchvision compatibility</li>
</ul></li>
</ol>
<p><strong>Why This Works for DIOR:</strong> - Fixed 800√ó800 images ‚Üí resize to 512√ó512 ‚Üí consistent 32√ó32 features - Objects range 20-200 pixels after resize (anchors cover this range) - Single-scale sufficient for consistent aerial imagery - Simpler architecture, fewer failure points</p>
</div>
</div>
</section>
</section>
<section id="prepare-data-for-training" class="level2">
<h2 class="anchored" data-anchor-id="prepare-data-for-training">Prepare Data for Training</h2>
<p>Let‚Äôs create train and validation splits from the DIOR dataset. We‚Äôll use a subset for faster training in this demonstration.</p>
<div id="8e2cc5e1" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom collate function for DIOR with Prithvi</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># DIOR has fixed 800√ó800 images - perfect for ViT!</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll resize to 512√ó512 (divisible by 16 for patch_size)</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detection_collate_fn(batch):</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Custom collate function for DIOR with Prithvi ViT.</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Since DIOR images are all 800√ó800:</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">    - Simple resize to 512√ó512 (no padding needed!)</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">    - Scale bounding boxes proportionally</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">    - ViT gets consistent 1024 patches (32√ó32 grid with patch_size=16)</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> []</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> []</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    target_size <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Resize to 512√ó512 for ViT</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sample <span class="kw">in</span> batch:</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract image and convert to tensor if needed</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> sample[<span class="st">'image'</span>]</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(img, torch.Tensor):</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert PIL to tensor</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>            <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            img <span class="op">=</span> torch.from_numpy(np.array(img)).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>).<span class="bu">float</span>() <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        _, orig_h, orig_w <span class="op">=</span> img.shape  <span class="co"># Should be 800√ó800</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize to target size</span></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        img_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>            img.unsqueeze(<span class="dv">0</span>),</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>(target_size, target_size),</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="dv">0</span>)</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scale bounding boxes (800√ó800 ‚Üí 512√ó512)</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> target_size <span class="op">/</span> orig_w  <span class="co"># 512 / 800 = 0.64</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">=</span> sample[<span class="st">'bbox_xyxy'</span>].clone().<span class="bu">float</span>()</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">*=</span> scale</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>        images.append(img_resized)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> {</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>            <span class="st">'boxes'</span>: boxes,</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>            <span class="st">'labels'</span>: sample[<span class="st">'label'</span>]</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>        targets.append(target)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> images, targets</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"‚úì Custom collate function defined for DIOR"</span>)</span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Resizes 800√ó800 ‚Üí 512√ó512 (ViT friendly!)"</span>)</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Scales bounding boxes proportionally"</span>)</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - No padding needed (square images)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why 512√ó512 for Prithvi?
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Patch Size = 16 pixels:</strong> - 512√∑16 = 32 patches per dimension - Total: 32√ó32 = 1,024 patches - Perfect square ‚úÖ</p>
<p><strong>Alternatives:</strong> - <strong>224√ó224</strong>: 14√ó14 = 196 patches (smaller, faster) - <strong>448√ó448</strong>: 28√ó28 = 784 patches (medium) - <strong>800√ó800</strong>: 50√ó50 = 2,500 patches (original size, slower)</p>
<p>We chose 512√ó512 as a good balance between resolution and speed!</p>
</div>
</div>
<div id="f540848d" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load validation split from Hugging Face</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading validation split..."</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>hf_val <span class="op">=</span> load_dataset(<span class="st">"HichTala/dior"</span>, split<span class="op">=</span><span class="st">"validation"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> DIORPyTorchDataset(hf_val)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># For this demo, use subset of training data to speed things up</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> Subset(dataset_train, <span class="bu">range</span>(<span class="dv">1000</span>))  <span class="co"># Use first 1000 images</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> Subset(val_dataset, <span class="bu">range</span>(<span class="dv">200</span>))  <span class="co"># Use first 200 val images</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Training samples: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Validation samples: </span><span class="sc">{</span><span class="bu">len</span>(val_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data loaders with custom collate function</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    train_dataset, </span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">2</span>,            <span class="co"># Small batch size for detection</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">0</span>,                     <span class="co"># Set to 0 for compatibility</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>detection_collate_fn    <span class="co"># Use custom collate function</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    val_dataset,</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>detection_collate_fn</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Training batches: </span><span class="sc">{</span><span class="bu">len</span>(train_loader)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Validation batches: </span><span class="sc">{</span><span class="bu">len</span>(val_loader)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the data loader</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>sample_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>images, targets <span class="op">=</span> sample_batch</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">‚úì Batch structure:"</span>)</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Images: list of </span><span class="sc">{</span><span class="bu">len</span>(images)<span class="sc">}</span><span class="ss"> tensors"</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Image 0 shape: </span><span class="sc">{</span>images[<span class="dv">0</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Image 1 shape: </span><span class="sc">{</span>images[<span class="dv">1</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Targets: list of </span><span class="sc">{</span><span class="bu">len</span>(targets)<span class="sc">}</span><span class="ss"> dicts"</span>)</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Target 0 boxes: </span><span class="sc">{</span>targets[<span class="dv">0</span>][<span class="st">'boxes'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Target 0 labels: </span><span class="sc">{</span>targets[<span class="dv">0</span>][<span class="st">'labels'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="train-the-object-detection-model" class="level2">
<h2 class="anchored" data-anchor-id="train-the-object-detection-model">Train the Object Detection Model</h2>
<p>Now let‚Äôs train the model! With all our adaptations in place, the model is ready for training.</p>
<p><strong>What we‚Äôve configured:</strong> - ‚úÖ Prithvi ViT backbone with single-scale 32√ó32 features - ‚úÖ Custom RPN with 15,360 anchors tuned for 512√ó512 images - ‚úÖ Custom single-scale ROI pooler (spatial scale 1/16) - ‚úÖ Custom forward pass handling ViT sequence ‚Üí spatial reshaping - ‚úÖ Fixed image transform (512√ó512, no resizing) - ‚úÖ Data pipeline: 800√ó800 DIOR ‚Üí 512√ó512 with bbox scaling</p>
<p><strong>Object detection training differs from classification:</strong> - Model computes losses internally (RPN objectness/bbox + ROI classification/bbox) - Must pass both images AND targets during training - Returns dict of losses in train mode, predictions in eval mode - Training typically requires 20-50 epochs for convergence (we‚Äôll start with 2 for demo)</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
GPU Acceleration with Automatic Fallback
</div>
</div>
<div class="callout-body-container callout-body">
<p>The training function automatically detects and uses available hardware acceleration: - <strong>Apple Silicon (M1/M2/M3)</strong>: Uses MPS (Metal Performance Shaders) - <strong>NVIDIA GPUs</strong>: Uses CUDA - <strong>CPU</strong>: Falls back to CPU if no GPU available</p>
<p><strong>MPS Compatibility</strong>: Some operations may not be fully supported on MPS. The training loop includes automatic fallback to CPU if MPS errors occur, ensuring training completes successfully.</p>
<p>This can significantly speed up training on compatible hardware!</p>
</div>
</div>
<div id="7fcad3e8" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_detection_model(model, train_loader, val_loader, num_epochs<span class="op">=</span><span class="dv">2</span>, lr<span class="op">=</span><span class="fl">0.0001</span>):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Train an object detection model with MPS support."""</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try MPS (Apple Silicon) first, then CUDA, then CPU</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training on: Apple Silicon (MPS)"</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Note: If MPS errors occur, model will fall back to CPU"</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training on: CUDA"</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training on: CPU"</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Failed to move model to </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Falling back to CPU"</span>)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use SGD optimizer (standard for object detection)</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>        model.parameters(),</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>        lr<span class="op">=</span>lr,</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>        momentum<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>        weight_decay<span class="op">=</span><span class="fl">0.0005</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_idx, (images, targets) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Move data to device</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>                images <span class="op">=</span> [img.to(device) <span class="cf">for</span> img <span class="kw">in</span> images]</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>                targets <span class="op">=</span> [{k: v.to(device) <span class="cf">for</span> k, v <span class="kw">in</span> t.items()} <span class="cf">for</span> t <span class="kw">in</span> targets]</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Forward pass - model returns loss dict</span></span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>                loss_dict <span class="op">=</span> model(images, targets)</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Sum all losses</span></span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>                losses <span class="op">=</span> <span class="bu">sum</span>(loss <span class="cf">for</span> loss <span class="kw">in</span> loss_dict.values())</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Backward pass</span></span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>                optimizer.zero_grad()</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>                losses.backward()</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>                optimizer.step()</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>                epoch_loss <span class="op">+=</span> losses.item()</span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Print progress every 50 batches</span></span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> batch_idx <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"  Batch </span><span class="sc">{</span>batch_idx<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(train_loader)<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>losses<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"    RPN: cls=</span><span class="sc">{</span>loss_dict<span class="sc">.</span>get(<span class="st">'loss_objectness'</span>, <span class="dv">0</span>)<span class="sc">:.3f}</span><span class="ss">, "</span></span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>                          <span class="ss">f"bbox=</span><span class="sc">{</span>loss_dict<span class="sc">.</span>get(<span class="st">'loss_rpn_box_reg'</span>, <span class="dv">0</span>)<span class="sc">:.3f}</span><span class="ss">, "</span></span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a>                          <span class="ss">f"ROI: cls=</span><span class="sc">{</span>loss_dict<span class="sc">.</span>get(<span class="st">'loss_classifier'</span>, <span class="dv">0</span>)<span class="sc">:.3f}</span><span class="ss">, "</span></span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>                          <span class="ss">f"bbox=</span><span class="sc">{</span>loss_dict<span class="sc">.</span>get(<span class="st">'loss_box_reg'</span>, <span class="dv">0</span>)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">"MPS"</span> <span class="kw">in</span> <span class="bu">str</span>(e) <span class="kw">or</span> <span class="st">"mps"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">MPS compatibility issue detected: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">"Switching to CPU for remaining training..."</span>)</span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>                    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>                    model <span class="op">=</span> model.to(device)</span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Retry this batch on CPU</span></span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a>                    images <span class="op">=</span> [img.to(device) <span class="cf">for</span> img <span class="kw">in</span> images]</span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>                    targets <span class="op">=</span> [{k: v.to(device) <span class="cf">for</span> k, v <span class="kw">in</span> t.items()} <span class="cf">for</span> t <span class="kw">in</span> targets]</span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>                    loss_dict <span class="op">=</span> model(images, targets)</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>                    losses <span class="op">=</span> <span class="bu">sum</span>(loss <span class="cf">for</span> loss <span class="kw">in</span> loss_dict.values())</span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>                    optimizer.zero_grad()</span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>                    losses.backward()</span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a>                    optimizer.step()</span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>                    epoch_loss <span class="op">+=</span> losses.item()</span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">raise</span></span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>        avg_loss <span class="op">=</span> epoch_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"‚úì Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> completed - Avg Loss: </span><span class="sc">{</span>avg_loss<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c4d2c1fd" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Start with 2 epochs for quick demo. For production:</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># - Use 20-50 epochs for convergence</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># - Monitor validation loss to avoid overfitting</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># - Consider learning rate scheduling (e.g., reduce on plateau)</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> train_detection_model(</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    detection_model,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    train_loader,</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    val_loader,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    num_epochs<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Increase to 20-50 for better results</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">0.0001</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure transform is configured correctly for inference</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>trained_model.transform.min_size <span class="op">=</span> (<span class="dv">512</span>,)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>trained_model.transform.max_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Lower score threshold for inference (default 0.05 is too high for early training)</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Some models store these in box_predictor or have different structures</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(trained_model.roi_heads, <span class="st">'score_thresh'</span>):</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    trained_model.roi_heads.score_thresh <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(trained_model.roi_heads, <span class="st">'nms_thresh'</span>):</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    trained_model.roi_heads.nms_thresh <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(trained_model.roi_heads, <span class="st">'detections_per_img'</span>):</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    trained_model.roi_heads.detections_per_img <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ROI heads attributes: </span><span class="sc">{</span>[attr <span class="cf">for</span> attr <span class="kw">in</span> <span class="bu">dir</span>(trained_model.roi_heads) <span class="cf">if</span> <span class="st">'thresh'</span> <span class="kw">in</span> attr.lower() <span class="kw">or</span> <span class="st">'detections'</span> <span class="kw">in</span> attr.lower()]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Configured thresholds: score=</span><span class="sc">{</span><span class="bu">getattr</span>(trained_model.roi_heads, <span class="st">'score_thresh'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">, nms=</span><span class="sc">{</span><span class="bu">getattr</span>(trained_model.roi_heads, <span class="st">'nms_thresh'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>##Evaluate the Trained Model</p>
<p>Let‚Äôs see how the model performs on validation data:</p>
<div id="56b99421" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_detection_model(model, val_loader, num_samples<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate object detection model on validation set."""</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use same device detection as training</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Failed to move model to </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Falling back to CPU for evaluation"</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Running inference on validation samples...</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    sample_images, sample_targets <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(val_loader))</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> [img.to(device) <span class="cf">for</span> img <span class="kw">in</span> sample_images]</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> model(images)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"MPS"</span> <span class="kw">in</span> <span class="bu">str</span>(e) <span class="kw">or</span> <span class="st">"mps"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">MPS compatibility issue during inference: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Switching to CPU for evaluation..."</span>)</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> model.to(device)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> [img.to(device) <span class="cf">for</span> img <span class="kw">in</span> sample_images]</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>                predictions <span class="op">=</span> model(images)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display results</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">min</span>(<span class="bu">len</span>(predictions), num_samples)):</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> predictions[i]</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> sample_targets[i]</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Sample </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Ground truth: </span><span class="sc">{</span><span class="bu">len</span>(target[<span class="st">'boxes'</span>])<span class="sc">}</span><span class="ss"> objects"</span>)</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Predicted: </span><span class="sc">{</span><span class="bu">len</span>(pred[<span class="st">'boxes'</span>])<span class="sc">}</span><span class="ss"> detections"</span>)</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Show top 3 predictions</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(pred[<span class="st">'boxes'</span>]) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>            top_indices <span class="op">=</span> pred[<span class="st">'scores'</span>].argsort(descending<span class="op">=</span><span class="va">True</span>)[:<span class="dv">3</span>]</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Top 3 detections:"</span>)</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx <span class="kw">in</span> top_indices:</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>                score <span class="op">=</span> pred[<span class="st">'scores'</span>][idx].item()</span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>                label <span class="op">=</span> pred[<span class="st">'labels'</span>][idx].item()</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"    - Class </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">, confidence: </span><span class="sc">{</span>score<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>evaluate_detection_model(trained_model, val_loader, num_samples<span class="op">=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="debug-model-predictions" class="level2">
<h2 class="anchored" data-anchor-id="debug-model-predictions">Debug Model Predictions</h2>
<p>Before visualizing, let‚Äôs check if the model is generating any predictions at all:</p>
<div id="0845c25c" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> debug_model_predictions(model, dataset, sample_idx<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Debug what the model is predicting with detailed introspection."""</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get sample</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> dataset[sample_idx]</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> detection_collate_fn([sample])</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    images, targets <span class="op">=</span> batch</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> images[<span class="dv">0</span>].to(device)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sample </span><span class="sc">{</span>sample_idx<span class="sc">}</span><span class="ss"> Debug Info:"</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Ground truth: </span><span class="sc">{</span><span class="bu">len</span>(targets[<span class="dv">0</span>][<span class="st">'boxes'</span>])<span class="sc">}</span><span class="ss"> objects"</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check current threshold settings</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Model threshold settings:"</span>)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    score_thresh: </span><span class="sc">{</span><span class="bu">getattr</span>(model.roi_heads, <span class="st">'score_thresh'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    nms_thresh: </span><span class="sc">{</span><span class="bu">getattr</span>(model.roi_heads, <span class="st">'nms_thresh'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    detections_per_img: </span><span class="sc">{</span><span class="bu">getattr</span>(model.roi_heads, <span class="st">'detections_per_img'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try with extremely low threshold</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>    original_score_thresh <span class="op">=</span> model.roi_heads.score_thresh</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    model.roi_heads.score_thresh <span class="op">=</span> <span class="fl">0.0001</span>  <span class="co"># Extremely low</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> model([img])</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> predictions[<span class="dv">0</span>]</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Model output (with score_thresh=0.0001):"</span>)</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    Total predictions after NMS: </span><span class="sc">{</span><span class="bu">len</span>(pred[<span class="st">'boxes'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(pred[<span class="st">'boxes'</span>]) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  ‚ö†Ô∏è  STILL no predictions! This suggests:"</span>)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"     1. ROI head is not producing any boxes with positive scores"</span>)</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"     2. All boxes might be getting filtered before score threshold"</span>)</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"     3. Check if box_predictor is outputting valid scores"</span>)</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>        model.roi_heads.score_thresh <span class="op">=</span> original_score_thresh</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    Score range: [</span><span class="sc">{</span>pred[<span class="st">'scores'</span>]<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.6f}</span><span class="ss">, </span><span class="sc">{</span>pred[<span class="st">'scores'</span>]<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.6f}</span><span class="ss">]"</span>)</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Show score distribution</span></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>    thresholds <span class="op">=</span> [<span class="fl">0.0001</span>, <span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>]</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Predictions by confidence threshold:"</span>)</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> thresh <span class="kw">in</span> thresholds:</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>        count <span class="op">=</span> (pred[<span class="st">'scores'</span>] <span class="op">&gt;</span> thresh).<span class="bu">sum</span>().item()</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"    &gt;</span><span class="sc">{</span>thresh<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>count<span class="sc">}</span><span class="ss"> detections"</span>)</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Top 10 predictions:"</span>)</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>    top_indices <span class="op">=</span> pred[<span class="st">'scores'</span>].argsort(descending<span class="op">=</span><span class="va">True</span>)[:<span class="dv">10</span>]</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(top_indices):</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> pred[<span class="st">'scores'</span>][idx].item()</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> pred[<span class="st">'labels'</span>][idx].item()</span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>        box <span class="op">=</span> pred[<span class="st">'boxes'</span>][idx].cpu().numpy()</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> DIOR_CLASSES[label] <span class="cf">if</span> label <span class="op">&lt;</span> <span class="bu">len</span>(DIOR_CLASSES) <span class="cf">else</span> <span class="ss">f"Class</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">, score=</span><span class="sc">{</span>score<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Restore original threshold</span></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>    model.roi_heads.score_thresh <span class="op">=</span> original_score_thresh</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7976526b" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>debug_model_predictions(trained_model, dataset_train, sample_idx<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualize-predictions-vs-ground-truth" class="level2">
<h2 class="anchored" data-anchor-id="visualize-predictions-vs-ground-truth">Visualize Predictions vs Ground Truth</h2>
<p>Let‚Äôs compare the model‚Äôs predictions against the actual labels for some of the images we explored earlier:</p>
<div id="87cf0abf" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> patches</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_predictions_vs_truth(model, dataset, indices<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>], confidence_threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Compare model predictions with ground truth annotations.</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="bu">len</span>(indices), <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">8</span> <span class="op">*</span> <span class="bu">len</span>(indices)))</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(indices) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>        axes <span class="op">=</span> axes.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> plot_idx, data_idx <span class="kw">in</span> <span class="bu">enumerate</span>(indices):</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> dataset[data_idx]</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use collate function to process image (same as training)</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> detection_collate_fn([sample])</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>        images, targets <span class="op">=</span> batch</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        img_resized <span class="op">=</span> images[<span class="dv">0</span>]</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>        boxes_scaled <span class="op">=</span> targets[<span class="dv">0</span>][<span class="st">'boxes'</span>]</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run inference</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>                predictions <span class="op">=</span> model([img_resized.to(device)])</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">"MPS"</span> <span class="kw">in</span> <span class="bu">str</span>(e) <span class="kw">or</span> <span class="st">"mps"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>                device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>                model <span class="op">=</span> model.to(device)</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>                    predictions <span class="op">=</span> model([img_resized.to(device)])</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> predictions[<span class="dv">0</span>]</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Filter by confidence</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>        keep <span class="op">=</span> pred[<span class="st">'scores'</span>] <span class="op">&gt;</span> confidence_threshold</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>        pred_boxes <span class="op">=</span> pred[<span class="st">'boxes'</span>][keep].cpu()</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>        pred_labels <span class="op">=</span> pred[<span class="st">'labels'</span>][keep].cpu()</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>        pred_scores <span class="op">=</span> pred[<span class="st">'scores'</span>][keep].cpu()</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image back to 512x512 for visualization</span></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>        img_display <span class="op">=</span> img_resized.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy()</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot ground truth</span></span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> axes[plot_idx, <span class="dv">0</span>]</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>        ax.imshow(img_display)</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> box, label <span class="kw">in</span> <span class="bu">zip</span>(boxes_scaled, sample[<span class="st">'label'</span>]):</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>            xmin, ymin, xmax, ymax <span class="op">=</span> box.numpy()</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>            width <span class="op">=</span> xmax <span class="op">-</span> xmin</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>            height <span class="op">=</span> ymax <span class="op">-</span> ymin</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>            rect <span class="op">=</span> patches.Rectangle(</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>                (xmin, ymin), width, height,</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>                linewidth<span class="op">=</span><span class="dv">2</span>, edgecolor<span class="op">=</span><span class="st">'lime'</span>, facecolor<span class="op">=</span><span class="st">'none'</span></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>            ax.add_patch(rect)</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>            class_name <span class="op">=</span> DIOR_CLASSES[<span class="bu">int</span>(label)]</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>            ax.text(</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>                xmin, ymin <span class="op">-</span> <span class="dv">5</span>, class_name,</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>, weight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>                bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'lime'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"Ground Truth (Sample </span><span class="sc">{</span>data_idx<span class="sc">}</span><span class="ss">): </span><span class="sc">{</span><span class="bu">len</span>(sample[<span class="st">'label'</span>])<span class="sc">}</span><span class="ss"> objects"</span>,</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>                     fontsize<span class="op">=</span><span class="dv">12</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot predictions</span></span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> axes[plot_idx, <span class="dv">1</span>]</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>        ax.imshow(img_display)</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> box, label, score <span class="kw">in</span> <span class="bu">zip</span>(pred_boxes, pred_labels, pred_scores):</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a>            xmin, ymin, xmax, ymax <span class="op">=</span> box.numpy()</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>            width <span class="op">=</span> xmax <span class="op">-</span> xmin</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>            height <span class="op">=</span> ymax <span class="op">-</span> ymin</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>            rect <span class="op">=</span> patches.Rectangle(</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>                (xmin, ymin), width, height,</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>                linewidth<span class="op">=</span><span class="dv">2</span>, edgecolor<span class="op">=</span><span class="st">'red'</span>, facecolor<span class="op">=</span><span class="st">'none'</span></span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>            ax.add_patch(rect)</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>            class_name <span class="op">=</span> DIOR_CLASSES[<span class="bu">int</span>(label)]</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>            ax.text(</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>                xmin, ymin <span class="op">-</span> <span class="dv">5</span>, <span class="ss">f"</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>score<span class="sc">:.2f}</span><span class="ss">)"</span>,</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>, weight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a>                bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"Model Predictions: </span><span class="sc">{</span><span class="bu">len</span>(pred_boxes)<span class="sc">}</span><span class="ss"> detections (conf&gt;</span><span class="sc">{</span>confidence_threshold<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>                     fontsize<span class="op">=</span><span class="dv">12</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a>    plt.suptitle(<span class="st">"Object Detection: Ground Truth vs Predictions (512√ó512)"</span>,</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">16</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>visualize_predictions_vs_truth(trained_model, dataset_train, indices<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>], confidence_threshold<span class="op">=</span><span class="fl">0.001</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Understanding the Visualization
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Left column (Green boxes)</strong>: Ground truth annotations from the DIOR dataset</p>
<p><strong>Right column (Red boxes)</strong>: Model predictions with confidence scores</p>
<p><strong>Training Status After 10 Epochs:</strong> - Model is generating predictions but with low confidence (~0.004-0.005) - Currently showing detections with confidence &gt; 0.001 (very low threshold) - Typical object detection requires 20-50+ epochs to converge - Expected behavior: confidence scores should increase to 0.5+ with more training</p>
<p><strong>Why Low Confidence?</strong> - Limited training data (1000 samples) and epochs (10) - Single-scale detection (no FPN) is challenging - ViT backbone requires careful tuning for detection tasks</p>
<p><strong>To Improve Results:</strong> - Train for 30-50 epochs with full dataset - Consider using learning rate scheduling - Experiment with different anchor sizes for your specific object scales</p>
<p>Note that images are resized to 512√ó512 for visualization, matching the model‚Äôs input size.</p>
</div>
</div>
</section>
<section id="summary-and-key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="summary-and-key-takeaways">Summary and Key Takeaways</h2>
<p>Congratulations! You‚Äôve successfully built and trained an object detection model. Here‚Äôs what you accomplished:</p>
<section id="complete-pipeline-built" class="level3">
<h3 class="anchored" data-anchor-id="complete-pipeline-built">‚úÖ Complete Pipeline Built</h3>
<ol type="1">
<li><strong>Dataset Loading &amp; Exploration</strong>
<ul>
<li>Loaded DIOR object detection dataset from Hugging Face</li>
<li>Converted from COCO format to PyTorch/torchvision format</li>
<li>Explored bounding box format (COCO <code>[x,y,w,h]</code> ‚Üí <code>[xmin,ymin,xmax,ymax]</code>)</li>
<li>Visualized 800√ó800 aerial images with bounding boxes</li>
</ul></li>
<li><strong>Model Architecture</strong>
<ul>
<li>Built complete detection pipeline: <strong>Prithvi ViT ‚Üí Faster R-CNN</strong> (single-scale)</li>
<li>Used actual Geospatial Foundation Model (HLS satellite pretrained!)</li>
<li>Fixed ViT‚Üídetection compatibility issues</li>
<li>Understood the role of each component (backbone, neck, head)</li>
</ul></li>
<li><strong>Data Pipeline</strong>
<ul>
<li>Created train/val splits with <code>Subset</code></li>
<li>Implemented custom <code>collate_fn</code> that resizes 800√ó800 ‚Üí 512√ó512</li>
<li>Scaled bounding boxes proportionally (factor: 0.64)</li>
<li>Formatted targets as list of dicts (torchvision standard)</li>
</ul></li>
<li><strong>Training &amp; Evaluation</strong>
<ul>
<li>Successfully trained detection model with decreasing losses</li>
<li>Understood detection-specific training (internal loss computation)</li>
<li>Monitored RPN and ROI head losses separately</li>
<li>Learned about convergence time (20-50 epochs typical for object detection)</li>
<li>Debugged prediction confidence scores and thresholds</li>
<li>Visualized predictions vs ground truth with adjustable confidence thresholds</li>
</ul></li>
</ol>
</section>
<section id="key-lessons" class="level3">
<h3 class="anchored" data-anchor-id="key-lessons">üéì Key Lessons</h3>
<p><strong>How We Made Prithvi ViT Work for Object Detection:</strong></p>
<p><strong>The Core Challenge:</strong> - Faster R-CNN expects multi-scale CNN features (FPN pyramid: multiple resolutions) - Prithvi ViT produces single-scale sequence features (1024 tokens of 768 dimensions) - Multi-scale components (RPN, ROI pooler) fail or filter incorrectly with single-scale input</p>
<p><strong>Our Four-Part Solution:</strong></p>
<ol type="1">
<li><strong>Skip FPN (neck=None)</strong>
<ul>
<li>No feature pyramid network</li>
<li>Work directly on ViT‚Äôs single 32√ó32 feature map</li>
<li>Avoids multi-scale expectations entirely</li>
</ul></li>
<li><strong>Custom RPN with Single-Scale Anchors</strong>
<ul>
<li>AnchorGenerator: 5 sizes √ó 3 ratios = 15 anchors per location</li>
<li>Anchor sizes: 16, 32, 64, 128, 256 pixels (tuned for 512√ó512 DIOR images after resize)</li>
<li>32√ó32 feature map √ó 15 anchors = 15,360 anchor boxes per image</li>
<li>RPN configured for single feature map (not FPN pyramid)</li>
</ul></li>
<li><strong>Custom Single-Scale ROI Pooler</strong>
<ul>
<li>Replace MultiScaleRoIAlign (expects FPN dict) with single RoIAlign</li>
<li>Spatial scale: 1/16 (ViT stride: 512 input ‚Üí 32√ó32 features)</li>
<li>Output: 7√ó7 pooled features per proposal (standard for detection head)</li>
<li>Bypasses multi-scale feature map filtering that caused empty results</li>
</ul></li>
<li><strong>Custom Forward Pass for ViT Features</strong>
<ul>
<li>Reshape sequence [B, 1024, 768] ‚Üí spatial [B, 768, 32, 32]</li>
<li>Handle [CLS] token removal if present (1025 ‚Üí 1024 patches)</li>
<li>Ensure tensor contiguity for MPS (Apple Silicon GPU) compatibility</li>
<li>Wrap in OrderedDict for torchvision Faster R-CNN interface</li>
</ul></li>
<li><strong>Fixed Image Transform</strong>
<ul>
<li>Set min_size=512, max_size=512 to prevent resizing</li>
<li>Collate function handles 800√ó800 ‚Üí 512√ó512 resize with bbox scaling</li>
<li>Model transform preserves 512√ó512 (doesn‚Äôt resize again)</li>
<li>Result: Consistent 32√ó32 ViT features throughout pipeline</li>
</ul></li>
</ol>
<p><strong>Why DIOR Made It Possible:</strong> - Fixed 800√ó800 images (no variable size complexity) - Square images (no aspect ratio issues)<br>
- Consistent patch count (always 1,024) - Large dataset (18,000 train, 3,463 test)</p>
<p><strong>Key Insight</strong>: - Prithvi (HLS pretrained) is a true <strong>Geospatial Foundation Model</strong> - With proper dataset (fixed-size) and architectural adaptation, ViT GFMs work for detection - The techniques you learned apply to integrating any new backbone architecture!</p>
<p><strong>Object Detection vs Classification:</strong> - Detection models compute losses internally - Must pass both images AND targets during training - Returns loss dict (train) or predictions (eval) - Variable-sized images require custom collate functions</p>
</section>
<section id="next-steps" class="level3">
<h3 class="anchored" data-anchor-id="next-steps">üöÄ Next Steps</h3>
<p><strong>To Use Other Geospatial Foundation Models:</strong></p>
<ol type="1">
<li><strong>Try other ViT-based GFMs</strong> - SatMAE, Clay (apply same adaptation techniques)</li>
<li><strong>Use larger Prithvi</strong> - Prithvi v2 300M or 600M for better performance</li>
<li><strong>Use DETR/Deformable DETR</strong> - Native transformer detection (no CNN conversion needed)</li>
<li><strong>Explore MMDetection</strong> - More flexible ViT integration than torchvision</li>
</ol>
<p><strong>To Improve This Model:</strong></p>
<ol type="1">
<li><strong>Train Longer</strong>: 20-50 epochs with learning rate scheduling
<ul>
<li>Current 2-10 epochs gives confidence ~0.004-0.01</li>
<li>Target 30+ epochs for confidence &gt;0.5</li>
<li>Use ReduceLROnPlateau or CosineAnnealingLR</li>
</ul></li>
<li><strong>Use Full Dataset</strong>: Currently using 1,000 samples subset
<ul>
<li>Full DIOR: 19,000 training images</li>
<li>More data = better convergence</li>
</ul></li>
<li><strong>Add Data Augmentation</strong>: Carefully with bbox transforms
<ul>
<li>Horizontal flips (easy)</li>
<li>Random crops (requires bbox adjustment)</li>
<li>Color jittering (safe - doesn‚Äôt affect boxes)</li>
</ul></li>
<li><strong>Tune Detection Thresholds</strong>:
<ul>
<li>NMS threshold (currently 0.5)</li>
<li>Score threshold (currently 0.001 for early training)</li>
<li>Increase score_thresh as confidence improves</li>
</ul></li>
<li><strong>Evaluate with Proper Metrics</strong>:
<ul>
<li>mAP@0.5 and mAP@[0.5:0.95] (COCO metrics)</li>
<li>Per-class AP to find difficult classes</li>
<li>Confusion matrix for misclassifications</li>
</ul></li>
<li><strong>Experiment with Anchor Sizes</strong>:
<ul>
<li>Current: 16, 32, 64, 128, 256</li>
<li>Analyze DIOR object size distribution</li>
<li>Adjust if objects are consistently smaller/larger</li>
</ul></li>
</ol>
<p><strong>Applying What You Learned to GFMs:</strong></p>
<p>When GFM+detection integration improves, you‚Äôll use the exact same: - Dataset loading and exploration patterns - Custom collate functions - Training loop structure<br>
- Evaluation approaches</p>
<p>The skills transfer completely - only the backbone changes!</p>
</section>
<section id="further-reading" class="level3">
<h3 class="anchored" data-anchor-id="further-reading">üìö Further Reading</h3>
<ul>
<li><a href="https://arxiv.org/abs/1708.02002">RetinaNet Paper</a> - Focal loss for dense object detection</li>
<li><a href="https://arxiv.org/abs/1612.03144">Feature Pyramid Networks</a> - Multi-scale features</li>
<li><a href="https://mmdetection.readthedocs.io/">MMDetection</a> - Production detection framework</li>
<li><a href="https://arxiv.org/abs/2005.12872">DETR</a> - Detection transformers for end-to-end detection</li>
</ul>
<p>You now have a working foundation for object detection that you can build upon! üéØ</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kcaylor\.github\.io\/GEOG-288KC-geospatial-foundation-models");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Object Detection with Prithvi"</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Fine-tuning Prithvi GFM on the DIOR aerial imagery dataset"</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> geoai</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overview</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>This tutorial demonstrates object detection using the **Prithvi geospatial foundation model** on the DIOR aerial imagery dataset. You'll learn how to adapt a Vision Transformer (ViT) backbone for object detection, solving the architectural challenges that arise when integrating ViTs with traditional detection frameworks.</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## What You'll Learn</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>**Dataset &amp; Pipeline:**</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Load DIOR (23,463 aerial images, 20 classes) from Hugging Face</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Build complete detection pipeline: **Prithvi ViT ‚Üí Faster R-CNN** (single-scale)</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create custom data collation for ViT-friendly 512√ó512 images</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>**ViT Detection Challenges &amp; Solutions:**</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Challenge**: Faster R-CNN expects multi-scale CNN features (FPN), ViT gives single-scale sequences</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Solution 1**: Skip FPN, work directly on ViT's 32√ó32 feature map</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Solution 2**: Custom single-scale RPN with 15,360 anchors tuned for aerial imagery</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Solution 3**: Replace MultiScaleRoIAlign with single-scale RoIAlign</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Solution 4**: Custom forward pass to reshape ViT sequences ‚Üí spatial features</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>**Training &amp; Evaluation:**</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train object detection model with automatic GPU/CPU support</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Monitor RPN and ROI losses separately</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Debug predictions with confidence score analysis</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visualize detections vs ground truth</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>By the end, you'll understand how to integrate ANY ViT-based GFM with object detection frameworks!</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>:::{.callout-important}</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why DIOR Is Perfect for Prithvi</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>**The VHR10 Problem:**</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variable image sizes (586√ó716 to 958√ó1024)</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ViTs need consistent input sizes</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Required complex padding/resizing logic</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>**The DIOR Solution:**</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ **Fixed 800√ó800 pixel images** - Every single image!</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ **23,463 images** - Much larger dataset</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ **20 object classes** - More diversity</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ **Simple resize to 512√ó512 or 224√ó224** - No padding needed</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>This makes DIOR the ideal dataset for testing Prithvi-based object detection!</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a><span class="fu">## Background</span></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>**Object Detection vs. Segmentation:**</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Object Detection**: Localizes and classifies objects in an image</span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Segmentation**: Assigns a label to each pixel independently</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a><span class="fu">### Load the DIOR Dataset  </span></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>We are using the HichTala/dior dataset from Hugging Face, which has proper bounding box annotations.</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>Citation: </span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>@article{Li_2020,</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>   title={Object detection in optical remote sensing images: A survey and a new benchmark},</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>   volume={159},</span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a>   ISSN={0924-2716},</span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>   url={http://dx.doi.org/10.1016/j.isprsjprs.2019.11.023},</span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>   DOI={10.1016/j.isprsjprs.2019.11.023},</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>   journal={ISPRS Journal of Photogrammetry and Remote Sensing},</span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a>   publisher={Elsevier BV},</span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a>   author={Li, Ke and Wan, Gang and Cheng, Gong and Meng, Liqiu and Han, Junwei},</span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a>   year={2020},</span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a>   month=jan, pages={296‚Äì307}}</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a>The **DIOR (Dataset for Object deteCtIon in aerial Images)** is an excellent dataset for object detection in aerial imagery, hosted on <span class="co">[</span><span class="ot">Hugging Face</span><span class="co">](https://huggingface.co/datasets/torchgeo/dior)</span>.</span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a>**Key Features:**</span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**23,463 total images** (19,000 train, 4,463 test)</span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**All images are 800√ó800 pixels** (consistent size!)</span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**20 object classes** common in aerial imagery</span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Standard train/test splits**</span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a>**Classes (20 total):**</span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a>airplane, airport, baseball field, basketball court, bridge, chimney, dam, expressway service area, expressway toll station, golf course, ground track field, harbor, overpass, ship, stadium, storage tank, tennis court, train station, vehicle, wind mill</span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a>Objects are annotated with bounding boxes in <span class="in">`[xmin, ymin, xmax, ymax]`</span> format.</span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Load DIOR from Hugging Face (COCO format with annotations!)</span></span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Using HichTala/dior which has proper bounding box annotations</span></span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading DIOR from Hugging Face (with annotations)..."</span>)</span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a>hf_train <span class="op">=</span> load_dataset(<span class="st">"HichTala/dior"</span>, split<span class="op">=</span><span class="st">"train"</span>)</span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Training samples: </span><span class="sc">{</span><span class="bu">len</span>(hf_train)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì All images are 800√ó800 pixels (perfect for ViT!)"</span>)</span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a><span class="co"># Examine structure</span></span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> hf_train[<span class="dv">0</span>]</span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">‚úì Sample keys: </span><span class="sc">{</span><span class="bu">list</span>(sample.keys())<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  image_id: </span><span class="sc">{</span>sample[<span class="st">'image_id'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  width: </span><span class="sc">{</span>sample[<span class="st">'width'</span>]<span class="sc">}</span><span class="ss">, height: </span><span class="sc">{</span>sample[<span class="st">'height'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  num objects: </span><span class="sc">{</span><span class="bu">len</span>(sample[<span class="st">'objects'</span>][<span class="st">'category'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a><span class="co"># Show first object</span></span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(sample[<span class="st">'objects'</span>][<span class="st">'category'</span>]) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">‚úì First object:"</span>)</span>
<span id="cb17-120"><a href="#cb17-120" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  category: </span><span class="sc">{</span>sample[<span class="st">'objects'</span>][<span class="st">'category'</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-121"><a href="#cb17-121" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  bbox: </span><span class="sc">{</span>sample[<span class="st">'objects'</span>][<span class="st">'bbox'</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># [x, y, w, h] COCO format</span></span>
<span id="cb17-122"><a href="#cb17-122" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  area: </span><span class="sc">{</span>sample[<span class="st">'objects'</span>][<span class="st">'area'</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-123"><a href="#cb17-123" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-124"><a href="#cb17-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-125"><a href="#cb17-125" aria-hidden="true" tabindex="-1"></a>Now let's create a PyTorch dataset wrapper and visualize DIOR samples:</span>
<span id="cb17-126"><a href="#cb17-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-129"><a href="#cb17-129" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-130"><a href="#cb17-130" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-131"><a href="#cb17-131" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-132"><a href="#cb17-132" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb17-133"><a href="#cb17-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-134"><a href="#cb17-134" aria-hidden="true" tabindex="-1"></a><span class="co"># DIOR class names (20 classes)</span></span>
<span id="cb17-135"><a href="#cb17-135" aria-hidden="true" tabindex="-1"></a>DIOR_CLASSES <span class="op">=</span> [</span>
<span id="cb17-136"><a href="#cb17-136" aria-hidden="true" tabindex="-1"></a>    <span class="st">'airplane'</span>, <span class="st">'airport'</span>, <span class="st">'baseball field'</span>, <span class="st">'basketball court'</span>, <span class="st">'bridge'</span>,</span>
<span id="cb17-137"><a href="#cb17-137" aria-hidden="true" tabindex="-1"></a>    <span class="st">'chimney'</span>, <span class="st">'dam'</span>, <span class="st">'expressway service area'</span>, <span class="st">'expressway toll station'</span>,</span>
<span id="cb17-138"><a href="#cb17-138" aria-hidden="true" tabindex="-1"></a>    <span class="st">'golf course'</span>, <span class="st">'ground track field'</span>, <span class="st">'harbor'</span>, <span class="st">'overpass'</span>, <span class="st">'ship'</span>,</span>
<span id="cb17-139"><a href="#cb17-139" aria-hidden="true" tabindex="-1"></a>    <span class="st">'stadium'</span>, <span class="st">'storage tank'</span>, <span class="st">'tennis court'</span>, <span class="st">'train station'</span>, <span class="st">'vehicle'</span>, <span class="st">'wind mill'</span></span>
<span id="cb17-140"><a href="#cb17-140" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-141"><a href="#cb17-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-142"><a href="#cb17-142" aria-hidden="true" tabindex="-1"></a><span class="co"># First, inspect what keys HuggingFace DIOR has</span></span>
<span id="cb17-143"><a href="#cb17-143" aria-hidden="true" tabindex="-1"></a>sample <span class="op">=</span> hf_train[<span class="dv">0</span>]</span>
<span id="cb17-144"><a href="#cb17-144" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"DIOR sample structure:"</span>)</span>
<span id="cb17-145"><a href="#cb17-145" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key <span class="kw">in</span> sample.keys():</span>
<span id="cb17-146"><a href="#cb17-146" aria-hidden="true" tabindex="-1"></a>    val <span class="op">=</span> sample[key]</span>
<span id="cb17-147"><a href="#cb17-147" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>key<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">type</span>(val)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-148"><a href="#cb17-148" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(val, (<span class="bu">list</span>, <span class="bu">dict</span>)):</span>
<span id="cb17-149"><a href="#cb17-149" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"    Content: </span><span class="sc">{</span>val<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-150"><a href="#cb17-150" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-151"><a href="#cb17-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-152"><a href="#cb17-152" aria-hidden="true" tabindex="-1"></a>Now let's create a proper PyTorch dataset wrapper:</span>
<span id="cb17-153"><a href="#cb17-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-156"><a href="#cb17-156" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-157"><a href="#cb17-157" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DIORPyTorchDataset(torch.utils.data.Dataset):</span>
<span id="cb17-158"><a href="#cb17-158" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-159"><a href="#cb17-159" aria-hidden="true" tabindex="-1"></a><span class="co">    Convert Hugging Face DIOR (COCO format) to PyTorch detection format.</span></span>
<span id="cb17-160"><a href="#cb17-160" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-161"><a href="#cb17-161" aria-hidden="true" tabindex="-1"></a><span class="co">    Input (COCO format):</span></span>
<span id="cb17-162"><a href="#cb17-162" aria-hidden="true" tabindex="-1"></a><span class="co">    - bbox: [x, y, width, height]</span></span>
<span id="cb17-163"><a href="#cb17-163" aria-hidden="true" tabindex="-1"></a><span class="co">    - category: class index</span></span>
<span id="cb17-164"><a href="#cb17-164" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-165"><a href="#cb17-165" aria-hidden="true" tabindex="-1"></a><span class="co">    Output (torchvision format):</span></span>
<span id="cb17-166"><a href="#cb17-166" aria-hidden="true" tabindex="-1"></a><span class="co">    - image: Tensor [3, 800, 800]</span></span>
<span id="cb17-167"><a href="#cb17-167" aria-hidden="true" tabindex="-1"></a><span class="co">    - bbox_xyxy: Tensor [N, 4] in [xmin, ymin, xmax, ymax] format</span></span>
<span id="cb17-168"><a href="#cb17-168" aria-hidden="true" tabindex="-1"></a><span class="co">    - label: Tensor [N]</span></span>
<span id="cb17-169"><a href="#cb17-169" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-170"><a href="#cb17-170" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, hf_dataset):</span>
<span id="cb17-171"><a href="#cb17-171" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hf_dataset <span class="op">=</span> hf_dataset</span>
<span id="cb17-172"><a href="#cb17-172" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-173"><a href="#cb17-173" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb17-174"><a href="#cb17-174" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.hf_dataset)</span>
<span id="cb17-175"><a href="#cb17-175" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-176"><a href="#cb17-176" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb17-177"><a href="#cb17-177" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> <span class="va">self</span>.hf_dataset[idx]</span>
<span id="cb17-178"><a href="#cb17-178" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-179"><a href="#cb17-179" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert PIL image to tensor [C, H, W]</span></span>
<span id="cb17-180"><a href="#cb17-180" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> sample[<span class="st">'image'</span>]</span>
<span id="cb17-181"><a href="#cb17-181" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(image, Image.Image):</span>
<span id="cb17-182"><a href="#cb17-182" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> torch.from_numpy(np.array(image)).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>).<span class="bu">float</span>() <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb17-183"><a href="#cb17-183" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-184"><a href="#cb17-184" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract bounding boxes (COCO format: [x, y, w, h])</span></span>
<span id="cb17-185"><a href="#cb17-185" aria-hidden="true" tabindex="-1"></a>        bboxes_coco <span class="op">=</span> sample[<span class="st">'objects'</span>][<span class="st">'bbox'</span>]  <span class="co"># List of [x, y, w, h]</span></span>
<span id="cb17-186"><a href="#cb17-186" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> sample[<span class="st">'objects'</span>][<span class="st">'category'</span>]   <span class="co"># List of class indices</span></span>
<span id="cb17-187"><a href="#cb17-187" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-188"><a href="#cb17-188" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert COCO [x, y, w, h] to xyxy [xmin, ymin, xmax, ymax]</span></span>
<span id="cb17-189"><a href="#cb17-189" aria-hidden="true" tabindex="-1"></a>        bboxes_xyxy <span class="op">=</span> []</span>
<span id="cb17-190"><a href="#cb17-190" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> bbox <span class="kw">in</span> bboxes_coco:</span>
<span id="cb17-191"><a href="#cb17-191" aria-hidden="true" tabindex="-1"></a>            x, y, w, h <span class="op">=</span> bbox</span>
<span id="cb17-192"><a href="#cb17-192" aria-hidden="true" tabindex="-1"></a>            xmin, ymin <span class="op">=</span> x, y</span>
<span id="cb17-193"><a href="#cb17-193" aria-hidden="true" tabindex="-1"></a>            xmax, ymax <span class="op">=</span> x <span class="op">+</span> w, y <span class="op">+</span> h</span>
<span id="cb17-194"><a href="#cb17-194" aria-hidden="true" tabindex="-1"></a>            bboxes_xyxy.append([xmin, ymin, xmax, ymax])</span>
<span id="cb17-195"><a href="#cb17-195" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-196"><a href="#cb17-196" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert to tensors</span></span>
<span id="cb17-197"><a href="#cb17-197" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">=</span> torch.tensor(bboxes_xyxy, dtype<span class="op">=</span>torch.float32) <span class="cf">if</span> bboxes_xyxy <span class="cf">else</span> torch.zeros((<span class="dv">0</span>, <span class="dv">4</span>), dtype<span class="op">=</span>torch.float32)</span>
<span id="cb17-198"><a href="#cb17-198" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> torch.tensor(labels, dtype<span class="op">=</span>torch.int64) <span class="cf">if</span> labels <span class="cf">else</span> torch.zeros((<span class="dv">0</span>,), dtype<span class="op">=</span>torch.int64)</span>
<span id="cb17-199"><a href="#cb17-199" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-200"><a href="#cb17-200" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb17-201"><a href="#cb17-201" aria-hidden="true" tabindex="-1"></a>            <span class="st">'image'</span>: image,</span>
<span id="cb17-202"><a href="#cb17-202" aria-hidden="true" tabindex="-1"></a>            <span class="st">'bbox_xyxy'</span>: boxes,</span>
<span id="cb17-203"><a href="#cb17-203" aria-hidden="true" tabindex="-1"></a>            <span class="st">'label'</span>: labels</span>
<span id="cb17-204"><a href="#cb17-204" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb17-205"><a href="#cb17-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-206"><a href="#cb17-206" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dataset</span></span>
<span id="cb17-207"><a href="#cb17-207" aria-hidden="true" tabindex="-1"></a>dataset_train <span class="op">=</span> DIORPyTorchDataset(hf_train)</span>
<span id="cb17-208"><a href="#cb17-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-209"><a href="#cb17-209" aria-hidden="true" tabindex="-1"></a><span class="co"># Test it</span></span>
<span id="cb17-210"><a href="#cb17-210" aria-hidden="true" tabindex="-1"></a>test_sample <span class="op">=</span> dataset_train[<span class="dv">0</span>]</span>
<span id="cb17-211"><a href="#cb17-211" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">‚úì PyTorch dataset created"</span>)</span>
<span id="cb17-212"><a href="#cb17-212" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Image shape: </span><span class="sc">{</span>test_sample[<span class="st">'image'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-213"><a href="#cb17-213" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Bounding boxes: </span><span class="sc">{</span>test_sample[<span class="st">'bbox_xyxy'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-214"><a href="#cb17-214" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Labels: </span><span class="sc">{</span>test_sample[<span class="st">'label'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-215"><a href="#cb17-215" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  Expected: [3, 800, 800] images with variable number of objects"</span>)</span>
<span id="cb17-216"><a href="#cb17-216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-217"><a href="#cb17-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-218"><a href="#cb17-218" aria-hidden="true" tabindex="-1"></a>Now let's visualize some DIOR samples with their annotations:</span>
<span id="cb17-219"><a href="#cb17-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-222"><a href="#cb17-222" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-223"><a href="#cb17-223" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-224"><a href="#cb17-224" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> patches</span>
<span id="cb17-225"><a href="#cb17-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-226"><a href="#cb17-226" aria-hidden="true" tabindex="-1"></a><span class="co"># DIOR class names</span></span>
<span id="cb17-227"><a href="#cb17-227" aria-hidden="true" tabindex="-1"></a>DIOR_CLASSES <span class="op">=</span> [</span>
<span id="cb17-228"><a href="#cb17-228" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Airplane'</span>, <span class="st">'Airport'</span>, <span class="st">'Baseball field'</span>, <span class="st">'Basketball court'</span>, <span class="st">'Bridge'</span>,</span>
<span id="cb17-229"><a href="#cb17-229" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Chimney'</span>, <span class="st">'Dam'</span>, <span class="st">'Expressway service area'</span>, <span class="st">'Expressway toll station'</span>,</span>
<span id="cb17-230"><a href="#cb17-230" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Golf course'</span>, <span class="st">'Ground track field'</span>, <span class="st">'Harbor'</span>, <span class="st">'Overpass'</span>, <span class="st">'Ship'</span>,</span>
<span id="cb17-231"><a href="#cb17-231" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Stadium'</span>, <span class="st">'Storage tank'</span>, <span class="st">'Tennis court'</span>, <span class="st">'Train station'</span>, <span class="st">'Vehicle'</span>, <span class="st">'Wind mill'</span></span>
<span id="cb17-232"><a href="#cb17-232" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-233"><a href="#cb17-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-234"><a href="#cb17-234" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize 4 samples</span></span>
<span id="cb17-235"><a href="#cb17-235" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">16</span>))</span>
<span id="cb17-236"><a href="#cb17-236" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb17-237"><a href="#cb17-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-238"><a href="#cb17-238" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb17-239"><a href="#cb17-239" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> dataset_train[idx]</span>
<span id="cb17-240"><a href="#cb17-240" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-241"><a href="#cb17-241" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get image</span></span>
<span id="cb17-242"><a href="#cb17-242" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> sample[<span class="st">'image'</span>].permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy()  <span class="co"># [C,H,W] ‚Üí [H,W,C]</span></span>
<span id="cb17-243"><a href="#cb17-243" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-244"><a href="#cb17-244" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get boxes and labels  </span></span>
<span id="cb17-245"><a href="#cb17-245" aria-hidden="true" tabindex="-1"></a>    boxes <span class="op">=</span> sample[<span class="st">'bbox_xyxy'</span>]</span>
<span id="cb17-246"><a href="#cb17-246" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> sample[<span class="st">'label'</span>]</span>
<span id="cb17-247"><a href="#cb17-247" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-248"><a href="#cb17-248" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display</span></span>
<span id="cb17-249"><a href="#cb17-249" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[idx]</span>
<span id="cb17-250"><a href="#cb17-250" aria-hidden="true" tabindex="-1"></a>    ax.imshow(image)</span>
<span id="cb17-251"><a href="#cb17-251" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-252"><a href="#cb17-252" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw bounding boxes</span></span>
<span id="cb17-253"><a href="#cb17-253" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> box, label <span class="kw">in</span> <span class="bu">zip</span>(boxes, labels):</span>
<span id="cb17-254"><a href="#cb17-254" aria-hidden="true" tabindex="-1"></a>        xmin, ymin, xmax, ymax <span class="op">=</span> box.numpy()</span>
<span id="cb17-255"><a href="#cb17-255" aria-hidden="true" tabindex="-1"></a>        width <span class="op">=</span> xmax <span class="op">-</span> xmin</span>
<span id="cb17-256"><a href="#cb17-256" aria-hidden="true" tabindex="-1"></a>        height <span class="op">=</span> ymax <span class="op">-</span> ymin</span>
<span id="cb17-257"><a href="#cb17-257" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-258"><a href="#cb17-258" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rectangle</span></span>
<span id="cb17-259"><a href="#cb17-259" aria-hidden="true" tabindex="-1"></a>        rect <span class="op">=</span> patches.Rectangle(</span>
<span id="cb17-260"><a href="#cb17-260" aria-hidden="true" tabindex="-1"></a>            (xmin, ymin), width, height,</span>
<span id="cb17-261"><a href="#cb17-261" aria-hidden="true" tabindex="-1"></a>            linewidth<span class="op">=</span><span class="dv">2</span>, edgecolor<span class="op">=</span><span class="st">'red'</span>, facecolor<span class="op">=</span><span class="st">'none'</span></span>
<span id="cb17-262"><a href="#cb17-262" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-263"><a href="#cb17-263" aria-hidden="true" tabindex="-1"></a>        ax.add_patch(rect)</span>
<span id="cb17-264"><a href="#cb17-264" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-265"><a href="#cb17-265" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Label</span></span>
<span id="cb17-266"><a href="#cb17-266" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> DIOR_CLASSES[<span class="bu">int</span>(label)]</span>
<span id="cb17-267"><a href="#cb17-267" aria-hidden="true" tabindex="-1"></a>        ax.text(</span>
<span id="cb17-268"><a href="#cb17-268" aria-hidden="true" tabindex="-1"></a>            xmin, ymin <span class="op">-</span> <span class="dv">5</span>,</span>
<span id="cb17-269"><a href="#cb17-269" aria-hidden="true" tabindex="-1"></a>            class_name,</span>
<span id="cb17-270"><a href="#cb17-270" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span><span class="st">'white'</span>,</span>
<span id="cb17-271"><a href="#cb17-271" aria-hidden="true" tabindex="-1"></a>            fontsize<span class="op">=</span><span class="dv">9</span>,</span>
<span id="cb17-272"><a href="#cb17-272" aria-hidden="true" tabindex="-1"></a>            weight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb17-273"><a href="#cb17-273" aria-hidden="true" tabindex="-1"></a>            bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb17-274"><a href="#cb17-274" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb17-275"><a href="#cb17-275" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-276"><a href="#cb17-276" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">'off'</span>)</span>
<span id="cb17-277"><a href="#cb17-277" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"Sample </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">len</span>(boxes)<span class="sc">}</span><span class="ss"> object(s)"</span>, fontsize<span class="op">=</span><span class="dv">12</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-278"><a href="#cb17-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-279"><a href="#cb17-279" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">"DIOR Dataset Examples (800√ó800 pixels)"</span>, fontsize<span class="op">=</span><span class="dv">16</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-280"><a href="#cb17-280" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb17-281"><a href="#cb17-281" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-282"><a href="#cb17-282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-283"><a href="#cb17-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-284"><a href="#cb17-284" aria-hidden="true" tabindex="-1"></a><span class="fu">### Understanding Object Detection Architecture</span></span>
<span id="cb17-285"><a href="#cb17-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-286"><a href="#cb17-286" aria-hidden="true" tabindex="-1"></a>Object detection requires more than just a backbone model - we need a complete detection pipeline:</span>
<span id="cb17-287"><a href="#cb17-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-288"><a href="#cb17-288" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Backbone**: Extract features from the image (e.g., Prithvi ViT)</span>
<span id="cb17-289"><a href="#cb17-289" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Neck** (optional): Process features at multiple scales (e.g., FPN)</span>
<span id="cb17-290"><a href="#cb17-290" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Detection Head**: Predict bounding boxes and classes (e.g., Faster R-CNN, RetinaNet)</span>
<span id="cb17-291"><a href="#cb17-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-292"><a href="#cb17-292" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb17-293"><a href="#cb17-293" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Object Detection is Complex</span></span>
<span id="cb17-294"><a href="#cb17-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-295"><a href="#cb17-295" aria-hidden="true" tabindex="-1"></a>Unlike classification or segmentation, object detection must:</span>
<span id="cb17-296"><a href="#cb17-296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Predict variable numbers of objects per image</span>
<span id="cb17-297"><a href="#cb17-297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Localize objects with bounding boxes</span>
<span id="cb17-298"><a href="#cb17-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handle objects at different scales</span>
<span id="cb17-299"><a href="#cb17-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deal with overlapping predictions (Non-Maximum Suppression)</span>
<span id="cb17-300"><a href="#cb17-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-301"><a href="#cb17-301" aria-hidden="true" tabindex="-1"></a>This complexity means we typically need specialized frameworks like **Detectron2**, **MMDetection**, or **TerraTorch** that provide pre-built detection pipelines.</span>
<span id="cb17-302"><a href="#cb17-302" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-303"><a href="#cb17-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-304"><a href="#cb17-304" aria-hidden="true" tabindex="-1"></a><span class="fu">### Setting Up Object Detection with TerraTorch</span></span>
<span id="cb17-305"><a href="#cb17-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-306"><a href="#cb17-306" aria-hidden="true" tabindex="-1"></a>Use the TerraTorch Object Detection Model Factory to create a model for object detection. The complete pipeline has three components:</span>
<span id="cb17-307"><a href="#cb17-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-308"><a href="#cb17-308" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Backbone (Prithvi)**: Extracts features from input images</span>
<span id="cb17-309"><a href="#cb17-309" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Neck (Optional - FPN)**: Can transform features into multi-scale pyramid</span>
<span id="cb17-310"><a href="#cb17-310" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Head (Faster R-CNN)**: Predicts bounding boxes and classes from features</span>
<span id="cb17-311"><a href="#cb17-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-312"><a href="#cb17-312" aria-hidden="true" tabindex="-1"></a>For ViT, we skip the neck and use single-scale detection directly.</span>
<span id="cb17-313"><a href="#cb17-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-314"><a href="#cb17-314" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb17-315"><a href="#cb17-315" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Skip the Neck for ViT?</span></span>
<span id="cb17-316"><a href="#cb17-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-317"><a href="#cb17-317" aria-hidden="true" tabindex="-1"></a>Traditional CNN detectors use FPN to create multi-scale features. However:</span>
<span id="cb17-318"><a href="#cb17-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-319"><a href="#cb17-319" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Prithvi ViT**: Outputs single-scale features (32√ó32)</span>
<span id="cb17-320"><a href="#cb17-320" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**FPN Complexity**: Feature map filtering and multi-scale expectations</span>
<span id="cb17-321"><a href="#cb17-321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DIOR Dataset**: Consistent image sizes and object scales</span>
<span id="cb17-322"><a href="#cb17-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-323"><a href="#cb17-323" aria-hidden="true" tabindex="-1"></a>**Our approach**: Skip FPN entirely and use **single-scale detection** directly on ViT features. This is simpler and avoids compatibility issues.</span>
<span id="cb17-324"><a href="#cb17-324" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-325"><a href="#cb17-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-326"><a href="#cb17-326" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb17-327"><a href="#cb17-327" aria-hidden="true" tabindex="-1"></a><span class="fu">## Working with TerraTorch Object Detection Models</span></span>
<span id="cb17-328"><a href="#cb17-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-329"><a href="#cb17-329" aria-hidden="true" tabindex="-1"></a>TerraTorch's <span class="in">`ObjectDetectionModel`</span> is a wrapper around torchvision's detection models. While the wrapper is useful for model construction, it has some quirks during training:</span>
<span id="cb17-330"><a href="#cb17-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-331"><a href="#cb17-331" aria-hidden="true" tabindex="-1"></a>**The Issue**: The wrapper's <span class="in">`forward()`</span> method doesn't properly pass <span class="in">`targets`</span> to the underlying model during training mode.</span>
<span id="cb17-332"><a href="#cb17-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-333"><a href="#cb17-333" aria-hidden="true" tabindex="-1"></a>**The Solution**: Extract the underlying torchvision model using <span class="in">`model.torchvision_model`</span> and train that directly.</span>
<span id="cb17-334"><a href="#cb17-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-335"><a href="#cb17-335" aria-hidden="true" tabindex="-1"></a>This is a common pattern when working with model wrappers - use them for construction, but train the underlying model directly.</span>
<span id="cb17-336"><a href="#cb17-336" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-337"><a href="#cb17-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-340"><a href="#cb17-340" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-341"><a href="#cb17-341" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> terratorch.models.object_detection_model_factory <span class="im">import</span> ObjectDetectionModelFactory</span>
<span id="cb17-342"><a href="#cb17-342" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.anchor_utils <span class="im">import</span> AnchorGenerator</span>
<span id="cb17-343"><a href="#cb17-343" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.detection.rpn <span class="im">import</span> RegionProposalNetwork, RPNHead</span>
<span id="cb17-344"><a href="#cb17-344" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb17-345"><a href="#cb17-345" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> types</span>
<span id="cb17-346"><a href="#cb17-346" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb17-347"><a href="#cb17-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-348"><a href="#cb17-348" aria-hidden="true" tabindex="-1"></a>model_factory <span class="op">=</span> ObjectDetectionModelFactory()</span>
<span id="cb17-349"><a href="#cb17-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-350"><a href="#cb17-350" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model_factory.build_model(</span>
<span id="cb17-351"><a href="#cb17-351" aria-hidden="true" tabindex="-1"></a>    task<span class="op">=</span><span class="st">"object_detection"</span>,</span>
<span id="cb17-352"><a href="#cb17-352" aria-hidden="true" tabindex="-1"></a>    backbone<span class="op">=</span><span class="st">"prithvi_eo_v1_100"</span>,</span>
<span id="cb17-353"><a href="#cb17-353" aria-hidden="true" tabindex="-1"></a>    backbone_bands<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb17-354"><a href="#cb17-354" aria-hidden="true" tabindex="-1"></a>    backbone_num_frames<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb17-355"><a href="#cb17-355" aria-hidden="true" tabindex="-1"></a>    backbone_pretrained<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb17-356"><a href="#cb17-356" aria-hidden="true" tabindex="-1"></a>    neck<span class="op">=</span><span class="va">None</span>,  <span class="co"># No FPN - simpler single-scale detection</span></span>
<span id="cb17-357"><a href="#cb17-357" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb17-358"><a href="#cb17-358" aria-hidden="true" tabindex="-1"></a>    framework<span class="op">=</span><span class="st">"faster-rcnn"</span></span>
<span id="cb17-359"><a href="#cb17-359" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-360"><a href="#cb17-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-361"><a href="#cb17-361" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(model, <span class="st">'torchvision_model'</span>):</span>
<span id="cb17-362"><a href="#cb17-362" aria-hidden="true" tabindex="-1"></a>    detection_model <span class="op">=</span> model.torchvision_model</span>
<span id="cb17-363"><a href="#cb17-363" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb17-364"><a href="#cb17-364" aria-hidden="true" tabindex="-1"></a>    detection_model <span class="op">=</span> model</span>
<span id="cb17-365"><a href="#cb17-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-366"><a href="#cb17-366" aria-hidden="true" tabindex="-1"></a><span class="co"># Fix image transform to keep 512√ó512 size (don't resize)</span></span>
<span id="cb17-367"><a href="#cb17-367" aria-hidden="true" tabindex="-1"></a>detection_model.transform.min_size <span class="op">=</span> (<span class="dv">512</span>,)</span>
<span id="cb17-368"><a href="#cb17-368" aria-hidden="true" tabindex="-1"></a>detection_model.transform.max_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb17-369"><a href="#cb17-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-370"><a href="#cb17-370" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model created with Prithvi backbone"</span>)</span>
<span id="cb17-371"><a href="#cb17-371" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Image transform configured: 512√ó512 (no resizing)"</span>)</span>
<span id="cb17-372"><a href="#cb17-372" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-373"><a href="#cb17-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-374"><a href="#cb17-374" aria-hidden="true" tabindex="-1"></a>Now we need to configure custom anchors for ViT's single-scale features:</span>
<span id="cb17-375"><a href="#cb17-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-378"><a href="#cb17-378" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-379"><a href="#cb17-379" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Anchor Generator and Custom Region Proposal Network for ViT Single-Scale Detection ---</span></span>
<span id="cb17-380"><a href="#cb17-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-381"><a href="#cb17-381" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_vit_anchor_generator():</span>
<span id="cb17-382"><a href="#cb17-382" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-383"><a href="#cb17-383" aria-hidden="true" tabindex="-1"></a><span class="co">    Create an AnchorGenerator specifically for Prithvi ViT's single-scale feature map.</span></span>
<span id="cb17-384"><a href="#cb17-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-385"><a href="#cb17-385" aria-hidden="true" tabindex="-1"></a><span class="co">    In the classic Faster R-CNN pipeline, anchors are generated over multiple feature map scales (FPN).</span></span>
<span id="cb17-386"><a href="#cb17-386" aria-hidden="true" tabindex="-1"></a><span class="co">    Since our ViT backbone outputs just a single feature map (no FPN), we use a single set of anchor sizes and ratios:</span></span>
<span id="cb17-387"><a href="#cb17-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-388"><a href="#cb17-388" aria-hidden="true" tabindex="-1"></a><span class="co">    For 512x512 input with patch_size=16:</span></span>
<span id="cb17-389"><a href="#cb17-389" aria-hidden="true" tabindex="-1"></a><span class="co">    - ViT outputs a single [32x32] feature map (stride 16).</span></span>
<span id="cb17-390"><a href="#cb17-390" aria-hidden="true" tabindex="-1"></a><span class="co">    - Each feature location has anchors of sizes: 32, 64, 128, 256, 512 pixels.</span></span>
<span id="cb17-391"><a href="#cb17-391" aria-hidden="true" tabindex="-1"></a><span class="co">    - Each anchor size is combined with aspect ratios: 0.5 (tall), 1.0 (square), 2.0 (wide).</span></span>
<span id="cb17-392"><a href="#cb17-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-393"><a href="#cb17-393" aria-hidden="true" tabindex="-1"></a><span class="co">    This ensures the RPN can propose boxes covering a wide range of object sizes/shapes</span></span>
<span id="cb17-394"><a href="#cb17-394" aria-hidden="true" tabindex="-1"></a><span class="co">    despite only a single feature resolution.</span></span>
<span id="cb17-395"><a href="#cb17-395" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-396"><a href="#cb17-396" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Anchor sizes tuned for objects in 512√ó512 images (DIOR resized from 800√ó800)</span></span>
<span id="cb17-397"><a href="#cb17-397" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Cover range: small vehicles (~20px) to large buildings/airports (~200px)</span></span>
<span id="cb17-398"><a href="#cb17-398" aria-hidden="true" tabindex="-1"></a>    anchor_sizes <span class="op">=</span> ((<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>),)</span>
<span id="cb17-399"><a href="#cb17-399" aria-hidden="true" tabindex="-1"></a>    aspect_ratios <span class="op">=</span> ((<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>),)</span>
<span id="cb17-400"><a href="#cb17-400" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> AnchorGenerator(sizes<span class="op">=</span>anchor_sizes, aspect_ratios<span class="op">=</span>aspect_ratios)</span>
<span id="cb17-401"><a href="#cb17-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-402"><a href="#cb17-402" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> replace_rpn(model):</span>
<span id="cb17-403"><a href="#cb17-403" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-404"><a href="#cb17-404" aria-hidden="true" tabindex="-1"></a><span class="co">    Replace the model's RPN (Region Proposal Network) with a version </span></span>
<span id="cb17-405"><a href="#cb17-405" aria-hidden="true" tabindex="-1"></a><span class="co">    compatible with ViT's single-scale features.</span></span>
<span id="cb17-406"><a href="#cb17-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-407"><a href="#cb17-407" aria-hidden="true" tabindex="-1"></a><span class="co">    - in_channels: Set to Prithvi's hidden dimension (768 for ViT-base)</span></span>
<span id="cb17-408"><a href="#cb17-408" aria-hidden="true" tabindex="-1"></a><span class="co">                   because that's the channel count of the feature map.</span></span>
<span id="cb17-409"><a href="#cb17-409" aria-hidden="true" tabindex="-1"></a><span class="co">    - anchor_generator: Single-scale anchor generator from above.</span></span>
<span id="cb17-410"><a href="#cb17-410" aria-hidden="true" tabindex="-1"></a><span class="co">    - RPNHead: Computes objectness and box regression deltas per anchor.</span></span>
<span id="cb17-411"><a href="#cb17-411" aria-hidden="true" tabindex="-1"></a><span class="co">    - RegionProposalNetwork: Uses anchors and predictions to generate </span></span>
<span id="cb17-412"><a href="#cb17-412" aria-hidden="true" tabindex="-1"></a><span class="co">                            object proposal regions. All RPN parameters</span></span>
<span id="cb17-413"><a href="#cb17-413" aria-hidden="true" tabindex="-1"></a><span class="co">                            are tuned to standard values but operate only</span></span>
<span id="cb17-414"><a href="#cb17-414" aria-hidden="true" tabindex="-1"></a><span class="co">                            on the single ViT feature map (not FPN).</span></span>
<span id="cb17-415"><a href="#cb17-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-416"><a href="#cb17-416" aria-hidden="true" tabindex="-1"></a><span class="co">    This customization is required because the default RPN expects FPN-style input,</span></span>
<span id="cb17-417"><a href="#cb17-417" aria-hidden="true" tabindex="-1"></a><span class="co">    but our model has just one feature map level (from ViT).</span></span>
<span id="cb17-418"><a href="#cb17-418" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-419"><a href="#cb17-419" aria-hidden="true" tabindex="-1"></a>    in_channels <span class="op">=</span> <span class="dv">768</span>  <span class="co"># Prithvi ViT-Base hidden dimension</span></span>
<span id="cb17-420"><a href="#cb17-420" aria-hidden="true" tabindex="-1"></a>    anchor_generator <span class="op">=</span> create_vit_anchor_generator()</span>
<span id="cb17-421"><a href="#cb17-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-422"><a href="#cb17-422" aria-hidden="true" tabindex="-1"></a>    <span class="co"># RPNHead = conv layers predicting objectness &amp; bbox offsets for each anchor at each location.</span></span>
<span id="cb17-423"><a href="#cb17-423" aria-hidden="true" tabindex="-1"></a>    rpn_head <span class="op">=</span> RPNHead(</span>
<span id="cb17-424"><a href="#cb17-424" aria-hidden="true" tabindex="-1"></a>        in_channels<span class="op">=</span>in_channels,</span>
<span id="cb17-425"><a href="#cb17-425" aria-hidden="true" tabindex="-1"></a>        num_anchors<span class="op">=</span>anchor_generator.num_anchors_per_location()[<span class="dv">0</span>]</span>
<span id="cb17-426"><a href="#cb17-426" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-427"><a href="#cb17-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-428"><a href="#cb17-428" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The actual RPN module, configured for single-scale input.</span></span>
<span id="cb17-429"><a href="#cb17-429" aria-hidden="true" tabindex="-1"></a>    rpn <span class="op">=</span> RegionProposalNetwork(</span>
<span id="cb17-430"><a href="#cb17-430" aria-hidden="true" tabindex="-1"></a>        anchor_generator<span class="op">=</span>anchor_generator,</span>
<span id="cb17-431"><a href="#cb17-431" aria-hidden="true" tabindex="-1"></a>        head<span class="op">=</span>rpn_head,</span>
<span id="cb17-432"><a href="#cb17-432" aria-hidden="true" tabindex="-1"></a>        fg_iou_thresh<span class="op">=</span><span class="fl">0.7</span>,    <span class="co"># IoU threshold for matching anchors to GT boxes (foreground)</span></span>
<span id="cb17-433"><a href="#cb17-433" aria-hidden="true" tabindex="-1"></a>        bg_iou_thresh<span class="op">=</span><span class="fl">0.3</span>,    <span class="co"># IoU threshold for matching anchors as background</span></span>
<span id="cb17-434"><a href="#cb17-434" aria-hidden="true" tabindex="-1"></a>        batch_size_per_image<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb17-435"><a href="#cb17-435" aria-hidden="true" tabindex="-1"></a>        positive_fraction<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb17-436"><a href="#cb17-436" aria-hidden="true" tabindex="-1"></a>        pre_nms_top_n<span class="op">=</span><span class="bu">dict</span>(training<span class="op">=</span><span class="dv">2000</span>, testing<span class="op">=</span><span class="dv">1000</span>),</span>
<span id="cb17-437"><a href="#cb17-437" aria-hidden="true" tabindex="-1"></a>        post_nms_top_n<span class="op">=</span><span class="bu">dict</span>(training<span class="op">=</span><span class="dv">2000</span>, testing<span class="op">=</span><span class="dv">1000</span>),</span>
<span id="cb17-438"><a href="#cb17-438" aria-hidden="true" tabindex="-1"></a>        nms_thresh<span class="op">=</span><span class="fl">0.7</span>         <span class="co"># Non-max suppression threshold for RPN proposals</span></span>
<span id="cb17-439"><a href="#cb17-439" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-440"><a href="#cb17-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-441"><a href="#cb17-441" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Override the model's RPN with our custom single-scale RPN.</span></span>
<span id="cb17-442"><a href="#cb17-442" aria-hidden="true" tabindex="-1"></a>    model.rpn <span class="op">=</span> rpn</span>
<span id="cb17-443"><a href="#cb17-443" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb17-444"><a href="#cb17-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-445"><a href="#cb17-445" aria-hidden="true" tabindex="-1"></a><span class="co"># Insert our custom single-scale RPN into the model pipeline.</span></span>
<span id="cb17-446"><a href="#cb17-446" aria-hidden="true" tabindex="-1"></a>detection_model <span class="op">=</span> replace_rpn(detection_model)</span>
<span id="cb17-447"><a href="#cb17-447" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Custom RPN configured for ViT features"</span>)</span>
<span id="cb17-448"><a href="#cb17-448" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-449"><a href="#cb17-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-450"><a href="#cb17-450" aria-hidden="true" tabindex="-1"></a>Now replace the ROI pooler to work with single-scale features:</span>
<span id="cb17-451"><a href="#cb17-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-454"><a href="#cb17-454" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-455"><a href="#cb17-455" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb17-456"><a href="#cb17-456" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.ops <span class="im">import</span> RoIAlign</span>
<span id="cb17-457"><a href="#cb17-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-458"><a href="#cb17-458" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> replace_roi_pooler(model):</span>
<span id="cb17-459"><a href="#cb17-459" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-460"><a href="#cb17-460" aria-hidden="true" tabindex="-1"></a><span class="co">    Replace the default multi-scale ROI pooler in Faster R-CNN with a</span></span>
<span id="cb17-461"><a href="#cb17-461" aria-hidden="true" tabindex="-1"></a><span class="co">    custom single-scale ROIAlign module suitable for ViT-style (single feature map) backbones.</span></span>
<span id="cb17-462"><a href="#cb17-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-463"><a href="#cb17-463" aria-hidden="true" tabindex="-1"></a><span class="co">    Why:</span></span>
<span id="cb17-464"><a href="#cb17-464" aria-hidden="true" tabindex="-1"></a><span class="co">        Faster R-CNN defaults to MultiScaleRoIAlign, which expects a dict of feature maps at </span></span>
<span id="cb17-465"><a href="#cb17-465" aria-hidden="true" tabindex="-1"></a><span class="co">        multiple pyramid levels (as with FPN). Vision Transformers output a single feature map,</span></span>
<span id="cb17-466"><a href="#cb17-466" aria-hidden="true" tabindex="-1"></a><span class="co">        so we need a simpler pooling module that operates on just one map.</span></span>
<span id="cb17-467"><a href="#cb17-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-468"><a href="#cb17-468" aria-hidden="true" tabindex="-1"></a><span class="co">    What this does:</span></span>
<span id="cb17-469"><a href="#cb17-469" aria-hidden="true" tabindex="-1"></a><span class="co">      - Instantiates a single-scale RoIAlign set to a 7x7 output, which is standard for the detection head.</span></span>
<span id="cb17-470"><a href="#cb17-470" aria-hidden="true" tabindex="-1"></a><span class="co">      - Uses a spatial scale of 1/16, as Prithvi/ViT feature stride is 16 pixels (512 input -&gt; 32x32 features).</span></span>
<span id="cb17-471"><a href="#cb17-471" aria-hidden="true" tabindex="-1"></a><span class="co">      - Defines a custom nn.Module that handles both dict and tensor feature inputs, always pulling out</span></span>
<span id="cb17-472"><a href="#cb17-472" aria-hidden="true" tabindex="-1"></a><span class="co">        the only feature map available, and ignoring FPN-style logic.</span></span>
<span id="cb17-473"><a href="#cb17-473" aria-hidden="true" tabindex="-1"></a><span class="co">      - Replaces model.roi_heads.box_roi_pool with this new single-scale implementation.</span></span>
<span id="cb17-474"><a href="#cb17-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-475"><a href="#cb17-475" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb17-476"><a href="#cb17-476" aria-hidden="true" tabindex="-1"></a><span class="co">        The modified model, ready for use with a ViT backbone.</span></span>
<span id="cb17-477"><a href="#cb17-477" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-478"><a href="#cb17-478" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create single-scale RoIAlign; output size 7x7 is default for detection heads</span></span>
<span id="cb17-479"><a href="#cb17-479" aria-hidden="true" tabindex="-1"></a>    roi_align <span class="op">=</span> RoIAlign(</span>
<span id="cb17-480"><a href="#cb17-480" aria-hidden="true" tabindex="-1"></a>        output_size<span class="op">=</span><span class="dv">7</span>,</span>
<span id="cb17-481"><a href="#cb17-481" aria-hidden="true" tabindex="-1"></a>        spatial_scale<span class="op">=</span><span class="fl">1.0</span><span class="op">/</span><span class="dv">16</span>,  <span class="co"># ViT stride=16 (e.g., input 512 -&gt; 32x32)</span></span>
<span id="cb17-482"><a href="#cb17-482" aria-hidden="true" tabindex="-1"></a>        sampling_ratio<span class="op">=</span><span class="dv">2</span></span>
<span id="cb17-483"><a href="#cb17-483" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-484"><a href="#cb17-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-485"><a href="#cb17-485" aria-hidden="true" tabindex="-1"></a>    <span class="kw">class</span> SingleScaleRoIPool(nn.Module):</span>
<span id="cb17-486"><a href="#cb17-486" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb17-487"><a href="#cb17-487" aria-hidden="true" tabindex="-1"></a><span class="co">        A wrapper for single-scale RoIAlign to mimic expected interface of MultiScaleRoIAlign,</span></span>
<span id="cb17-488"><a href="#cb17-488" aria-hidden="true" tabindex="-1"></a><span class="co">        allowing seamless plug-in to torchvision's Faster R-CNN code.</span></span>
<span id="cb17-489"><a href="#cb17-489" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb17-490"><a href="#cb17-490" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, roi_align):</span>
<span id="cb17-491"><a href="#cb17-491" aria-hidden="true" tabindex="-1"></a>            <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb17-492"><a href="#cb17-492" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.roi_align <span class="op">=</span> roi_align</span>
<span id="cb17-493"><a href="#cb17-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-494"><a href="#cb17-494" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> forward(<span class="va">self</span>, features, proposals, image_shapes):</span>
<span id="cb17-495"><a href="#cb17-495" aria-hidden="true" tabindex="-1"></a>            <span class="co">"""</span></span>
<span id="cb17-496"><a href="#cb17-496" aria-hidden="true" tabindex="-1"></a><span class="co">            Apply RoIAlign to a single feature map.</span></span>
<span id="cb17-497"><a href="#cb17-497" aria-hidden="true" tabindex="-1"></a><span class="co">            - features: Either a dict (assumed to contain only one entry) or a Tensor.</span></span>
<span id="cb17-498"><a href="#cb17-498" aria-hidden="true" tabindex="-1"></a><span class="co">            - proposals: List of proposal boxes per image (as required by RoIAlign).</span></span>
<span id="cb17-499"><a href="#cb17-499" aria-hidden="true" tabindex="-1"></a><span class="co">            - image_shapes: (Ignored) present for API compatibility only.</span></span>
<span id="cb17-500"><a href="#cb17-500" aria-hidden="true" tabindex="-1"></a><span class="co">            Returns: Tensor of pooled RoI features.</span></span>
<span id="cb17-501"><a href="#cb17-501" aria-hidden="true" tabindex="-1"></a><span class="co">            """</span></span>
<span id="cb17-502"><a href="#cb17-502" aria-hidden="true" tabindex="-1"></a>            <span class="co"># If features is a dict (from model), extract its sole value; otherwise, use it as-is</span></span>
<span id="cb17-503"><a href="#cb17-503" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(features, <span class="bu">dict</span>):</span>
<span id="cb17-504"><a href="#cb17-504" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> <span class="bu">list</span>(features.values())[<span class="dv">0</span>]</span>
<span id="cb17-505"><a href="#cb17-505" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb17-506"><a href="#cb17-506" aria-hidden="true" tabindex="-1"></a>                x <span class="op">=</span> features</span>
<span id="cb17-507"><a href="#cb17-507" aria-hidden="true" tabindex="-1"></a>            <span class="co"># image_shapes argument is not used, as RoIAlign doesn't require it for single-scale</span></span>
<span id="cb17-508"><a href="#cb17-508" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.roi_align(x, proposals)</span>
<span id="cb17-509"><a href="#cb17-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-510"><a href="#cb17-510" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Swap out multi-scale RoI pooler for single-scale version</span></span>
<span id="cb17-511"><a href="#cb17-511" aria-hidden="true" tabindex="-1"></a>    model.roi_heads.box_roi_pool <span class="op">=</span> SingleScaleRoIPool(roi_align)</span>
<span id="cb17-512"><a href="#cb17-512" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb17-513"><a href="#cb17-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-514"><a href="#cb17-514" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch the model to use the new single-scale ROI pooler</span></span>
<span id="cb17-515"><a href="#cb17-515" aria-hidden="true" tabindex="-1"></a>detection_model <span class="op">=</span> replace_roi_pooler(detection_model)</span>
<span id="cb17-516"><a href="#cb17-516" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Custom ROI pooler configured for single-scale features"</span>)</span>
<span id="cb17-517"><a href="#cb17-517" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-518"><a href="#cb17-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-519"><a href="#cb17-519" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb17-520"><a href="#cb17-520" aria-hidden="true" tabindex="-1"></a><span class="fu">## Four Key Adaptations for ViT Detection</span></span>
<span id="cb17-521"><a href="#cb17-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-522"><a href="#cb17-522" aria-hidden="true" tabindex="-1"></a>To make Prithvi ViT work with Faster R-CNN, we made four critical changes:</span>
<span id="cb17-523"><a href="#cb17-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-524"><a href="#cb17-524" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Skip FPN (neck=None)**: Avoid multi-scale complexity, work directly on single 32√ó32 feature map</span>
<span id="cb17-525"><a href="#cb17-525" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Custom RPN**: Single-scale anchor generator with 15,360 anchors tuned for 512√ó512 images</span>
<span id="cb17-526"><a href="#cb17-526" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Custom ROI Pooler**: Replace MultiScaleRoIAlign with single-scale RoIAlign at 1/16 spatial scale</span>
<span id="cb17-527"><a href="#cb17-527" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Custom Forward Pass**: Reshape ViT sequence <span class="co">[</span><span class="ot">B,1024,768</span><span class="co">]</span> ‚Üí spatial <span class="co">[</span><span class="ot">B,768,32,32</span><span class="co">]</span> with MPS compatibility</span>
<span id="cb17-528"><a href="#cb17-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-529"><a href="#cb17-529" aria-hidden="true" tabindex="-1"></a>These adaptations allow single-scale detection without FPN complexity while maintaining full compatibility with Faster R-CNN's architecture.</span>
<span id="cb17-530"><a href="#cb17-530" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-531"><a href="#cb17-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-532"><a href="#cb17-532" aria-hidden="true" tabindex="-1"></a>Apply the compatibility patch for ViT features:</span>
<span id="cb17-533"><a href="#cb17-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-536"><a href="#cb17-536" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-537"><a href="#cb17-537" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fixed_fasterrcnn_forward(<span class="va">self</span>, images, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb17-538"><a href="#cb17-538" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-539"><a href="#cb17-539" aria-hidden="true" tabindex="-1"></a><span class="co">    Custom forward pass to enable Faster R-CNN with Prithvi ViT backbone.</span></span>
<span id="cb17-540"><a href="#cb17-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-541"><a href="#cb17-541" aria-hidden="true" tabindex="-1"></a><span class="co">    This replaces the original forward to handle single-scale ViT features, which differ</span></span>
<span id="cb17-542"><a href="#cb17-542" aria-hidden="true" tabindex="-1"></a><span class="co">    from typical multi-scale CNN backbones (like FPN).</span></span>
<span id="cb17-543"><a href="#cb17-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-544"><a href="#cb17-544" aria-hidden="true" tabindex="-1"></a><span class="co">    Steps performed:</span></span>
<span id="cb17-545"><a href="#cb17-545" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Converts ViT sequence output from shape [B, N, C] into a spatial feature map [B, C, H, W], </span></span>
<span id="cb17-546"><a href="#cb17-546" aria-hidden="true" tabindex="-1"></a><span class="co">        where:</span></span>
<span id="cb17-547"><a href="#cb17-547" aria-hidden="true" tabindex="-1"></a><span class="co">         B=batch size, </span></span>
<span id="cb17-548"><a href="#cb17-548" aria-hidden="true" tabindex="-1"></a><span class="co">         N=number of tokens (patches), </span></span>
<span id="cb17-549"><a href="#cb17-549" aria-hidden="true" tabindex="-1"></a><span class="co">         C=channels, and </span></span>
<span id="cb17-550"><a href="#cb17-550" aria-hidden="true" tabindex="-1"></a><span class="co">         H and W are the height and width of the feature map (typically recovered from N as H √ó W = N).</span></span>
<span id="cb17-551"><a href="#cb17-551" aria-hidden="true" tabindex="-1"></a><span class="co">       - Handles [CLS] token (N==1025) by removing the first token.</span></span>
<span id="cb17-552"><a href="#cb17-552" aria-hidden="true" tabindex="-1"></a><span class="co">       - Reshapes flat sequence into square map (ViT patch output).</span></span>
<span id="cb17-553"><a href="#cb17-553" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Wraps features in an OrderedDict, preserving required interface.</span></span>
<span id="cb17-554"><a href="#cb17-554" aria-hidden="true" tabindex="-1"></a><span class="co">       - Ensures tensor data is contiguous for MPS (Apple Silicon) and general backend compatibility.</span></span>
<span id="cb17-555"><a href="#cb17-555" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Proceeds with proposal, ROI, and loss computation as usual.</span></span>
<span id="cb17-556"><a href="#cb17-556" aria-hidden="true" tabindex="-1"></a><span class="co">    4. Returns post-processed detections or losses depending on training mode.</span></span>
<span id="cb17-557"><a href="#cb17-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-558"><a href="#cb17-558" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb17-559"><a href="#cb17-559" aria-hidden="true" tabindex="-1"></a><span class="co">        images: List of input images, already as Tensors.</span></span>
<span id="cb17-560"><a href="#cb17-560" aria-hidden="true" tabindex="-1"></a><span class="co">        targets: List of ground truth targets (boxes/labels). Required in training mode.</span></span>
<span id="cb17-561"><a href="#cb17-561" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb17-562"><a href="#cb17-562" aria-hidden="true" tabindex="-1"></a><span class="co">        Loss dict (training mode) or list of detection outputs (eval mode).</span></span>
<span id="cb17-563"><a href="#cb17-563" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-564"><a href="#cb17-564" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training mode sanity check‚Äîtargets required for loss computation</span></span>
<span id="cb17-565"><a href="#cb17-565" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.training <span class="kw">and</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb17-566"><a href="#cb17-566" aria-hidden="true" tabindex="-1"></a>        torch._assert(<span class="va">False</span>, <span class="st">"targets should not be none when in training mode"</span>)</span>
<span id="cb17-567"><a href="#cb17-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-568"><a href="#cb17-568" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Record original sizes before transforms (needed for inverse mapping in postprocess)</span></span>
<span id="cb17-569"><a href="#cb17-569" aria-hidden="true" tabindex="-1"></a>    original_image_sizes <span class="op">=</span> []</span>
<span id="cb17-570"><a href="#cb17-570" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> img <span class="kw">in</span> images:</span>
<span id="cb17-571"><a href="#cb17-571" aria-hidden="true" tabindex="-1"></a>        val <span class="op">=</span> img.shape[<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb17-572"><a href="#cb17-572" aria-hidden="true" tabindex="-1"></a>        original_image_sizes.append((val[<span class="dv">0</span>], val[<span class="dv">1</span>]))</span>
<span id="cb17-573"><a href="#cb17-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-574"><a href="#cb17-574" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply input transforms (e.g. resizing, normalization)</span></span>
<span id="cb17-575"><a href="#cb17-575" aria-hidden="true" tabindex="-1"></a>    images, targets <span class="op">=</span> <span class="va">self</span>.transform(images, targets)</span>
<span id="cb17-576"><a href="#cb17-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-577"><a href="#cb17-577" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward through the ViT backbone</span></span>
<span id="cb17-578"><a href="#cb17-578" aria-hidden="true" tabindex="-1"></a>    features <span class="op">=</span> <span class="va">self</span>.backbone(images.tensors)  <span class="co"># Could be Tensor, list, or OrderedDict</span></span>
<span id="cb17-579"><a href="#cb17-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-580"><a href="#cb17-580" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If backbone outputs a list, use the last element (common for some backbones)</span></span>
<span id="cb17-581"><a href="#cb17-581" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(features, <span class="bu">list</span>):</span>
<span id="cb17-582"><a href="#cb17-582" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> features[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-583"><a href="#cb17-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-584"><a href="#cb17-584" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If features are [B, N, C] (ViT style: sequence length), reshape to [B, C, H, W]</span></span>
<span id="cb17-585"><a href="#cb17-585" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(features, torch.Tensor) <span class="kw">and</span> features.ndim <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb17-586"><a href="#cb17-586" aria-hidden="true" tabindex="-1"></a>        B, N, C <span class="op">=</span> features.shape</span>
<span id="cb17-587"><a href="#cb17-587" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Some ViT models output CLS token: strip if N==1025 ([B, 1025, C])</span></span>
<span id="cb17-588"><a href="#cb17-588" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> N <span class="op">==</span> <span class="dv">1025</span>:</span>
<span id="cb17-589"><a href="#cb17-589" aria-hidden="true" tabindex="-1"></a>            features <span class="op">=</span> features[:, <span class="dv">1</span>:, :]</span>
<span id="cb17-590"><a href="#cb17-590" aria-hidden="true" tabindex="-1"></a>            N <span class="op">=</span> <span class="dv">1024</span>  <span class="co"># Now we have only patch tokens</span></span>
<span id="cb17-591"><a href="#cb17-591" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Recover H and W; for ViT this is usually 32x32</span></span>
<span id="cb17-592"><a href="#cb17-592" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> W <span class="op">=</span> <span class="bu">int</span>(math.sqrt(N))</span>
<span id="cb17-593"><a href="#cb17-593" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Rearrange to [B, C, H, W] for convolutional-style heads</span></span>
<span id="cb17-594"><a href="#cb17-594" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> features.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>).reshape(B, C, H, W)</span>
<span id="cb17-595"><a href="#cb17-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-596"><a href="#cb17-596" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to OrderedDict if not already, as expected by torchvision detection components.</span></span>
<span id="cb17-597"><a href="#cb17-597" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Always call .contiguous() for MPS (Metal) backend compatibility and for safety.</span></span>
<span id="cb17-598"><a href="#cb17-598" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(features, OrderedDict):</span>
<span id="cb17-599"><a href="#cb17-599" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Only one feature map ('0' key)</span></span>
<span id="cb17-600"><a href="#cb17-600" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> OrderedDict([(<span class="st">"0"</span>, features.contiguous())])</span>
<span id="cb17-601"><a href="#cb17-601" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb17-602"><a href="#cb17-602" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Make sure all tensors in the dict are contiguous</span></span>
<span id="cb17-603"><a href="#cb17-603" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> OrderedDict([(k, v.contiguous()) <span class="cf">for</span> k, v <span class="kw">in</span> features.items()])</span>
<span id="cb17-604"><a href="#cb17-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-605"><a href="#cb17-605" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate proposals with Region Proposal Network (RPN), then detect boxes/labels/sores</span></span>
<span id="cb17-606"><a href="#cb17-606" aria-hidden="true" tabindex="-1"></a>    proposals, proposal_losses <span class="op">=</span> <span class="va">self</span>.rpn(images, features, targets)</span>
<span id="cb17-607"><a href="#cb17-607" aria-hidden="true" tabindex="-1"></a>    detections, detector_losses <span class="op">=</span> <span class="va">self</span>.roi_heads(</span>
<span id="cb17-608"><a href="#cb17-608" aria-hidden="true" tabindex="-1"></a>        features, proposals, images.image_sizes, targets</span>
<span id="cb17-609"><a href="#cb17-609" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-610"><a href="#cb17-610" aria-hidden="true" tabindex="-1"></a>    detections <span class="op">=</span> <span class="va">self</span>.transform.postprocess(</span>
<span id="cb17-611"><a href="#cb17-611" aria-hidden="true" tabindex="-1"></a>        detections, images.image_sizes, original_image_sizes</span>
<span id="cb17-612"><a href="#cb17-612" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-613"><a href="#cb17-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-614"><a href="#cb17-614" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Aggregate all computed losses for training, or return detections in eval mode</span></span>
<span id="cb17-615"><a href="#cb17-615" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> {}</span>
<span id="cb17-616"><a href="#cb17-616" aria-hidden="true" tabindex="-1"></a>    losses.update(detector_losses)</span>
<span id="cb17-617"><a href="#cb17-617" aria-hidden="true" tabindex="-1"></a>    losses.update(proposal_losses)</span>
<span id="cb17-618"><a href="#cb17-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-619"><a href="#cb17-619" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb17-620"><a href="#cb17-620" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> losses</span>
<span id="cb17-621"><a href="#cb17-621" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> detections</span>
<span id="cb17-622"><a href="#cb17-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-623"><a href="#cb17-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-624"><a href="#cb17-624" aria-hidden="true" tabindex="-1"></a><span class="co"># Overwrite the model's forward method with our ViT-compatible implementation</span></span>
<span id="cb17-625"><a href="#cb17-625" aria-hidden="true" tabindex="-1"></a>detection_model.forward <span class="op">=</span> types.MethodType(fixed_fasterrcnn_forward, detection_model)</span>
<span id="cb17-626"><a href="#cb17-626" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Compatibility patch applied for ViT backbone (single-scale features)"</span>)</span>
<span id="cb17-627"><a href="#cb17-627" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model ready for training or inference"</span>)</span>
<span id="cb17-628"><a href="#cb17-628" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-629"><a href="#cb17-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-630"><a href="#cb17-630" aria-hidden="true" tabindex="-1"></a>:::{.callout-important}</span>
<span id="cb17-631"><a href="#cb17-631" aria-hidden="true" tabindex="-1"></a><span class="fu">## Complete Solution: Three Adaptations for ViT Detection</span></span>
<span id="cb17-632"><a href="#cb17-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-633"><a href="#cb17-633" aria-hidden="true" tabindex="-1"></a>**The Challenge:**</span>
<span id="cb17-634"><a href="#cb17-634" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Default Faster R-CNN expects multi-scale CNN features (FPN pyramid)</span>
<span id="cb17-635"><a href="#cb17-635" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prithvi ViT produces single-scale features (32√ó32 at stride 16)</span>
<span id="cb17-636"><a href="#cb17-636" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-scale components (RPN, ROI pooler) fail with single-scale input</span>
<span id="cb17-637"><a href="#cb17-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-638"><a href="#cb17-638" aria-hidden="true" tabindex="-1"></a>**Our Three-Part Solution:**</span>
<span id="cb17-639"><a href="#cb17-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-640"><a href="#cb17-640" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Skip FPN** (<span class="in">`neck=None`</span>)</span>
<span id="cb17-641"><a href="#cb17-641" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Avoid multi-scale complexity entirely</span>
<span id="cb17-642"><a href="#cb17-642" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Direct single-scale detection</span>
<span id="cb17-643"><a href="#cb17-643" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Works directly on ViT's 32√ó32 feature map</span>
<span id="cb17-644"><a href="#cb17-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-645"><a href="#cb17-645" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Custom RPN**</span>
<span id="cb17-646"><a href="#cb17-646" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Single anchor tuple for 32√ó32 feature map</span>
<span id="cb17-647"><a href="#cb17-647" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Anchor sizes: 16, 32, 64, 128, 256 pixels (tuned for 512√ó512 input)</span>
<span id="cb17-648"><a href="#cb17-648" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Aspect ratios: 0.5 (tall), 1.0 (square), 2.0 (wide)</span>
<span id="cb17-649"><a href="#cb17-649" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Total: 5 sizes √ó 3 ratios √ó 32√ó32 locations = 15,360 anchors per image</span>
<span id="cb17-650"><a href="#cb17-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-651"><a href="#cb17-651" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Custom ROI Pooler**</span>
<span id="cb17-652"><a href="#cb17-652" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Replace <span class="in">`MultiScaleRoIAlign`</span> with simple <span class="in">`RoIAlign`</span></span>
<span id="cb17-653"><a href="#cb17-653" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Spatial scale: 1/16 (matches ViT stride)</span>
<span id="cb17-654"><a href="#cb17-654" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: 7√ó7 features per proposal (standard for detection head)</span>
<span id="cb17-655"><a href="#cb17-655" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Bypass FPN-style feature map filtering</span>
<span id="cb17-656"><a href="#cb17-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-657"><a href="#cb17-657" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Custom Forward Pass**</span>
<span id="cb17-658"><a href="#cb17-658" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Reshape ViT sequence <span class="co">[</span><span class="ot">B, 1024, 768</span><span class="co">]</span> ‚Üí spatial <span class="co">[</span><span class="ot">B, 768, 32, 32</span><span class="co">]</span></span>
<span id="cb17-659"><a href="#cb17-659" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Handle <span class="co">[</span><span class="ot">CLS</span><span class="co">]</span> token removal if present</span>
<span id="cb17-660"><a href="#cb17-660" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Ensure tensor contiguity for MPS (Apple Silicon) compatibility</span>
<span id="cb17-661"><a href="#cb17-661" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Wrap features in OrderedDict for torchvision compatibility</span>
<span id="cb17-662"><a href="#cb17-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-663"><a href="#cb17-663" aria-hidden="true" tabindex="-1"></a>**Why This Works for DIOR:**</span>
<span id="cb17-664"><a href="#cb17-664" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fixed 800√ó800 images ‚Üí resize to 512√ó512 ‚Üí consistent 32√ó32 features</span>
<span id="cb17-665"><a href="#cb17-665" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Objects range 20-200 pixels after resize (anchors cover this range)</span>
<span id="cb17-666"><a href="#cb17-666" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Single-scale sufficient for consistent aerial imagery</span>
<span id="cb17-667"><a href="#cb17-667" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Simpler architecture, fewer failure points</span>
<span id="cb17-668"><a href="#cb17-668" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-669"><a href="#cb17-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-670"><a href="#cb17-670" aria-hidden="true" tabindex="-1"></a><span class="fu">## Prepare Data for Training</span></span>
<span id="cb17-671"><a href="#cb17-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-672"><a href="#cb17-672" aria-hidden="true" tabindex="-1"></a>Let's create train and validation splits from the DIOR dataset. We'll use a subset for faster training in this demonstration.</span>
<span id="cb17-673"><a href="#cb17-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-676"><a href="#cb17-676" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-677"><a href="#cb17-677" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Subset</span>
<span id="cb17-678"><a href="#cb17-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-679"><a href="#cb17-679" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom collate function for DIOR with Prithvi</span></span>
<span id="cb17-680"><a href="#cb17-680" aria-hidden="true" tabindex="-1"></a><span class="co"># DIOR has fixed 800√ó800 images - perfect for ViT!</span></span>
<span id="cb17-681"><a href="#cb17-681" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll resize to 512√ó512 (divisible by 16 for patch_size)</span></span>
<span id="cb17-682"><a href="#cb17-682" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb17-683"><a href="#cb17-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-684"><a href="#cb17-684" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> detection_collate_fn(batch):</span>
<span id="cb17-685"><a href="#cb17-685" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-686"><a href="#cb17-686" aria-hidden="true" tabindex="-1"></a><span class="co">    Custom collate function for DIOR with Prithvi ViT.</span></span>
<span id="cb17-687"><a href="#cb17-687" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb17-688"><a href="#cb17-688" aria-hidden="true" tabindex="-1"></a><span class="co">    Since DIOR images are all 800√ó800:</span></span>
<span id="cb17-689"><a href="#cb17-689" aria-hidden="true" tabindex="-1"></a><span class="co">    - Simple resize to 512√ó512 (no padding needed!)</span></span>
<span id="cb17-690"><a href="#cb17-690" aria-hidden="true" tabindex="-1"></a><span class="co">    - Scale bounding boxes proportionally</span></span>
<span id="cb17-691"><a href="#cb17-691" aria-hidden="true" tabindex="-1"></a><span class="co">    - ViT gets consistent 1024 patches (32√ó32 grid with patch_size=16)</span></span>
<span id="cb17-692"><a href="#cb17-692" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-693"><a href="#cb17-693" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> []</span>
<span id="cb17-694"><a href="#cb17-694" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> []</span>
<span id="cb17-695"><a href="#cb17-695" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-696"><a href="#cb17-696" aria-hidden="true" tabindex="-1"></a>    target_size <span class="op">=</span> <span class="dv">512</span>  <span class="co"># Resize to 512√ó512 for ViT</span></span>
<span id="cb17-697"><a href="#cb17-697" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-698"><a href="#cb17-698" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sample <span class="kw">in</span> batch:</span>
<span id="cb17-699"><a href="#cb17-699" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract image and convert to tensor if needed</span></span>
<span id="cb17-700"><a href="#cb17-700" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> sample[<span class="st">'image'</span>]</span>
<span id="cb17-701"><a href="#cb17-701" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(img, torch.Tensor):</span>
<span id="cb17-702"><a href="#cb17-702" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert PIL to tensor</span></span>
<span id="cb17-703"><a href="#cb17-703" aria-hidden="true" tabindex="-1"></a>            <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-704"><a href="#cb17-704" aria-hidden="true" tabindex="-1"></a>            img <span class="op">=</span> torch.from_numpy(np.array(img)).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>).<span class="bu">float</span>() <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb17-705"><a href="#cb17-705" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-706"><a href="#cb17-706" aria-hidden="true" tabindex="-1"></a>        _, orig_h, orig_w <span class="op">=</span> img.shape  <span class="co"># Should be 800√ó800</span></span>
<span id="cb17-707"><a href="#cb17-707" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-708"><a href="#cb17-708" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize to target size</span></span>
<span id="cb17-709"><a href="#cb17-709" aria-hidden="true" tabindex="-1"></a>        img_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb17-710"><a href="#cb17-710" aria-hidden="true" tabindex="-1"></a>            img.unsqueeze(<span class="dv">0</span>),</span>
<span id="cb17-711"><a href="#cb17-711" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>(target_size, target_size),</span>
<span id="cb17-712"><a href="#cb17-712" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb17-713"><a href="#cb17-713" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb17-714"><a href="#cb17-714" aria-hidden="true" tabindex="-1"></a>        ).squeeze(<span class="dv">0</span>)</span>
<span id="cb17-715"><a href="#cb17-715" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-716"><a href="#cb17-716" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Scale bounding boxes (800√ó800 ‚Üí 512√ó512)</span></span>
<span id="cb17-717"><a href="#cb17-717" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> target_size <span class="op">/</span> orig_w  <span class="co"># 512 / 800 = 0.64</span></span>
<span id="cb17-718"><a href="#cb17-718" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-719"><a href="#cb17-719" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">=</span> sample[<span class="st">'bbox_xyxy'</span>].clone().<span class="bu">float</span>()</span>
<span id="cb17-720"><a href="#cb17-720" aria-hidden="true" tabindex="-1"></a>        boxes <span class="op">*=</span> scale</span>
<span id="cb17-721"><a href="#cb17-721" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-722"><a href="#cb17-722" aria-hidden="true" tabindex="-1"></a>        images.append(img_resized)</span>
<span id="cb17-723"><a href="#cb17-723" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> {</span>
<span id="cb17-724"><a href="#cb17-724" aria-hidden="true" tabindex="-1"></a>            <span class="st">'boxes'</span>: boxes,</span>
<span id="cb17-725"><a href="#cb17-725" aria-hidden="true" tabindex="-1"></a>            <span class="st">'labels'</span>: sample[<span class="st">'label'</span>]</span>
<span id="cb17-726"><a href="#cb17-726" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb17-727"><a href="#cb17-727" aria-hidden="true" tabindex="-1"></a>        targets.append(target)</span>
<span id="cb17-728"><a href="#cb17-728" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-729"><a href="#cb17-729" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> images, targets</span>
<span id="cb17-730"><a href="#cb17-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-731"><a href="#cb17-731" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"‚úì Custom collate function defined for DIOR"</span>)</span>
<span id="cb17-732"><a href="#cb17-732" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Resizes 800√ó800 ‚Üí 512√ó512 (ViT friendly!)"</span>)</span>
<span id="cb17-733"><a href="#cb17-733" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - Scales bounding boxes proportionally"</span>)</span>
<span id="cb17-734"><a href="#cb17-734" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"  - No padding needed (square images)"</span>)</span>
<span id="cb17-735"><a href="#cb17-735" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-736"><a href="#cb17-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-737"><a href="#cb17-737" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb17-738"><a href="#cb17-738" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why 512√ó512 for Prithvi?</span></span>
<span id="cb17-739"><a href="#cb17-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-740"><a href="#cb17-740" aria-hidden="true" tabindex="-1"></a>**Patch Size = 16 pixels:**</span>
<span id="cb17-741"><a href="#cb17-741" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>512√∑16 = 32 patches per dimension</span>
<span id="cb17-742"><a href="#cb17-742" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Total: 32√ó32 = 1,024 patches</span>
<span id="cb17-743"><a href="#cb17-743" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Perfect square ‚úÖ</span>
<span id="cb17-744"><a href="#cb17-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-745"><a href="#cb17-745" aria-hidden="true" tabindex="-1"></a>**Alternatives:**</span>
<span id="cb17-746"><a href="#cb17-746" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**224√ó224**: 14√ó14 = 196 patches (smaller, faster)</span>
<span id="cb17-747"><a href="#cb17-747" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**448√ó448**: 28√ó28 = 784 patches (medium)</span>
<span id="cb17-748"><a href="#cb17-748" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**800√ó800**: 50√ó50 = 2,500 patches (original size, slower)</span>
<span id="cb17-749"><a href="#cb17-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-750"><a href="#cb17-750" aria-hidden="true" tabindex="-1"></a>We chose 512√ó512 as a good balance between resolution and speed!</span>
<span id="cb17-751"><a href="#cb17-751" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-752"><a href="#cb17-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-755"><a href="#cb17-755" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-756"><a href="#cb17-756" aria-hidden="true" tabindex="-1"></a><span class="co"># Load validation split from Hugging Face</span></span>
<span id="cb17-757"><a href="#cb17-757" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loading validation split..."</span>)</span>
<span id="cb17-758"><a href="#cb17-758" aria-hidden="true" tabindex="-1"></a>hf_val <span class="op">=</span> load_dataset(<span class="st">"HichTala/dior"</span>, split<span class="op">=</span><span class="st">"validation"</span>)</span>
<span id="cb17-759"><a href="#cb17-759" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> DIORPyTorchDataset(hf_val)</span>
<span id="cb17-760"><a href="#cb17-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-761"><a href="#cb17-761" aria-hidden="true" tabindex="-1"></a><span class="co"># For this demo, use subset of training data to speed things up</span></span>
<span id="cb17-762"><a href="#cb17-762" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> Subset(dataset_train, <span class="bu">range</span>(<span class="dv">1000</span>))  <span class="co"># Use first 1000 images</span></span>
<span id="cb17-763"><a href="#cb17-763" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> Subset(val_dataset, <span class="bu">range</span>(<span class="dv">200</span>))  <span class="co"># Use first 200 val images</span></span>
<span id="cb17-764"><a href="#cb17-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-765"><a href="#cb17-765" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Training samples: </span><span class="sc">{</span><span class="bu">len</span>(train_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-766"><a href="#cb17-766" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Validation samples: </span><span class="sc">{</span><span class="bu">len</span>(val_dataset)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-767"><a href="#cb17-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-768"><a href="#cb17-768" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data loaders with custom collate function</span></span>
<span id="cb17-769"><a href="#cb17-769" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(</span>
<span id="cb17-770"><a href="#cb17-770" aria-hidden="true" tabindex="-1"></a>    train_dataset, </span>
<span id="cb17-771"><a href="#cb17-771" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">2</span>,            <span class="co"># Small batch size for detection</span></span>
<span id="cb17-772"><a href="#cb17-772" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb17-773"><a href="#cb17-773" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">0</span>,                     <span class="co"># Set to 0 for compatibility</span></span>
<span id="cb17-774"><a href="#cb17-774" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>detection_collate_fn    <span class="co"># Use custom collate function</span></span>
<span id="cb17-775"><a href="#cb17-775" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-776"><a href="#cb17-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-777"><a href="#cb17-777" aria-hidden="true" tabindex="-1"></a>val_loader <span class="op">=</span> DataLoader(</span>
<span id="cb17-778"><a href="#cb17-778" aria-hidden="true" tabindex="-1"></a>    val_dataset,</span>
<span id="cb17-779"><a href="#cb17-779" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb17-780"><a href="#cb17-780" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb17-781"><a href="#cb17-781" aria-hidden="true" tabindex="-1"></a>    num_workers<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb17-782"><a href="#cb17-782" aria-hidden="true" tabindex="-1"></a>    collate_fn<span class="op">=</span>detection_collate_fn</span>
<span id="cb17-783"><a href="#cb17-783" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-784"><a href="#cb17-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-785"><a href="#cb17-785" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Training batches: </span><span class="sc">{</span><span class="bu">len</span>(train_loader)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-786"><a href="#cb17-786" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"‚úì Validation batches: </span><span class="sc">{</span><span class="bu">len</span>(val_loader)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-787"><a href="#cb17-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-788"><a href="#cb17-788" aria-hidden="true" tabindex="-1"></a><span class="co"># Test the data loader</span></span>
<span id="cb17-789"><a href="#cb17-789" aria-hidden="true" tabindex="-1"></a>sample_batch <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader))</span>
<span id="cb17-790"><a href="#cb17-790" aria-hidden="true" tabindex="-1"></a>images, targets <span class="op">=</span> sample_batch</span>
<span id="cb17-791"><a href="#cb17-791" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">‚úì Batch structure:"</span>)</span>
<span id="cb17-792"><a href="#cb17-792" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Images: list of </span><span class="sc">{</span><span class="bu">len</span>(images)<span class="sc">}</span><span class="ss"> tensors"</span>)</span>
<span id="cb17-793"><a href="#cb17-793" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Image 0 shape: </span><span class="sc">{</span>images[<span class="dv">0</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-794"><a href="#cb17-794" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Image 1 shape: </span><span class="sc">{</span>images[<span class="dv">1</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-795"><a href="#cb17-795" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Targets: list of </span><span class="sc">{</span><span class="bu">len</span>(targets)<span class="sc">}</span><span class="ss"> dicts"</span>)</span>
<span id="cb17-796"><a href="#cb17-796" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Target 0 boxes: </span><span class="sc">{</span>targets[<span class="dv">0</span>][<span class="st">'boxes'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-797"><a href="#cb17-797" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"  - Target 0 labels: </span><span class="sc">{</span>targets[<span class="dv">0</span>][<span class="st">'labels'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-798"><a href="#cb17-798" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-799"><a href="#cb17-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-800"><a href="#cb17-800" aria-hidden="true" tabindex="-1"></a><span class="fu">## Train the Object Detection Model</span></span>
<span id="cb17-801"><a href="#cb17-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-802"><a href="#cb17-802" aria-hidden="true" tabindex="-1"></a>Now let's train the model! With all our adaptations in place, the model is ready for training.</span>
<span id="cb17-803"><a href="#cb17-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-804"><a href="#cb17-804" aria-hidden="true" tabindex="-1"></a>**What we've configured:**</span>
<span id="cb17-805"><a href="#cb17-805" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ Prithvi ViT backbone with single-scale 32√ó32 features</span>
<span id="cb17-806"><a href="#cb17-806" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ Custom RPN with 15,360 anchors tuned for 512√ó512 images</span>
<span id="cb17-807"><a href="#cb17-807" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ Custom single-scale ROI pooler (spatial scale 1/16)</span>
<span id="cb17-808"><a href="#cb17-808" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ Custom forward pass handling ViT sequence ‚Üí spatial reshaping</span>
<span id="cb17-809"><a href="#cb17-809" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ Fixed image transform (512√ó512, no resizing)</span>
<span id="cb17-810"><a href="#cb17-810" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>‚úÖ Data pipeline: 800√ó800 DIOR ‚Üí 512√ó512 with bbox scaling</span>
<span id="cb17-811"><a href="#cb17-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-812"><a href="#cb17-812" aria-hidden="true" tabindex="-1"></a>**Object detection training differs from classification:**</span>
<span id="cb17-813"><a href="#cb17-813" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model computes losses internally (RPN objectness/bbox + ROI classification/bbox)</span>
<span id="cb17-814"><a href="#cb17-814" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Must pass both images AND targets during training</span>
<span id="cb17-815"><a href="#cb17-815" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Returns dict of losses in train mode, predictions in eval mode</span>
<span id="cb17-816"><a href="#cb17-816" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training typically requires 20-50 epochs for convergence (we'll start with 2 for demo)</span>
<span id="cb17-817"><a href="#cb17-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-818"><a href="#cb17-818" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb17-819"><a href="#cb17-819" aria-hidden="true" tabindex="-1"></a><span class="fu">## GPU Acceleration with Automatic Fallback</span></span>
<span id="cb17-820"><a href="#cb17-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-821"><a href="#cb17-821" aria-hidden="true" tabindex="-1"></a>The training function automatically detects and uses available hardware acceleration:</span>
<span id="cb17-822"><a href="#cb17-822" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Apple Silicon (M1/M2/M3)**: Uses MPS (Metal Performance Shaders)</span>
<span id="cb17-823"><a href="#cb17-823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**NVIDIA GPUs**: Uses CUDA</span>
<span id="cb17-824"><a href="#cb17-824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CPU**: Falls back to CPU if no GPU available</span>
<span id="cb17-825"><a href="#cb17-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-826"><a href="#cb17-826" aria-hidden="true" tabindex="-1"></a>**MPS Compatibility**: Some operations may not be fully supported on MPS. The training loop includes automatic fallback to CPU if MPS errors occur, ensuring training completes successfully.</span>
<span id="cb17-827"><a href="#cb17-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-828"><a href="#cb17-828" aria-hidden="true" tabindex="-1"></a>This can significantly speed up training on compatible hardware!</span>
<span id="cb17-829"><a href="#cb17-829" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-830"><a href="#cb17-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-833"><a href="#cb17-833" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-834"><a href="#cb17-834" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-835"><a href="#cb17-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-836"><a href="#cb17-836" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_detection_model(model, train_loader, val_loader, num_epochs<span class="op">=</span><span class="dv">2</span>, lr<span class="op">=</span><span class="fl">0.0001</span>):</span>
<span id="cb17-837"><a href="#cb17-837" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Train an object detection model with MPS support."""</span></span>
<span id="cb17-838"><a href="#cb17-838" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try MPS (Apple Silicon) first, then CUDA, then CPU</span></span>
<span id="cb17-839"><a href="#cb17-839" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb17-840"><a href="#cb17-840" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb17-841"><a href="#cb17-841" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training on: Apple Silicon (MPS)"</span>)</span>
<span id="cb17-842"><a href="#cb17-842" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Note: If MPS errors occur, model will fall back to CPU"</span>)</span>
<span id="cb17-843"><a href="#cb17-843" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb17-844"><a href="#cb17-844" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb17-845"><a href="#cb17-845" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training on: CUDA"</span>)</span>
<span id="cb17-846"><a href="#cb17-846" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb17-847"><a href="#cb17-847" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-848"><a href="#cb17-848" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Training on: CPU"</span>)</span>
<span id="cb17-849"><a href="#cb17-849" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb17-850"><a href="#cb17-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-851"><a href="#cb17-851" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb17-852"><a href="#cb17-852" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb17-853"><a href="#cb17-853" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-854"><a href="#cb17-854" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Failed to move model to </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-855"><a href="#cb17-855" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Falling back to CPU"</span>)</span>
<span id="cb17-856"><a href="#cb17-856" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-857"><a href="#cb17-857" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb17-858"><a href="#cb17-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-859"><a href="#cb17-859" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb17-860"><a href="#cb17-860" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-861"><a href="#cb17-861" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use SGD optimizer (standard for object detection)</span></span>
<span id="cb17-862"><a href="#cb17-862" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(</span>
<span id="cb17-863"><a href="#cb17-863" aria-hidden="true" tabindex="-1"></a>        model.parameters(),</span>
<span id="cb17-864"><a href="#cb17-864" aria-hidden="true" tabindex="-1"></a>        lr<span class="op">=</span>lr,</span>
<span id="cb17-865"><a href="#cb17-865" aria-hidden="true" tabindex="-1"></a>        momentum<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb17-866"><a href="#cb17-866" aria-hidden="true" tabindex="-1"></a>        weight_decay<span class="op">=</span><span class="fl">0.0005</span></span>
<span id="cb17-867"><a href="#cb17-867" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb17-868"><a href="#cb17-868" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-869"><a href="#cb17-869" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb17-870"><a href="#cb17-870" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-871"><a href="#cb17-871" aria-hidden="true" tabindex="-1"></a>        epoch_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb17-872"><a href="#cb17-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-873"><a href="#cb17-873" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_idx, (images, targets) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb17-874"><a href="#cb17-874" aria-hidden="true" tabindex="-1"></a>            <span class="cf">try</span>:</span>
<span id="cb17-875"><a href="#cb17-875" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Move data to device</span></span>
<span id="cb17-876"><a href="#cb17-876" aria-hidden="true" tabindex="-1"></a>                images <span class="op">=</span> [img.to(device) <span class="cf">for</span> img <span class="kw">in</span> images]</span>
<span id="cb17-877"><a href="#cb17-877" aria-hidden="true" tabindex="-1"></a>                targets <span class="op">=</span> [{k: v.to(device) <span class="cf">for</span> k, v <span class="kw">in</span> t.items()} <span class="cf">for</span> t <span class="kw">in</span> targets]</span>
<span id="cb17-878"><a href="#cb17-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-879"><a href="#cb17-879" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Forward pass - model returns loss dict</span></span>
<span id="cb17-880"><a href="#cb17-880" aria-hidden="true" tabindex="-1"></a>                loss_dict <span class="op">=</span> model(images, targets)</span>
<span id="cb17-881"><a href="#cb17-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-882"><a href="#cb17-882" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Sum all losses</span></span>
<span id="cb17-883"><a href="#cb17-883" aria-hidden="true" tabindex="-1"></a>                losses <span class="op">=</span> <span class="bu">sum</span>(loss <span class="cf">for</span> loss <span class="kw">in</span> loss_dict.values())</span>
<span id="cb17-884"><a href="#cb17-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-885"><a href="#cb17-885" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Backward pass</span></span>
<span id="cb17-886"><a href="#cb17-886" aria-hidden="true" tabindex="-1"></a>                optimizer.zero_grad()</span>
<span id="cb17-887"><a href="#cb17-887" aria-hidden="true" tabindex="-1"></a>                losses.backward()</span>
<span id="cb17-888"><a href="#cb17-888" aria-hidden="true" tabindex="-1"></a>                optimizer.step()</span>
<span id="cb17-889"><a href="#cb17-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-890"><a href="#cb17-890" aria-hidden="true" tabindex="-1"></a>                epoch_loss <span class="op">+=</span> losses.item()</span>
<span id="cb17-891"><a href="#cb17-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-892"><a href="#cb17-892" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Print progress every 50 batches</span></span>
<span id="cb17-893"><a href="#cb17-893" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> batch_idx <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-894"><a href="#cb17-894" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"  Batch </span><span class="sc">{</span>batch_idx<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span><span class="bu">len</span>(train_loader)<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>losses<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb17-895"><a href="#cb17-895" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"    RPN: cls=</span><span class="sc">{</span>loss_dict<span class="sc">.</span>get(<span class="st">'loss_objectness'</span>, <span class="dv">0</span>)<span class="sc">:.3f}</span><span class="ss">, "</span></span>
<span id="cb17-896"><a href="#cb17-896" aria-hidden="true" tabindex="-1"></a>                          <span class="ss">f"bbox=</span><span class="sc">{</span>loss_dict<span class="sc">.</span>get(<span class="st">'loss_rpn_box_reg'</span>, <span class="dv">0</span>)<span class="sc">:.3f}</span><span class="ss">, "</span></span>
<span id="cb17-897"><a href="#cb17-897" aria-hidden="true" tabindex="-1"></a>                          <span class="ss">f"ROI: cls=</span><span class="sc">{</span>loss_dict<span class="sc">.</span>get(<span class="st">'loss_classifier'</span>, <span class="dv">0</span>)<span class="sc">:.3f}</span><span class="ss">, "</span></span>
<span id="cb17-898"><a href="#cb17-898" aria-hidden="true" tabindex="-1"></a>                          <span class="ss">f"bbox=</span><span class="sc">{</span>loss_dict<span class="sc">.</span>get(<span class="st">'loss_box_reg'</span>, <span class="dv">0</span>)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb17-899"><a href="#cb17-899" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-900"><a href="#cb17-900" aria-hidden="true" tabindex="-1"></a>            <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb17-901"><a href="#cb17-901" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="st">"MPS"</span> <span class="kw">in</span> <span class="bu">str</span>(e) <span class="kw">or</span> <span class="st">"mps"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb17-902"><a href="#cb17-902" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">MPS compatibility issue detected: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-903"><a href="#cb17-903" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">print</span>(<span class="st">"Switching to CPU for remaining training..."</span>)</span>
<span id="cb17-904"><a href="#cb17-904" aria-hidden="true" tabindex="-1"></a>                    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-905"><a href="#cb17-905" aria-hidden="true" tabindex="-1"></a>                    model <span class="op">=</span> model.to(device)</span>
<span id="cb17-906"><a href="#cb17-906" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Retry this batch on CPU</span></span>
<span id="cb17-907"><a href="#cb17-907" aria-hidden="true" tabindex="-1"></a>                    images <span class="op">=</span> [img.to(device) <span class="cf">for</span> img <span class="kw">in</span> images]</span>
<span id="cb17-908"><a href="#cb17-908" aria-hidden="true" tabindex="-1"></a>                    targets <span class="op">=</span> [{k: v.to(device) <span class="cf">for</span> k, v <span class="kw">in</span> t.items()} <span class="cf">for</span> t <span class="kw">in</span> targets]</span>
<span id="cb17-909"><a href="#cb17-909" aria-hidden="true" tabindex="-1"></a>                    loss_dict <span class="op">=</span> model(images, targets)</span>
<span id="cb17-910"><a href="#cb17-910" aria-hidden="true" tabindex="-1"></a>                    losses <span class="op">=</span> <span class="bu">sum</span>(loss <span class="cf">for</span> loss <span class="kw">in</span> loss_dict.values())</span>
<span id="cb17-911"><a href="#cb17-911" aria-hidden="true" tabindex="-1"></a>                    optimizer.zero_grad()</span>
<span id="cb17-912"><a href="#cb17-912" aria-hidden="true" tabindex="-1"></a>                    losses.backward()</span>
<span id="cb17-913"><a href="#cb17-913" aria-hidden="true" tabindex="-1"></a>                    optimizer.step()</span>
<span id="cb17-914"><a href="#cb17-914" aria-hidden="true" tabindex="-1"></a>                    epoch_loss <span class="op">+=</span> losses.item()</span>
<span id="cb17-915"><a href="#cb17-915" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb17-916"><a href="#cb17-916" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">raise</span></span>
<span id="cb17-917"><a href="#cb17-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-918"><a href="#cb17-918" aria-hidden="true" tabindex="-1"></a>        avg_loss <span class="op">=</span> epoch_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb17-919"><a href="#cb17-919" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"‚úì Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> completed - Avg Loss: </span><span class="sc">{</span>avg_loss<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb17-920"><a href="#cb17-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-921"><a href="#cb17-921" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb17-922"><a href="#cb17-922" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-923"><a href="#cb17-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-926"><a href="#cb17-926" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-927"><a href="#cb17-927" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb17-928"><a href="#cb17-928" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Start with 2 epochs for quick demo. For production:</span></span>
<span id="cb17-929"><a href="#cb17-929" aria-hidden="true" tabindex="-1"></a><span class="co"># - Use 20-50 epochs for convergence</span></span>
<span id="cb17-930"><a href="#cb17-930" aria-hidden="true" tabindex="-1"></a><span class="co"># - Monitor validation loss to avoid overfitting</span></span>
<span id="cb17-931"><a href="#cb17-931" aria-hidden="true" tabindex="-1"></a><span class="co"># - Consider learning rate scheduling (e.g., reduce on plateau)</span></span>
<span id="cb17-932"><a href="#cb17-932" aria-hidden="true" tabindex="-1"></a>trained_model <span class="op">=</span> train_detection_model(</span>
<span id="cb17-933"><a href="#cb17-933" aria-hidden="true" tabindex="-1"></a>    detection_model,</span>
<span id="cb17-934"><a href="#cb17-934" aria-hidden="true" tabindex="-1"></a>    train_loader,</span>
<span id="cb17-935"><a href="#cb17-935" aria-hidden="true" tabindex="-1"></a>    val_loader,</span>
<span id="cb17-936"><a href="#cb17-936" aria-hidden="true" tabindex="-1"></a>    num_epochs<span class="op">=</span><span class="dv">2</span>,  <span class="co"># Increase to 20-50 for better results</span></span>
<span id="cb17-937"><a href="#cb17-937" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">0.0001</span></span>
<span id="cb17-938"><a href="#cb17-938" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-939"><a href="#cb17-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-940"><a href="#cb17-940" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure transform is configured correctly for inference</span></span>
<span id="cb17-941"><a href="#cb17-941" aria-hidden="true" tabindex="-1"></a>trained_model.transform.min_size <span class="op">=</span> (<span class="dv">512</span>,)</span>
<span id="cb17-942"><a href="#cb17-942" aria-hidden="true" tabindex="-1"></a>trained_model.transform.max_size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb17-943"><a href="#cb17-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-944"><a href="#cb17-944" aria-hidden="true" tabindex="-1"></a><span class="co"># Lower score threshold for inference (default 0.05 is too high for early training)</span></span>
<span id="cb17-945"><a href="#cb17-945" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: Some models store these in box_predictor or have different structures</span></span>
<span id="cb17-946"><a href="#cb17-946" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(trained_model.roi_heads, <span class="st">'score_thresh'</span>):</span>
<span id="cb17-947"><a href="#cb17-947" aria-hidden="true" tabindex="-1"></a>    trained_model.roi_heads.score_thresh <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb17-948"><a href="#cb17-948" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(trained_model.roi_heads, <span class="st">'nms_thresh'</span>):</span>
<span id="cb17-949"><a href="#cb17-949" aria-hidden="true" tabindex="-1"></a>    trained_model.roi_heads.nms_thresh <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb17-950"><a href="#cb17-950" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">hasattr</span>(trained_model.roi_heads, <span class="st">'detections_per_img'</span>):</span>
<span id="cb17-951"><a href="#cb17-951" aria-hidden="true" tabindex="-1"></a>    trained_model.roi_heads.detections_per_img <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb17-952"><a href="#cb17-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-953"><a href="#cb17-953" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ROI heads attributes: </span><span class="sc">{</span>[attr <span class="cf">for</span> attr <span class="kw">in</span> <span class="bu">dir</span>(trained_model.roi_heads) <span class="cf">if</span> <span class="st">'thresh'</span> <span class="kw">in</span> attr.lower() <span class="kw">or</span> <span class="st">'detections'</span> <span class="kw">in</span> attr.lower()]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-954"><a href="#cb17-954" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Configured thresholds: score=</span><span class="sc">{</span><span class="bu">getattr</span>(trained_model.roi_heads, <span class="st">'score_thresh'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">, nms=</span><span class="sc">{</span><span class="bu">getattr</span>(trained_model.roi_heads, <span class="st">'nms_thresh'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-955"><a href="#cb17-955" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-956"><a href="#cb17-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-957"><a href="#cb17-957" aria-hidden="true" tabindex="-1"></a>##Evaluate the Trained Model</span>
<span id="cb17-958"><a href="#cb17-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-959"><a href="#cb17-959" aria-hidden="true" tabindex="-1"></a>Let's see how the model performs on validation data:</span>
<span id="cb17-960"><a href="#cb17-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-963"><a href="#cb17-963" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-964"><a href="#cb17-964" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-965"><a href="#cb17-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-966"><a href="#cb17-966" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_detection_model(model, val_loader, num_samples<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb17-967"><a href="#cb17-967" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate object detection model on validation set."""</span></span>
<span id="cb17-968"><a href="#cb17-968" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use same device detection as training</span></span>
<span id="cb17-969"><a href="#cb17-969" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb17-970"><a href="#cb17-970" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb17-971"><a href="#cb17-971" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb17-972"><a href="#cb17-972" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb17-973"><a href="#cb17-973" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb17-974"><a href="#cb17-974" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-975"><a href="#cb17-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-976"><a href="#cb17-976" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb17-977"><a href="#cb17-977" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb17-978"><a href="#cb17-978" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-979"><a href="#cb17-979" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Failed to move model to </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-980"><a href="#cb17-980" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Falling back to CPU for evaluation"</span>)</span>
<span id="cb17-981"><a href="#cb17-981" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-982"><a href="#cb17-982" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb17-983"><a href="#cb17-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-984"><a href="#cb17-984" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb17-985"><a href="#cb17-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-986"><a href="#cb17-986" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Running inference on validation samples...</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb17-987"><a href="#cb17-987" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-988"><a href="#cb17-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-989"><a href="#cb17-989" aria-hidden="true" tabindex="-1"></a>    sample_images, sample_targets <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(val_loader))</span>
<span id="cb17-990"><a href="#cb17-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-991"><a href="#cb17-991" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb17-992"><a href="#cb17-992" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> [img.to(device) <span class="cf">for</span> img <span class="kw">in</span> sample_images]</span>
<span id="cb17-993"><a href="#cb17-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-994"><a href="#cb17-994" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-995"><a href="#cb17-995" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> model(images)</span>
<span id="cb17-996"><a href="#cb17-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-997"><a href="#cb17-997" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb17-998"><a href="#cb17-998" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">"MPS"</span> <span class="kw">in</span> <span class="bu">str</span>(e) <span class="kw">or</span> <span class="st">"mps"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb17-999"><a href="#cb17-999" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">MPS compatibility issue during inference: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-1000"><a href="#cb17-1000" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Switching to CPU for evaluation..."</span>)</span>
<span id="cb17-1001"><a href="#cb17-1001" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-1002"><a href="#cb17-1002" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> model.to(device)</span>
<span id="cb17-1003"><a href="#cb17-1003" aria-hidden="true" tabindex="-1"></a>            images <span class="op">=</span> [img.to(device) <span class="cf">for</span> img <span class="kw">in</span> sample_images]</span>
<span id="cb17-1004"><a href="#cb17-1004" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-1005"><a href="#cb17-1005" aria-hidden="true" tabindex="-1"></a>                predictions <span class="op">=</span> model(images)</span>
<span id="cb17-1006"><a href="#cb17-1006" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb17-1007"><a href="#cb17-1007" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span></span>
<span id="cb17-1008"><a href="#cb17-1008" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-1009"><a href="#cb17-1009" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Display results</span></span>
<span id="cb17-1010"><a href="#cb17-1010" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">min</span>(<span class="bu">len</span>(predictions), num_samples)):</span>
<span id="cb17-1011"><a href="#cb17-1011" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> predictions[i]</span>
<span id="cb17-1012"><a href="#cb17-1012" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> sample_targets[i]</span>
<span id="cb17-1013"><a href="#cb17-1013" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-1014"><a href="#cb17-1014" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Sample </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb17-1015"><a href="#cb17-1015" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Ground truth: </span><span class="sc">{</span><span class="bu">len</span>(target[<span class="st">'boxes'</span>])<span class="sc">}</span><span class="ss"> objects"</span>)</span>
<span id="cb17-1016"><a href="#cb17-1016" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  Predicted: </span><span class="sc">{</span><span class="bu">len</span>(pred[<span class="st">'boxes'</span>])<span class="sc">}</span><span class="ss"> detections"</span>)</span>
<span id="cb17-1017"><a href="#cb17-1017" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-1018"><a href="#cb17-1018" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Show top 3 predictions</span></span>
<span id="cb17-1019"><a href="#cb17-1019" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(pred[<span class="st">'boxes'</span>]) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb17-1020"><a href="#cb17-1020" aria-hidden="true" tabindex="-1"></a>            top_indices <span class="op">=</span> pred[<span class="st">'scores'</span>].argsort(descending<span class="op">=</span><span class="va">True</span>)[:<span class="dv">3</span>]</span>
<span id="cb17-1021"><a href="#cb17-1021" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Top 3 detections:"</span>)</span>
<span id="cb17-1022"><a href="#cb17-1022" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx <span class="kw">in</span> top_indices:</span>
<span id="cb17-1023"><a href="#cb17-1023" aria-hidden="true" tabindex="-1"></a>                score <span class="op">=</span> pred[<span class="st">'scores'</span>][idx].item()</span>
<span id="cb17-1024"><a href="#cb17-1024" aria-hidden="true" tabindex="-1"></a>                label <span class="op">=</span> pred[<span class="st">'labels'</span>][idx].item()</span>
<span id="cb17-1025"><a href="#cb17-1025" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"    - Class </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">, confidence: </span><span class="sc">{</span>score<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb17-1026"><a href="#cb17-1026" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span>
<span id="cb17-1027"><a href="#cb17-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1028"><a href="#cb17-1028" aria-hidden="true" tabindex="-1"></a>evaluate_detection_model(trained_model, val_loader, num_samples<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb17-1029"><a href="#cb17-1029" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-1030"><a href="#cb17-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1031"><a href="#cb17-1031" aria-hidden="true" tabindex="-1"></a><span class="fu">## Debug Model Predictions</span></span>
<span id="cb17-1032"><a href="#cb17-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1033"><a href="#cb17-1033" aria-hidden="true" tabindex="-1"></a>Before visualizing, let's check if the model is generating any predictions at all:</span>
<span id="cb17-1034"><a href="#cb17-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1037"><a href="#cb17-1037" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-1038"><a href="#cb17-1038" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-1039"><a href="#cb17-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1040"><a href="#cb17-1040" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> debug_model_predictions(model, dataset, sample_idx<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb17-1041"><a href="#cb17-1041" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Debug what the model is predicting with detailed introspection."""</span></span>
<span id="cb17-1042"><a href="#cb17-1042" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb17-1043"><a href="#cb17-1043" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb17-1044"><a href="#cb17-1044" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb17-1045"><a href="#cb17-1045" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb17-1046"><a href="#cb17-1046" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb17-1047"><a href="#cb17-1047" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-1048"><a href="#cb17-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1049"><a href="#cb17-1049" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb17-1050"><a href="#cb17-1050" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb17-1051"><a href="#cb17-1051" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-1052"><a href="#cb17-1052" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-1053"><a href="#cb17-1053" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb17-1054"><a href="#cb17-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1055"><a href="#cb17-1055" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb17-1056"><a href="#cb17-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1057"><a href="#cb17-1057" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get sample</span></span>
<span id="cb17-1058"><a href="#cb17-1058" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> dataset[sample_idx]</span>
<span id="cb17-1059"><a href="#cb17-1059" aria-hidden="true" tabindex="-1"></a>    batch <span class="op">=</span> detection_collate_fn([sample])</span>
<span id="cb17-1060"><a href="#cb17-1060" aria-hidden="true" tabindex="-1"></a>    images, targets <span class="op">=</span> batch</span>
<span id="cb17-1061"><a href="#cb17-1061" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> images[<span class="dv">0</span>].to(device)</span>
<span id="cb17-1062"><a href="#cb17-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1063"><a href="#cb17-1063" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sample </span><span class="sc">{</span>sample_idx<span class="sc">}</span><span class="ss"> Debug Info:"</span>)</span>
<span id="cb17-1064"><a href="#cb17-1064" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Ground truth: </span><span class="sc">{</span><span class="bu">len</span>(targets[<span class="dv">0</span>][<span class="st">'boxes'</span>])<span class="sc">}</span><span class="ss"> objects"</span>)</span>
<span id="cb17-1065"><a href="#cb17-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1066"><a href="#cb17-1066" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check current threshold settings</span></span>
<span id="cb17-1067"><a href="#cb17-1067" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Model threshold settings:"</span>)</span>
<span id="cb17-1068"><a href="#cb17-1068" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    score_thresh: </span><span class="sc">{</span><span class="bu">getattr</span>(model.roi_heads, <span class="st">'score_thresh'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-1069"><a href="#cb17-1069" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    nms_thresh: </span><span class="sc">{</span><span class="bu">getattr</span>(model.roi_heads, <span class="st">'nms_thresh'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-1070"><a href="#cb17-1070" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    detections_per_img: </span><span class="sc">{</span><span class="bu">getattr</span>(model.roi_heads, <span class="st">'detections_per_img'</span>, <span class="st">'N/A'</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-1071"><a href="#cb17-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1072"><a href="#cb17-1072" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Try with extremely low threshold</span></span>
<span id="cb17-1073"><a href="#cb17-1073" aria-hidden="true" tabindex="-1"></a>    original_score_thresh <span class="op">=</span> model.roi_heads.score_thresh</span>
<span id="cb17-1074"><a href="#cb17-1074" aria-hidden="true" tabindex="-1"></a>    model.roi_heads.score_thresh <span class="op">=</span> <span class="fl">0.0001</span>  <span class="co"># Extremely low</span></span>
<span id="cb17-1075"><a href="#cb17-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1076"><a href="#cb17-1076" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-1077"><a href="#cb17-1077" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> model([img])</span>
<span id="cb17-1078"><a href="#cb17-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1079"><a href="#cb17-1079" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> predictions[<span class="dv">0</span>]</span>
<span id="cb17-1080"><a href="#cb17-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1081"><a href="#cb17-1081" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Model output (with score_thresh=0.0001):"</span>)</span>
<span id="cb17-1082"><a href="#cb17-1082" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    Total predictions after NMS: </span><span class="sc">{</span><span class="bu">len</span>(pred[<span class="st">'boxes'</span>])<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-1083"><a href="#cb17-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1084"><a href="#cb17-1084" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(pred[<span class="st">'boxes'</span>]) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-1085"><a href="#cb17-1085" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"  ‚ö†Ô∏è  STILL no predictions! This suggests:"</span>)</span>
<span id="cb17-1086"><a href="#cb17-1086" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"     1. ROI head is not producing any boxes with positive scores"</span>)</span>
<span id="cb17-1087"><a href="#cb17-1087" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"     2. All boxes might be getting filtered before score threshold"</span>)</span>
<span id="cb17-1088"><a href="#cb17-1088" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"     3. Check if box_predictor is outputting valid scores"</span>)</span>
<span id="cb17-1089"><a href="#cb17-1089" aria-hidden="true" tabindex="-1"></a>        model.roi_heads.score_thresh <span class="op">=</span> original_score_thresh</span>
<span id="cb17-1090"><a href="#cb17-1090" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb17-1091"><a href="#cb17-1091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1092"><a href="#cb17-1092" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"    Score range: [</span><span class="sc">{</span>pred[<span class="st">'scores'</span>]<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.6f}</span><span class="ss">, </span><span class="sc">{</span>pred[<span class="st">'scores'</span>]<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.6f}</span><span class="ss">]"</span>)</span>
<span id="cb17-1093"><a href="#cb17-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1094"><a href="#cb17-1094" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Show score distribution</span></span>
<span id="cb17-1095"><a href="#cb17-1095" aria-hidden="true" tabindex="-1"></a>    thresholds <span class="op">=</span> [<span class="fl">0.0001</span>, <span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>, <span class="fl">0.5</span>]</span>
<span id="cb17-1096"><a href="#cb17-1096" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Predictions by confidence threshold:"</span>)</span>
<span id="cb17-1097"><a href="#cb17-1097" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> thresh <span class="kw">in</span> thresholds:</span>
<span id="cb17-1098"><a href="#cb17-1098" aria-hidden="true" tabindex="-1"></a>        count <span class="op">=</span> (pred[<span class="st">'scores'</span>] <span class="op">&gt;</span> thresh).<span class="bu">sum</span>().item()</span>
<span id="cb17-1099"><a href="#cb17-1099" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"    &gt;</span><span class="sc">{</span>thresh<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>count<span class="sc">}</span><span class="ss"> detections"</span>)</span>
<span id="cb17-1100"><a href="#cb17-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1101"><a href="#cb17-1101" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">  Top 10 predictions:"</span>)</span>
<span id="cb17-1102"><a href="#cb17-1102" aria-hidden="true" tabindex="-1"></a>    top_indices <span class="op">=</span> pred[<span class="st">'scores'</span>].argsort(descending<span class="op">=</span><span class="va">True</span>)[:<span class="dv">10</span>]</span>
<span id="cb17-1103"><a href="#cb17-1103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, idx <span class="kw">in</span> <span class="bu">enumerate</span>(top_indices):</span>
<span id="cb17-1104"><a href="#cb17-1104" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> pred[<span class="st">'scores'</span>][idx].item()</span>
<span id="cb17-1105"><a href="#cb17-1105" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> pred[<span class="st">'labels'</span>][idx].item()</span>
<span id="cb17-1106"><a href="#cb17-1106" aria-hidden="true" tabindex="-1"></a>        box <span class="op">=</span> pred[<span class="st">'boxes'</span>][idx].cpu().numpy()</span>
<span id="cb17-1107"><a href="#cb17-1107" aria-hidden="true" tabindex="-1"></a>        class_name <span class="op">=</span> DIOR_CLASSES[label] <span class="cf">if</span> label <span class="op">&lt;</span> <span class="bu">len</span>(DIOR_CLASSES) <span class="cf">else</span> <span class="ss">f"Class</span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb17-1108"><a href="#cb17-1108" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"    </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">. </span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss">, score=</span><span class="sc">{</span>score<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb17-1109"><a href="#cb17-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1110"><a href="#cb17-1110" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Restore original threshold</span></span>
<span id="cb17-1111"><a href="#cb17-1111" aria-hidden="true" tabindex="-1"></a>    model.roi_heads.score_thresh <span class="op">=</span> original_score_thresh</span>
<span id="cb17-1112"><a href="#cb17-1112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-1113"><a href="#cb17-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1116"><a href="#cb17-1116" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-1117"><a href="#cb17-1117" aria-hidden="true" tabindex="-1"></a>debug_model_predictions(trained_model, dataset_train, sample_idx<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-1118"><a href="#cb17-1118" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-1119"><a href="#cb17-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1120"><a href="#cb17-1120" aria-hidden="true" tabindex="-1"></a><span class="fu">## Visualize Predictions vs Ground Truth</span></span>
<span id="cb17-1121"><a href="#cb17-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1122"><a href="#cb17-1122" aria-hidden="true" tabindex="-1"></a>Let's compare the model's predictions against the actual labels for some of the images we explored earlier:</span>
<span id="cb17-1123"><a href="#cb17-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1126"><a href="#cb17-1126" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-1127"><a href="#cb17-1127" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-1128"><a href="#cb17-1128" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.patches <span class="im">as</span> patches</span>
<span id="cb17-1129"><a href="#cb17-1129" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-1130"><a href="#cb17-1130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1131"><a href="#cb17-1131" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_predictions_vs_truth(model, dataset, indices<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>], confidence_threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb17-1132"><a href="#cb17-1132" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb17-1133"><a href="#cb17-1133" aria-hidden="true" tabindex="-1"></a><span class="co">    Compare model predictions with ground truth annotations.</span></span>
<span id="cb17-1134"><a href="#cb17-1134" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb17-1135"><a href="#cb17-1135" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb17-1136"><a href="#cb17-1136" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)</span>
<span id="cb17-1137"><a href="#cb17-1137" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb17-1138"><a href="#cb17-1138" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb17-1139"><a href="#cb17-1139" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb17-1140"><a href="#cb17-1140" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-1141"><a href="#cb17-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1142"><a href="#cb17-1142" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb17-1143"><a href="#cb17-1143" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb17-1144"><a href="#cb17-1144" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-1145"><a href="#cb17-1145" aria-hidden="true" tabindex="-1"></a>        device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-1146"><a href="#cb17-1146" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> model.to(device)</span>
<span id="cb17-1147"><a href="#cb17-1147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1148"><a href="#cb17-1148" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb17-1149"><a href="#cb17-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1150"><a href="#cb17-1150" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="bu">len</span>(indices), <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">8</span> <span class="op">*</span> <span class="bu">len</span>(indices)))</span>
<span id="cb17-1151"><a href="#cb17-1151" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(indices) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb17-1152"><a href="#cb17-1152" aria-hidden="true" tabindex="-1"></a>        axes <span class="op">=</span> axes.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb17-1153"><a href="#cb17-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1154"><a href="#cb17-1154" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> plot_idx, data_idx <span class="kw">in</span> <span class="bu">enumerate</span>(indices):</span>
<span id="cb17-1155"><a href="#cb17-1155" aria-hidden="true" tabindex="-1"></a>        sample <span class="op">=</span> dataset[data_idx]</span>
<span id="cb17-1156"><a href="#cb17-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1157"><a href="#cb17-1157" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use collate function to process image (same as training)</span></span>
<span id="cb17-1158"><a href="#cb17-1158" aria-hidden="true" tabindex="-1"></a>        batch <span class="op">=</span> detection_collate_fn([sample])</span>
<span id="cb17-1159"><a href="#cb17-1159" aria-hidden="true" tabindex="-1"></a>        images, targets <span class="op">=</span> batch</span>
<span id="cb17-1160"><a href="#cb17-1160" aria-hidden="true" tabindex="-1"></a>        img_resized <span class="op">=</span> images[<span class="dv">0</span>]</span>
<span id="cb17-1161"><a href="#cb17-1161" aria-hidden="true" tabindex="-1"></a>        boxes_scaled <span class="op">=</span> targets[<span class="dv">0</span>][<span class="st">'boxes'</span>]</span>
<span id="cb17-1162"><a href="#cb17-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1163"><a href="#cb17-1163" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run inference</span></span>
<span id="cb17-1164"><a href="#cb17-1164" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb17-1165"><a href="#cb17-1165" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-1166"><a href="#cb17-1166" aria-hidden="true" tabindex="-1"></a>                predictions <span class="op">=</span> model([img_resized.to(device)])</span>
<span id="cb17-1167"><a href="#cb17-1167" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb17-1168"><a href="#cb17-1168" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">"MPS"</span> <span class="kw">in</span> <span class="bu">str</span>(e) <span class="kw">or</span> <span class="st">"mps"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb17-1169"><a href="#cb17-1169" aria-hidden="true" tabindex="-1"></a>                device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb17-1170"><a href="#cb17-1170" aria-hidden="true" tabindex="-1"></a>                model <span class="op">=</span> model.to(device)</span>
<span id="cb17-1171"><a href="#cb17-1171" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-1172"><a href="#cb17-1172" aria-hidden="true" tabindex="-1"></a>                    predictions <span class="op">=</span> model([img_resized.to(device)])</span>
<span id="cb17-1173"><a href="#cb17-1173" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb17-1174"><a href="#cb17-1174" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span></span>
<span id="cb17-1175"><a href="#cb17-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1176"><a href="#cb17-1176" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> predictions[<span class="dv">0</span>]</span>
<span id="cb17-1177"><a href="#cb17-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1178"><a href="#cb17-1178" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Filter by confidence</span></span>
<span id="cb17-1179"><a href="#cb17-1179" aria-hidden="true" tabindex="-1"></a>        keep <span class="op">=</span> pred[<span class="st">'scores'</span>] <span class="op">&gt;</span> confidence_threshold</span>
<span id="cb17-1180"><a href="#cb17-1180" aria-hidden="true" tabindex="-1"></a>        pred_boxes <span class="op">=</span> pred[<span class="st">'boxes'</span>][keep].cpu()</span>
<span id="cb17-1181"><a href="#cb17-1181" aria-hidden="true" tabindex="-1"></a>        pred_labels <span class="op">=</span> pred[<span class="st">'labels'</span>][keep].cpu()</span>
<span id="cb17-1182"><a href="#cb17-1182" aria-hidden="true" tabindex="-1"></a>        pred_scores <span class="op">=</span> pred[<span class="st">'scores'</span>][keep].cpu()</span>
<span id="cb17-1183"><a href="#cb17-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1184"><a href="#cb17-1184" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image back to 512x512 for visualization</span></span>
<span id="cb17-1185"><a href="#cb17-1185" aria-hidden="true" tabindex="-1"></a>        img_display <span class="op">=</span> img_resized.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy()</span>
<span id="cb17-1186"><a href="#cb17-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1187"><a href="#cb17-1187" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot ground truth</span></span>
<span id="cb17-1188"><a href="#cb17-1188" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> axes[plot_idx, <span class="dv">0</span>]</span>
<span id="cb17-1189"><a href="#cb17-1189" aria-hidden="true" tabindex="-1"></a>        ax.imshow(img_display)</span>
<span id="cb17-1190"><a href="#cb17-1190" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> box, label <span class="kw">in</span> <span class="bu">zip</span>(boxes_scaled, sample[<span class="st">'label'</span>]):</span>
<span id="cb17-1191"><a href="#cb17-1191" aria-hidden="true" tabindex="-1"></a>            xmin, ymin, xmax, ymax <span class="op">=</span> box.numpy()</span>
<span id="cb17-1192"><a href="#cb17-1192" aria-hidden="true" tabindex="-1"></a>            width <span class="op">=</span> xmax <span class="op">-</span> xmin</span>
<span id="cb17-1193"><a href="#cb17-1193" aria-hidden="true" tabindex="-1"></a>            height <span class="op">=</span> ymax <span class="op">-</span> ymin</span>
<span id="cb17-1194"><a href="#cb17-1194" aria-hidden="true" tabindex="-1"></a>            rect <span class="op">=</span> patches.Rectangle(</span>
<span id="cb17-1195"><a href="#cb17-1195" aria-hidden="true" tabindex="-1"></a>                (xmin, ymin), width, height,</span>
<span id="cb17-1196"><a href="#cb17-1196" aria-hidden="true" tabindex="-1"></a>                linewidth<span class="op">=</span><span class="dv">2</span>, edgecolor<span class="op">=</span><span class="st">'lime'</span>, facecolor<span class="op">=</span><span class="st">'none'</span></span>
<span id="cb17-1197"><a href="#cb17-1197" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-1198"><a href="#cb17-1198" aria-hidden="true" tabindex="-1"></a>            ax.add_patch(rect)</span>
<span id="cb17-1199"><a href="#cb17-1199" aria-hidden="true" tabindex="-1"></a>            class_name <span class="op">=</span> DIOR_CLASSES[<span class="bu">int</span>(label)]</span>
<span id="cb17-1200"><a href="#cb17-1200" aria-hidden="true" tabindex="-1"></a>            ax.text(</span>
<span id="cb17-1201"><a href="#cb17-1201" aria-hidden="true" tabindex="-1"></a>                xmin, ymin <span class="op">-</span> <span class="dv">5</span>, class_name,</span>
<span id="cb17-1202"><a href="#cb17-1202" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>, weight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb17-1203"><a href="#cb17-1203" aria-hidden="true" tabindex="-1"></a>                bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'lime'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb17-1204"><a href="#cb17-1204" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-1205"><a href="#cb17-1205" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb17-1206"><a href="#cb17-1206" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"Ground Truth (Sample </span><span class="sc">{</span>data_idx<span class="sc">}</span><span class="ss">): </span><span class="sc">{</span><span class="bu">len</span>(sample[<span class="st">'label'</span>])<span class="sc">}</span><span class="ss"> objects"</span>,</span>
<span id="cb17-1207"><a href="#cb17-1207" aria-hidden="true" tabindex="-1"></a>                     fontsize<span class="op">=</span><span class="dv">12</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-1208"><a href="#cb17-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1209"><a href="#cb17-1209" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Plot predictions</span></span>
<span id="cb17-1210"><a href="#cb17-1210" aria-hidden="true" tabindex="-1"></a>        ax <span class="op">=</span> axes[plot_idx, <span class="dv">1</span>]</span>
<span id="cb17-1211"><a href="#cb17-1211" aria-hidden="true" tabindex="-1"></a>        ax.imshow(img_display)</span>
<span id="cb17-1212"><a href="#cb17-1212" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> box, label, score <span class="kw">in</span> <span class="bu">zip</span>(pred_boxes, pred_labels, pred_scores):</span>
<span id="cb17-1213"><a href="#cb17-1213" aria-hidden="true" tabindex="-1"></a>            xmin, ymin, xmax, ymax <span class="op">=</span> box.numpy()</span>
<span id="cb17-1214"><a href="#cb17-1214" aria-hidden="true" tabindex="-1"></a>            width <span class="op">=</span> xmax <span class="op">-</span> xmin</span>
<span id="cb17-1215"><a href="#cb17-1215" aria-hidden="true" tabindex="-1"></a>            height <span class="op">=</span> ymax <span class="op">-</span> ymin</span>
<span id="cb17-1216"><a href="#cb17-1216" aria-hidden="true" tabindex="-1"></a>            rect <span class="op">=</span> patches.Rectangle(</span>
<span id="cb17-1217"><a href="#cb17-1217" aria-hidden="true" tabindex="-1"></a>                (xmin, ymin), width, height,</span>
<span id="cb17-1218"><a href="#cb17-1218" aria-hidden="true" tabindex="-1"></a>                linewidth<span class="op">=</span><span class="dv">2</span>, edgecolor<span class="op">=</span><span class="st">'red'</span>, facecolor<span class="op">=</span><span class="st">'none'</span></span>
<span id="cb17-1219"><a href="#cb17-1219" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-1220"><a href="#cb17-1220" aria-hidden="true" tabindex="-1"></a>            ax.add_patch(rect)</span>
<span id="cb17-1221"><a href="#cb17-1221" aria-hidden="true" tabindex="-1"></a>            class_name <span class="op">=</span> DIOR_CLASSES[<span class="bu">int</span>(label)]</span>
<span id="cb17-1222"><a href="#cb17-1222" aria-hidden="true" tabindex="-1"></a>            ax.text(</span>
<span id="cb17-1223"><a href="#cb17-1223" aria-hidden="true" tabindex="-1"></a>                xmin, ymin <span class="op">-</span> <span class="dv">5</span>, <span class="ss">f"</span><span class="sc">{</span>class_name<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>score<span class="sc">:.2f}</span><span class="ss">)"</span>,</span>
<span id="cb17-1224"><a href="#cb17-1224" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">'white'</span>, fontsize<span class="op">=</span><span class="dv">8</span>, weight<span class="op">=</span><span class="st">'bold'</span>,</span>
<span id="cb17-1225"><a href="#cb17-1225" aria-hidden="true" tabindex="-1"></a>                bbox<span class="op">=</span><span class="bu">dict</span>(boxstyle<span class="op">=</span><span class="st">'round'</span>, facecolor<span class="op">=</span><span class="st">'red'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb17-1226"><a href="#cb17-1226" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb17-1227"><a href="#cb17-1227" aria-hidden="true" tabindex="-1"></a>        ax.axis(<span class="st">'off'</span>)</span>
<span id="cb17-1228"><a href="#cb17-1228" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"Model Predictions: </span><span class="sc">{</span><span class="bu">len</span>(pred_boxes)<span class="sc">}</span><span class="ss"> detections (conf&gt;</span><span class="sc">{</span>confidence_threshold<span class="sc">}</span><span class="ss">)"</span>,</span>
<span id="cb17-1229"><a href="#cb17-1229" aria-hidden="true" tabindex="-1"></a>                     fontsize<span class="op">=</span><span class="dv">12</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-1230"><a href="#cb17-1230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1231"><a href="#cb17-1231" aria-hidden="true" tabindex="-1"></a>    plt.suptitle(<span class="st">"Object Detection: Ground Truth vs Predictions (512√ó512)"</span>,</span>
<span id="cb17-1232"><a href="#cb17-1232" aria-hidden="true" tabindex="-1"></a>                 fontsize<span class="op">=</span><span class="dv">16</span>, weight<span class="op">=</span><span class="st">'bold'</span>)</span>
<span id="cb17-1233"><a href="#cb17-1233" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb17-1234"><a href="#cb17-1234" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb17-1235"><a href="#cb17-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1236"><a href="#cb17-1236" aria-hidden="true" tabindex="-1"></a>visualize_predictions_vs_truth(trained_model, dataset_train, indices<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>], confidence_threshold<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb17-1237"><a href="#cb17-1237" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-1238"><a href="#cb17-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1239"><a href="#cb17-1239" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb17-1240"><a href="#cb17-1240" aria-hidden="true" tabindex="-1"></a><span class="fu">## Understanding the Visualization</span></span>
<span id="cb17-1241"><a href="#cb17-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1242"><a href="#cb17-1242" aria-hidden="true" tabindex="-1"></a>**Left column (Green boxes)**: Ground truth annotations from the DIOR dataset</span>
<span id="cb17-1243"><a href="#cb17-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1244"><a href="#cb17-1244" aria-hidden="true" tabindex="-1"></a>**Right column (Red boxes)**: Model predictions with confidence scores</span>
<span id="cb17-1245"><a href="#cb17-1245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1246"><a href="#cb17-1246" aria-hidden="true" tabindex="-1"></a>**Training Status After 10 Epochs:**</span>
<span id="cb17-1247"><a href="#cb17-1247" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model is generating predictions but with low confidence (~0.004-0.005)</span>
<span id="cb17-1248"><a href="#cb17-1248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Currently showing detections with confidence &gt; 0.001 (very low threshold)</span>
<span id="cb17-1249"><a href="#cb17-1249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Typical object detection requires 20-50+ epochs to converge</span>
<span id="cb17-1250"><a href="#cb17-1250" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Expected behavior: confidence scores should increase to 0.5+ with more training</span>
<span id="cb17-1251"><a href="#cb17-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1252"><a href="#cb17-1252" aria-hidden="true" tabindex="-1"></a>**Why Low Confidence?**</span>
<span id="cb17-1253"><a href="#cb17-1253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Limited training data (1000 samples) and epochs (10)</span>
<span id="cb17-1254"><a href="#cb17-1254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Single-scale detection (no FPN) is challenging</span>
<span id="cb17-1255"><a href="#cb17-1255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ViT backbone requires careful tuning for detection tasks</span>
<span id="cb17-1256"><a href="#cb17-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1257"><a href="#cb17-1257" aria-hidden="true" tabindex="-1"></a>**To Improve Results:**</span>
<span id="cb17-1258"><a href="#cb17-1258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Train for 30-50 epochs with full dataset</span>
<span id="cb17-1259"><a href="#cb17-1259" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consider using learning rate scheduling</span>
<span id="cb17-1260"><a href="#cb17-1260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Experiment with different anchor sizes for your specific object scales</span>
<span id="cb17-1261"><a href="#cb17-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1262"><a href="#cb17-1262" aria-hidden="true" tabindex="-1"></a>Note that images are resized to 512√ó512 for visualization, matching the model's input size.</span>
<span id="cb17-1263"><a href="#cb17-1263" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb17-1264"><a href="#cb17-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1265"><a href="#cb17-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1266"><a href="#cb17-1266" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary and Key Takeaways</span></span>
<span id="cb17-1267"><a href="#cb17-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1268"><a href="#cb17-1268" aria-hidden="true" tabindex="-1"></a>Congratulations! You've successfully built and trained an object detection model. Here's what you accomplished:</span>
<span id="cb17-1269"><a href="#cb17-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1270"><a href="#cb17-1270" aria-hidden="true" tabindex="-1"></a><span class="fu">### ‚úÖ Complete Pipeline Built</span></span>
<span id="cb17-1271"><a href="#cb17-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1272"><a href="#cb17-1272" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Dataset Loading &amp; Exploration**</span>
<span id="cb17-1273"><a href="#cb17-1273" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Loaded DIOR object detection dataset from Hugging Face</span>
<span id="cb17-1274"><a href="#cb17-1274" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Converted from COCO format to PyTorch/torchvision format</span>
<span id="cb17-1275"><a href="#cb17-1275" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Explored bounding box format (COCO <span class="in">`[x,y,w,h]`</span> ‚Üí <span class="in">`[xmin,ymin,xmax,ymax]`</span>)</span>
<span id="cb17-1276"><a href="#cb17-1276" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Visualized 800√ó800 aerial images with bounding boxes</span>
<span id="cb17-1277"><a href="#cb17-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1278"><a href="#cb17-1278" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Model Architecture**</span>
<span id="cb17-1279"><a href="#cb17-1279" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Built complete detection pipeline: **Prithvi ViT ‚Üí Faster R-CNN** (single-scale)</span>
<span id="cb17-1280"><a href="#cb17-1280" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Used actual Geospatial Foundation Model (HLS satellite pretrained!)</span>
<span id="cb17-1281"><a href="#cb17-1281" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Fixed ViT‚Üídetection compatibility issues</span>
<span id="cb17-1282"><a href="#cb17-1282" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Understood the role of each component (backbone, neck, head)</span>
<span id="cb17-1283"><a href="#cb17-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1284"><a href="#cb17-1284" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Data Pipeline**</span>
<span id="cb17-1285"><a href="#cb17-1285" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Created train/val splits with <span class="in">`Subset`</span></span>
<span id="cb17-1286"><a href="#cb17-1286" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Implemented custom <span class="in">`collate_fn`</span> that resizes 800√ó800 ‚Üí 512√ó512</span>
<span id="cb17-1287"><a href="#cb17-1287" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Scaled bounding boxes proportionally (factor: 0.64)</span>
<span id="cb17-1288"><a href="#cb17-1288" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Formatted targets as list of dicts (torchvision standard)</span>
<span id="cb17-1289"><a href="#cb17-1289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1290"><a href="#cb17-1290" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Training &amp; Evaluation**</span>
<span id="cb17-1291"><a href="#cb17-1291" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Successfully trained detection model with decreasing losses</span>
<span id="cb17-1292"><a href="#cb17-1292" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Understood detection-specific training (internal loss computation)</span>
<span id="cb17-1293"><a href="#cb17-1293" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Monitored RPN and ROI head losses separately</span>
<span id="cb17-1294"><a href="#cb17-1294" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Learned about convergence time (20-50 epochs typical for object detection)</span>
<span id="cb17-1295"><a href="#cb17-1295" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Debugged prediction confidence scores and thresholds</span>
<span id="cb17-1296"><a href="#cb17-1296" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Visualized predictions vs ground truth with adjustable confidence thresholds</span>
<span id="cb17-1297"><a href="#cb17-1297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1298"><a href="#cb17-1298" aria-hidden="true" tabindex="-1"></a><span class="fu">### üéì Key Lessons</span></span>
<span id="cb17-1299"><a href="#cb17-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1300"><a href="#cb17-1300" aria-hidden="true" tabindex="-1"></a>**How We Made Prithvi ViT Work for Object Detection:**</span>
<span id="cb17-1301"><a href="#cb17-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1302"><a href="#cb17-1302" aria-hidden="true" tabindex="-1"></a>**The Core Challenge:**</span>
<span id="cb17-1303"><a href="#cb17-1303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Faster R-CNN expects multi-scale CNN features (FPN pyramid: multiple resolutions)</span>
<span id="cb17-1304"><a href="#cb17-1304" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prithvi ViT produces single-scale sequence features (1024 tokens of 768 dimensions)</span>
<span id="cb17-1305"><a href="#cb17-1305" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-scale components (RPN, ROI pooler) fail or filter incorrectly with single-scale input</span>
<span id="cb17-1306"><a href="#cb17-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1307"><a href="#cb17-1307" aria-hidden="true" tabindex="-1"></a>**Our Four-Part Solution:**</span>
<span id="cb17-1308"><a href="#cb17-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1309"><a href="#cb17-1309" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Skip FPN (neck=None)**</span>
<span id="cb17-1310"><a href="#cb17-1310" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>No feature pyramid network</span>
<span id="cb17-1311"><a href="#cb17-1311" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Work directly on ViT's single 32√ó32 feature map</span>
<span id="cb17-1312"><a href="#cb17-1312" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Avoids multi-scale expectations entirely</span>
<span id="cb17-1313"><a href="#cb17-1313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1314"><a href="#cb17-1314" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Custom RPN with Single-Scale Anchors**</span>
<span id="cb17-1315"><a href="#cb17-1315" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>AnchorGenerator: 5 sizes √ó 3 ratios = 15 anchors per location</span>
<span id="cb17-1316"><a href="#cb17-1316" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Anchor sizes: 16, 32, 64, 128, 256 pixels (tuned for 512√ó512 DIOR images after resize)</span>
<span id="cb17-1317"><a href="#cb17-1317" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>32√ó32 feature map √ó 15 anchors = 15,360 anchor boxes per image</span>
<span id="cb17-1318"><a href="#cb17-1318" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>RPN configured for single feature map (not FPN pyramid)</span>
<span id="cb17-1319"><a href="#cb17-1319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1320"><a href="#cb17-1320" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Custom Single-Scale ROI Pooler**</span>
<span id="cb17-1321"><a href="#cb17-1321" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Replace MultiScaleRoIAlign (expects FPN dict) with single RoIAlign</span>
<span id="cb17-1322"><a href="#cb17-1322" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Spatial scale: 1/16 (ViT stride: 512 input ‚Üí 32√ó32 features)</span>
<span id="cb17-1323"><a href="#cb17-1323" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Output: 7√ó7 pooled features per proposal (standard for detection head)</span>
<span id="cb17-1324"><a href="#cb17-1324" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Bypasses multi-scale feature map filtering that caused empty results</span>
<span id="cb17-1325"><a href="#cb17-1325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1326"><a href="#cb17-1326" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Custom Forward Pass for ViT Features**</span>
<span id="cb17-1327"><a href="#cb17-1327" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Reshape sequence <span class="co">[</span><span class="ot">B, 1024, 768</span><span class="co">]</span> ‚Üí spatial <span class="co">[</span><span class="ot">B, 768, 32, 32</span><span class="co">]</span></span>
<span id="cb17-1328"><a href="#cb17-1328" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Handle <span class="co">[</span><span class="ot">CLS</span><span class="co">]</span> token removal if present (1025 ‚Üí 1024 patches)</span>
<span id="cb17-1329"><a href="#cb17-1329" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Ensure tensor contiguity for MPS (Apple Silicon GPU) compatibility</span>
<span id="cb17-1330"><a href="#cb17-1330" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Wrap in OrderedDict for torchvision Faster R-CNN interface</span>
<span id="cb17-1331"><a href="#cb17-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1332"><a href="#cb17-1332" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Fixed Image Transform**</span>
<span id="cb17-1333"><a href="#cb17-1333" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Set min_size=512, max_size=512 to prevent resizing</span>
<span id="cb17-1334"><a href="#cb17-1334" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Collate function handles 800√ó800 ‚Üí 512√ó512 resize with bbox scaling</span>
<span id="cb17-1335"><a href="#cb17-1335" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Model transform preserves 512√ó512 (doesn't resize again)</span>
<span id="cb17-1336"><a href="#cb17-1336" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Result: Consistent 32√ó32 ViT features throughout pipeline</span>
<span id="cb17-1337"><a href="#cb17-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1338"><a href="#cb17-1338" aria-hidden="true" tabindex="-1"></a>**Why DIOR Made It Possible:**</span>
<span id="cb17-1339"><a href="#cb17-1339" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fixed 800√ó800 images (no variable size complexity)</span>
<span id="cb17-1340"><a href="#cb17-1340" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Square images (no aspect ratio issues)  </span>
<span id="cb17-1341"><a href="#cb17-1341" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Consistent patch count (always 1,024)</span>
<span id="cb17-1342"><a href="#cb17-1342" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large dataset (18,000 train, 3,463 test)</span>
<span id="cb17-1343"><a href="#cb17-1343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1344"><a href="#cb17-1344" aria-hidden="true" tabindex="-1"></a>**Key Insight**: </span>
<span id="cb17-1345"><a href="#cb17-1345" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Prithvi (HLS pretrained) is a true **Geospatial Foundation Model**</span>
<span id="cb17-1346"><a href="#cb17-1346" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With proper dataset (fixed-size) and architectural adaptation, ViT GFMs work for detection</span>
<span id="cb17-1347"><a href="#cb17-1347" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The techniques you learned apply to integrating any new backbone architecture!</span>
<span id="cb17-1348"><a href="#cb17-1348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1349"><a href="#cb17-1349" aria-hidden="true" tabindex="-1"></a>**Object Detection vs Classification:**</span>
<span id="cb17-1350"><a href="#cb17-1350" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Detection models compute losses internally</span>
<span id="cb17-1351"><a href="#cb17-1351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Must pass both images AND targets during training</span>
<span id="cb17-1352"><a href="#cb17-1352" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Returns loss dict (train) or predictions (eval)</span>
<span id="cb17-1353"><a href="#cb17-1353" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variable-sized images require custom collate functions</span>
<span id="cb17-1354"><a href="#cb17-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1355"><a href="#cb17-1355" aria-hidden="true" tabindex="-1"></a><span class="fu">### üöÄ Next Steps</span></span>
<span id="cb17-1356"><a href="#cb17-1356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1357"><a href="#cb17-1357" aria-hidden="true" tabindex="-1"></a>**To Use Other Geospatial Foundation Models:**</span>
<span id="cb17-1358"><a href="#cb17-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1359"><a href="#cb17-1359" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Try other ViT-based GFMs** - SatMAE, Clay (apply same adaptation techniques)</span>
<span id="cb17-1360"><a href="#cb17-1360" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Use larger Prithvi** - Prithvi v2 300M or 600M for better performance</span>
<span id="cb17-1361"><a href="#cb17-1361" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Use DETR/Deformable DETR** - Native transformer detection (no CNN conversion needed)</span>
<span id="cb17-1362"><a href="#cb17-1362" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Explore MMDetection** - More flexible ViT integration than torchvision</span>
<span id="cb17-1363"><a href="#cb17-1363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1364"><a href="#cb17-1364" aria-hidden="true" tabindex="-1"></a>**To Improve This Model:**</span>
<span id="cb17-1365"><a href="#cb17-1365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1366"><a href="#cb17-1366" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Train Longer**: 20-50 epochs with learning rate scheduling</span>
<span id="cb17-1367"><a href="#cb17-1367" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Current 2-10 epochs gives confidence ~0.004-0.01</span>
<span id="cb17-1368"><a href="#cb17-1368" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Target 30+ epochs for confidence &gt;0.5</span>
<span id="cb17-1369"><a href="#cb17-1369" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Use ReduceLROnPlateau or CosineAnnealingLR</span>
<span id="cb17-1370"><a href="#cb17-1370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1371"><a href="#cb17-1371" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Use Full Dataset**: Currently using 1,000 samples subset</span>
<span id="cb17-1372"><a href="#cb17-1372" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Full DIOR: 19,000 training images</span>
<span id="cb17-1373"><a href="#cb17-1373" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>More data = better convergence</span>
<span id="cb17-1374"><a href="#cb17-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1375"><a href="#cb17-1375" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Add Data Augmentation**: Carefully with bbox transforms</span>
<span id="cb17-1376"><a href="#cb17-1376" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Horizontal flips (easy)</span>
<span id="cb17-1377"><a href="#cb17-1377" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Random crops (requires bbox adjustment)</span>
<span id="cb17-1378"><a href="#cb17-1378" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Color jittering (safe - doesn't affect boxes)</span>
<span id="cb17-1379"><a href="#cb17-1379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1380"><a href="#cb17-1380" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Tune Detection Thresholds**:</span>
<span id="cb17-1381"><a href="#cb17-1381" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>NMS threshold (currently 0.5)</span>
<span id="cb17-1382"><a href="#cb17-1382" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Score threshold (currently 0.001 for early training)</span>
<span id="cb17-1383"><a href="#cb17-1383" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Increase score_thresh as confidence improves</span>
<span id="cb17-1384"><a href="#cb17-1384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1385"><a href="#cb17-1385" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Evaluate with Proper Metrics**:</span>
<span id="cb17-1386"><a href="#cb17-1386" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>mAP@0.5 and mAP@<span class="co">[</span><span class="ot">0.5:0.95</span><span class="co">]</span> (COCO metrics)</span>
<span id="cb17-1387"><a href="#cb17-1387" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Per-class AP to find difficult classes</span>
<span id="cb17-1388"><a href="#cb17-1388" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Confusion matrix for misclassifications</span>
<span id="cb17-1389"><a href="#cb17-1389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1390"><a href="#cb17-1390" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Experiment with Anchor Sizes**:</span>
<span id="cb17-1391"><a href="#cb17-1391" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Current: 16, 32, 64, 128, 256</span>
<span id="cb17-1392"><a href="#cb17-1392" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Analyze DIOR object size distribution</span>
<span id="cb17-1393"><a href="#cb17-1393" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Adjust if objects are consistently smaller/larger</span>
<span id="cb17-1394"><a href="#cb17-1394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1395"><a href="#cb17-1395" aria-hidden="true" tabindex="-1"></a>**Applying What You Learned to GFMs:**</span>
<span id="cb17-1396"><a href="#cb17-1396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1397"><a href="#cb17-1397" aria-hidden="true" tabindex="-1"></a>When GFM+detection integration improves, you'll use the exact same:</span>
<span id="cb17-1398"><a href="#cb17-1398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dataset loading and exploration patterns</span>
<span id="cb17-1399"><a href="#cb17-1399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Custom collate functions</span>
<span id="cb17-1400"><a href="#cb17-1400" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training loop structure  </span>
<span id="cb17-1401"><a href="#cb17-1401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluation approaches</span>
<span id="cb17-1402"><a href="#cb17-1402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1403"><a href="#cb17-1403" aria-hidden="true" tabindex="-1"></a>The skills transfer completely - only the backbone changes!</span>
<span id="cb17-1404"><a href="#cb17-1404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1405"><a href="#cb17-1405" aria-hidden="true" tabindex="-1"></a><span class="fu">### üìö Further Reading</span></span>
<span id="cb17-1406"><a href="#cb17-1406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1407"><a href="#cb17-1407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">RetinaNet Paper</span><span class="co">](https://arxiv.org/abs/1708.02002)</span> - Focal loss for dense object detection</span>
<span id="cb17-1408"><a href="#cb17-1408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Feature Pyramid Networks</span><span class="co">](https://arxiv.org/abs/1612.03144)</span> - Multi-scale features</span>
<span id="cb17-1409"><a href="#cb17-1409" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">MMDetection</span><span class="co">](https://mmdetection.readthedocs.io/)</span> - Production detection framework</span>
<span id="cb17-1410"><a href="#cb17-1410" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">DETR</span><span class="co">](https://arxiv.org/abs/2005.12872)</span> - Detection transformers for end-to-end detection</span>
<span id="cb17-1411"><a href="#cb17-1411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-1412"><a href="#cb17-1412" aria-hidden="true" tabindex="-1"></a>You now have a working foundation for object detection that you can build upon! üéØ</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/geog-logo.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Department of Geography logo</figcaption>
</figure>
</div>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is built with <a href="https://github.com/kcaylor/GEOG-288KC-geospatial-foundation-models"><i class="fa-brands fa-github" title="the github octocat logo" aria-label="github"></i></a> and <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>