<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Model Inference &amp; Feature Extraction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/all.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-1.2.0/latex-fontsize.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">GEOG 288KC</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">🏠 home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Syllabus.html"> 
<span class="menu-text">📋 syllabus</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-weekly-sessions" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">💻 weekly sessions</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-weekly-sessions">    
        <li>
    <a class="dropdown-item" href="../../chapters/c01-geospatial-data-foundations.html">
 <span class="dropdown-text">Week 1 - 🚀 Core Tools and Data Access</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c02-spatial-temporal-attention-mechanisms.html">
 <span class="dropdown-text">Week 2 - ⚡ Rapid Remote Sensing Preprocessing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c03a-terratorch-foundations.html">
 <span class="dropdown-text">Week 3a - 🌍 TerraTorch Foundations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c03-complete-gfm-architecture.html">
 <span class="dropdown-text">Week 3b - 🤖 Machine Learning on Remote Sensing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c04-pretraining-implementation.html">
 <span class="dropdown-text">Week 4 - 🏗️ Foundation Models in Practice</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c05-training-loop-optimization.html">
 <span class="dropdown-text">Week 5 - 🔧 Fine-Tuning &amp; Transfer Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c06-model-evaluation-analysis.html">
 <span class="dropdown-text">Week 6 - ⏰ Spatiotemporal Modeling &amp; Projects</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-cheatsheets" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">👀 cheatsheets</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-cheatsheets">    
        <li>
    <a class="dropdown-item" href="../../cheatsheets.html">
 <span class="dropdown-text">📋 All Cheatsheets</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li class="dropdown-header">⚡ Quick Starts</li>
        <li>
    <a class="dropdown-item" href="../../extras/cheatsheets/week01_imports.html">
 <span class="dropdown-text">Week 01: Import Guide</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-explainers" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">🧩 explainers</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-explainers">    
        <li class="dropdown-header">1️⃣ Week 1</li>
        <li>
    <a class="dropdown-item" href="../../extras/ai-ml-dl-fm-hierarchy.html">
 <span class="dropdown-text">🤖 AI/ML/DL/FM Hierarchy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-foundation-model-predictions-standalone.html">
 <span class="dropdown-text">🎯 GFM Predictions (Standalone)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/geospatial-prediction-hierarchy.html">
 <span class="dropdown-text">✅ Geospatial Task/Prediction Types</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/neural_networks_explainer.html">
 <span class="dropdown-text">🧠 Neural Networks: Neurons to Transformers</span></a>
  </li>  
        <li class="dropdown-header">2️⃣ Week 2</li>
        <li>
    <a class="dropdown-item" href="../../chapters/c00a-foundation_model_architectures.html">
 <span class="dropdown-text">🏗️ Foundation Model Architectures</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../chapters/c00b-introduction-to-deeplearning-architecture.html">
 <span class="dropdown-text">🎓 Introduction to Deep Learning Architecture</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-extras" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">📖 extras</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-extras">    
        <li class="dropdown-header">🎯 Practical Examples</li>
        <li>
    <a class="dropdown-item" href="../../extras/examples/normalization_comparison.html">
 <span class="dropdown-text">Normalization Comparison</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/resnet.html">
 <span class="dropdown-text">ResNet Implementation</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/segmentation_finetuning.html">
 <span class="dropdown-text">Segmentation Fine-Tuning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/text_encoder.html">
 <span class="dropdown-text">Text Encoder</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/tiling-and-patches.html">
 <span class="dropdown-text">Tiling and Patches</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/examples/terratorch_workflows.html">
 <span class="dropdown-text">TerraTorch Workflows</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/resources/course_resources.html">
 <span class="dropdown-text">📚 Reference Materials</span></a>
  </li>  
        <li class="dropdown-header">📁 Project Templates</li>
        <li>
    <a class="dropdown-item" href="../../extras/projects/project-proposal-template.html">
 <span class="dropdown-text">Project Proposal Template</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../extras/projects/mvp-template.html">
 <span class="dropdown-text">Project Results Template</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/gfms-from-scratch/gfms-from-scratch.github.io" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Model Inference &amp; Feature Extraction</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
            <p class="subtitle lead">Running inference and extracting features</p>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-model-inference" id="toc-introduction-to-model-inference" class="nav-link active" data-scroll-target="#introduction-to-model-inference">Introduction to Model Inference</a></li>
  <li><a href="#basic-inference-patterns" id="toc-basic-inference-patterns" class="nav-link" data-scroll-target="#basic-inference-patterns">Basic Inference Patterns</a>
  <ul class="collapse">
  <li><a href="#single-image-inference" id="toc-single-image-inference" class="nav-link" data-scroll-target="#single-image-inference">Single image inference</a></li>
  <li><a href="#batch-inference" id="toc-batch-inference" class="nav-link" data-scroll-target="#batch-inference">Batch inference</a></li>
  </ul></li>
  <li><a href="#feature-extraction-techniques" id="toc-feature-extraction-techniques" class="nav-link" data-scroll-target="#feature-extraction-techniques">Feature Extraction Techniques</a>
  <ul class="collapse">
  <li><a href="#layer-wise-feature-extraction" id="toc-layer-wise-feature-extraction" class="nav-link" data-scroll-target="#layer-wise-feature-extraction">Layer-wise feature extraction</a></li>
  <li><a href="#multi-scale-feature-extraction" id="toc-multi-scale-feature-extraction" class="nav-link" data-scroll-target="#multi-scale-feature-extraction">Multi-scale feature extraction</a></li>
  </ul></li>
  <li><a href="#advanced-inference-techniques" id="toc-advanced-inference-techniques" class="nav-link" data-scroll-target="#advanced-inference-techniques">Advanced Inference Techniques</a>
  <ul class="collapse">
  <li><a href="#attention-map-visualization" id="toc-attention-map-visualization" class="nav-link" data-scroll-target="#attention-map-visualization">Attention map visualization</a></li>
  <li><a href="#gradient-based-explanations" id="toc-gradient-based-explanations" class="nav-link" data-scroll-target="#gradient-based-explanations">Gradient-based explanations</a></li>
  </ul></li>
  <li><a href="#feature-analysis-and-dimensionality-reduction" id="toc-feature-analysis-and-dimensionality-reduction" class="nav-link" data-scroll-target="#feature-analysis-and-dimensionality-reduction">Feature Analysis and Dimensionality Reduction</a>
  <ul class="collapse">
  <li><a href="#pca-analysis-of-features" id="toc-pca-analysis-of-features" class="nav-link" data-scroll-target="#pca-analysis-of-features">PCA analysis of features</a></li>
  <li><a href="#t-sne-visualization" id="toc-t-sne-visualization" class="nav-link" data-scroll-target="#t-sne-visualization">t-SNE visualization</a></li>
  </ul></li>
  <li><a href="#inference-optimization" id="toc-inference-optimization" class="nav-link" data-scroll-target="#inference-optimization">Inference Optimization</a>
  <ul class="collapse">
  <li><a href="#model-quantization-for-faster-inference" id="toc-model-quantization-for-faster-inference" class="nav-link" data-scroll-target="#model-quantization-for-faster-inference">Model quantization for faster inference</a></li>
  <li><a href="#batch-size-optimization" id="toc-batch-size-optimization" class="nav-link" data-scroll-target="#batch-size-optimization">Batch size optimization</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<section id="introduction-to-model-inference" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-model-inference">Introduction to Model Inference</h2>
<p>Model inference and feature extraction are crucial for applying trained models to satellite imagery analysis and extracting meaningful representations.</p>
<div id="f79a0de0" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="basic-inference-patterns" class="level2">
<h2 class="anchored" data-anchor-id="basic-inference-patterns">Basic Inference Patterns</h2>
<section id="single-image-inference" class="level3">
<h3 class="anchored" data-anchor-id="single-image-inference">Single image inference</h3>
<div id="c3b49148" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GeospatialClassifier(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example geospatial classifier for demonstration"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels<span class="op">=</span><span class="dv">6</span>, num_classes<span class="op">=</span><span class="dv">10</span>, embed_dim<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction layers</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(num_channels, <span class="dv">64</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global pooling and classification</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.global_pool <span class="op">=</span> nn.AdaptiveAvgPool2d(<span class="dv">1</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature embedding layer</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_embed <span class="op">=</span> nn.Linear(<span class="dv">256</span>, embed_dim)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, return_features<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> F.relu(<span class="va">self</span>.conv3(x))</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global pooling</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        pooled <span class="op">=</span> <span class="va">self</span>.global_pool(features).flatten(<span class="dv">1</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classification</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.classifier(pooled)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_features:</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>            embeddings <span class="op">=</span> <span class="va">self</span>.feature_embed(pooled)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>                <span class="st">'logits'</span>: logits,</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>                <span class="st">'features'</span>: embeddings,</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>                <span class="st">'spatial_features'</span>: features,</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>                <span class="st">'pooled_features'</span>: pooled</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GeospatialClassifier(num_channels<span class="op">=</span><span class="dv">6</span>, num_classes<span class="op">=</span><span class="dv">10</span>, embed_dim<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Single image inference</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>sample_image <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)  <span class="co"># Batch of 1, 6 channels</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Basic inference</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(sample_image)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inference with features</span></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(sample_image, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>sample_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predictions shape: </span><span class="sc">{</span>predictions<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Logits shape: </span><span class="sc">{</span>outputs[<span class="st">'logits'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Features shape: </span><span class="sc">{</span>outputs[<span class="st">'features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Spatial features shape: </span><span class="sc">{</span>outputs[<span class="st">'spatial_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="batch-inference" class="level3">
<h3 class="anchored" data-anchor-id="batch-inference">Batch inference</h3>
<div id="da38a864" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_inference(model, images, batch_size<span class="op">=</span><span class="dv">32</span>, device<span class="op">=</span><span class="st">'cpu'</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Perform batch inference on multiple images"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    all_predictions <span class="op">=</span> []</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    all_features <span class="op">=</span> []</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process in batches</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    n_images <span class="op">=</span> <span class="bu">len</span>(images)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    n_batches <span class="op">=</span> (n_images <span class="op">+</span> batch_size <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_batches):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            start_idx <span class="op">=</span> i <span class="op">*</span> batch_size</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            end_idx <span class="op">=</span> <span class="bu">min</span>(start_idx <span class="op">+</span> batch_size, n_images)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            batch <span class="op">=</span> images[start_idx:end_idx].to(device)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get predictions and features</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(batch, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>            all_predictions.append(outputs[<span class="st">'logits'</span>].cpu())</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            all_features.append(outputs[<span class="st">'features'</span>].cpu())</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Processed batch </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_batches<span class="sc">}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">'</span><span class="ch">\r</span><span class="st">'</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenate results</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    final_predictions <span class="op">=</span> torch.cat(all_predictions, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    final_features <span class="op">=</span> torch.cat(all_features, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Completed inference on </span><span class="sc">{</span>n_images<span class="sc">}</span><span class="ss"> images"</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_predictions, final_features</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample batch</span></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>batch_images <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Run batch inference</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>predictions, features <span class="op">=</span> batch_inference(model, batch_images, batch_size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch predictions shape: </span><span class="sc">{</span>predictions<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch features shape: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="feature-extraction-techniques" class="level2">
<h2 class="anchored" data-anchor-id="feature-extraction-techniques">Feature Extraction Techniques</h2>
<section id="layer-wise-feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="layer-wise-feature-extraction">Layer-wise feature extraction</h3>
<div id="2f4c19b0" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeatureExtractor:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract features from specific layers of a model"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, layer_names<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> {}</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> layer_names <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract from all named modules</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            layer_names <span class="op">=</span> [name <span class="cf">for</span> name, _ <span class="kw">in</span> model.named_modules() <span class="cf">if</span> name]</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_hooks(layer_names)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_hooks(<span class="va">self</span>, layer_names):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register forward hooks for feature extraction"""</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> make_hook(name):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> hook(module, <span class="bu">input</span>, output):</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Store detached copy to avoid gradient tracking</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">isinstance</span>(output, torch.Tensor):</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> output.detach().cpu()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, (<span class="bu">list</span>, <span class="bu">tuple</span>)):</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> [o.detach().cpu() <span class="cf">if</span> <span class="bu">isinstance</span>(o, torch.Tensor) <span class="cf">else</span> o <span class="cf">for</span> o <span class="kw">in</span> output]</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, <span class="bu">dict</span>):</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> {k: v.detach().cpu() <span class="cf">if</span> <span class="bu">isinstance</span>(v, torch.Tensor) <span class="cf">else</span> v </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                                         <span class="cf">for</span> k, v <span class="kw">in</span> output.items()}</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> hook</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register hooks</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.model.named_modules():</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> name <span class="kw">in</span> layer_names:</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>                handle <span class="op">=</span> module.register_forward_hook(make_hook(name))</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(handle)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Registered hook for layer: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract(<span class="va">self</span>, images):</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract features from registered layers"""</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features.clear()</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass triggers hooks</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.features.copy()</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_hooks(<span class="va">self</span>):</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Remove all registered hooks"""</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>            hook.remove()</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks.clear()</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Create feature extractor</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> FeatureExtractor(</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    layer_names<span class="op">=</span>[<span class="st">'conv1'</span>, <span class="st">'conv2'</span>, <span class="st">'conv3'</span>, <span class="st">'global_pool'</span>]</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract features</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>extracted_features <span class="op">=</span> extractor.extract(sample_input)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Extracted features:"</span>)</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer_name, features <span class="kw">in</span> extracted_features.items():</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(features, torch.Tensor):</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">type</span>(features)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean up</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>extractor.remove_hooks()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="multi-scale-feature-extraction" class="level3">
<h3 class="anchored" data-anchor-id="multi-scale-feature-extraction">Multi-scale feature extraction</h3>
<div id="f3c76e41" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiScaleFeatureExtractor(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract features at multiple scales"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, backbone):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> backbone</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature pyramid levels</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scales <span class="op">=</span> [<span class="fl">1.0</span>, <span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>]</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        batch_size, channels, height, width <span class="op">=</span> x.shape</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        multiscale_features <span class="op">=</span> {}</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> scale <span class="kw">in</span> <span class="va">self</span>.scales:</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resize input</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> scale <span class="op">!=</span> <span class="fl">1.0</span>:</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>                new_size <span class="op">=</span> (<span class="bu">int</span>(height <span class="op">*</span> scale), <span class="bu">int</span>(width <span class="op">*</span> scale))</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>                scaled_input <span class="op">=</span> F.interpolate(x, size<span class="op">=</span>new_size, mode<span class="op">=</span><span class="st">'bilinear'</span>, align_corners<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>                scaled_input <span class="op">=</span> x</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract features</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> <span class="va">self</span>.backbone(scaled_input, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store features with scale info</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>            scale_key <span class="op">=</span> <span class="ss">f"scale_</span><span class="sc">{</span>scale<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>            multiscale_features[scale_key] <span class="op">=</span> {</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>                <span class="st">'features'</span>: outputs[<span class="st">'features'</span>],</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>                <span class="st">'spatial_features'</span>: outputs[<span class="st">'spatial_features'</span>],</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>                <span class="st">'input_size'</span>: scaled_input.shape[<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> multiscale_features</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Create multi-scale extractor</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>multiscale_extractor <span class="op">=</span> MultiScaleFeatureExtractor(model)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>multiscale_extractor.<span class="bu">eval</span>()</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract multi-scale features</span></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>multiscale_features <span class="op">=</span> multiscale_extractor(sample_input)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Multi-scale features:"</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> scale, features <span class="kw">in</span> multiscale_features.items():</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>scale<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Features: </span><span class="sc">{</span>features[<span class="st">'features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Spatial: </span><span class="sc">{</span>features[<span class="st">'spatial_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Input size: </span><span class="sc">{</span>features[<span class="st">'input_size'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="advanced-inference-techniques" class="level2">
<h2 class="anchored" data-anchor-id="advanced-inference-techniques">Advanced Inference Techniques</h2>
<section id="attention-map-visualization" class="level3">
<h3 class="anchored" data-anchor-id="attention-map-visualization">Attention map visualization</h3>
<div id="8e8f4700" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionExtractor:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract and visualize attention maps"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_maps <span class="op">=</span> {}</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_attention_hooks(<span class="va">self</span>):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register hooks for attention layers"""</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> attention_hook(name):</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> hook(module, <span class="bu">input</span>, output):</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>                <span class="co"># For attention mechanisms, we typically want the attention weights</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>                <span class="co"># This is a simplified example - actual implementation depends on model architecture</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(module, <span class="st">'attention_weights'</span>):</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> module.attention_weights.detach().cpu()</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, <span class="bu">tuple</span>) <span class="kw">and</span> <span class="bu">len</span>(output) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Assume second output contains attention weights</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> output[<span class="dv">1</span>].detach().cpu()</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">hasattr</span>(output, <span class="st">'attentions'</span>):</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> output.attentions.detach().cpu()</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> hook</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Look for attention-related modules</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.model.named_modules():</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'attention'</span> <span class="kw">in</span> name.lower() <span class="kw">or</span> <span class="st">'attn'</span> <span class="kw">in</span> name.lower():</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>                handle <span class="op">=</span> module.register_forward_hook(attention_hook(name))</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(handle)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Registered attention hook: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract_attention(<span class="va">self</span>, images):</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract attention maps"""</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_maps.clear()</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.attention_maps.copy()</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> visualize_attention(<span class="va">self</span>, image, attention_map, alpha<span class="op">=</span><span class="fl">0.6</span>):</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Visualize attention map overlaid on image"""</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image to RGB if needed</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image.shape[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">3</span>:</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use first 3 channels as RGB</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>            rgb_image <span class="op">=</span> image[:<span class="dv">3</span>]</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>            rgb_image <span class="op">=</span> image</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize image for display</span></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>        rgb_image <span class="op">=</span> (rgb_image <span class="op">-</span> rgb_image.<span class="bu">min</span>()) <span class="op">/</span> (rgb_image.<span class="bu">max</span>() <span class="op">-</span> rgb_image.<span class="bu">min</span>())</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>        rgb_image <span class="op">=</span> rgb_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy()</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process attention map</span></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> attention_map.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>            attention_map <span class="op">=</span> attention_map.mean(dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># Average over heads/channels</span></span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize attention map to match image size</span></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>        attention_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>            attention_map.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>rgb_image.shape[:<span class="dv">2</span>],</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>        ).squeeze().numpy()</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create visualization</span></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Original image</span></span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].imshow(rgb_image)</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Original Image'</span>)</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention map</span></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].imshow(attention_resized, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Attention Map'</span>)</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Overlay</span></span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(rgb_image)</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(attention_resized, alpha<span class="op">=</span>alpha, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'Attention Overlay'</span>)</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_hooks(<span class="va">self</span>):</span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Remove attention hooks"""</span></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a>            hook.remove()</span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks.clear()</span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a><span class="co"># Create attention extractor (mock example)</span></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>attention_extractor <span class="op">=</span> AttentionExtractor(model)</span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a><span class="co"># attention_extractor.register_attention_hooks()  # Would need actual attention layers</span></span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention extractor ready (requires model with attention layers)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-based-explanations" class="level3">
<h3 class="anchored" data-anchor-id="gradient-based-explanations">Gradient-based explanations</h3>
<div id="260933a7" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GradCAM:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Gradient-weighted Class Activation Mapping"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, target_layer):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_layer <span class="op">=</span> target_layer</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gradients <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activations <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register hooks</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_hooks()</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_hooks(<span class="va">self</span>):</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register hooks for gradients and activations"""</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> backward_hook(module, grad_input, grad_output):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.gradients <span class="op">=</span> grad_output[<span class="dv">0</span>].detach()</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> forward_hook(module, <span class="bu">input</span>, output):</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.activations <span class="op">=</span> output.detach()</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find target layer</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        target_module <span class="op">=</span> <span class="bu">dict</span>(<span class="va">self</span>.model.named_modules())[<span class="va">self</span>.target_layer]</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        target_module.register_forward_hook(forward_hook)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        target_module.register_backward_hook(backward_hook)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_cam(<span class="va">self</span>, images, class_idx<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Generate Class Activation Map"""</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Enable gradients</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        images.requires_grad_(<span class="va">True</span>)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If class_idx not specified, use predicted class</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> class_idx <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>            class_idx <span class="op">=</span> outputs.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass for target class</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.zero_grad()</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        class_loss <span class="op">=</span> outputs[<span class="dv">0</span>, class_idx[<span class="dv">0</span>]] <span class="cf">if</span> <span class="bu">isinstance</span>(class_idx, torch.Tensor) <span class="cf">else</span> outputs[<span class="dv">0</span>, class_idx]</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        class_loss.backward()</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute CAM</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> <span class="va">self</span>.gradients[<span class="dv">0</span>]  <span class="co"># First image in batch</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>        activations <span class="op">=</span> <span class="va">self</span>.activations[<span class="dv">0</span>]  <span class="co"># First image in batch</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global average pooling of gradients</span></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> torch.mean(gradients, dim<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Weighted combination of activation maps</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> torch.zeros(activations.shape[<span class="dv">1</span>], activations.shape[<span class="dv">2</span>])</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, w <span class="kw">in</span> <span class="bu">enumerate</span>(weights):</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>            cam <span class="op">+=</span> w <span class="op">*</span> activations[i]</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU and normalize</span></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> F.relu(cam)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> cam <span class="op">/</span> torch.<span class="bu">max</span>(cam) <span class="cf">if</span> torch.<span class="bu">max</span>(cam) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> cam</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> cam</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> visualize_cam(<span class="va">self</span>, image, cam, alpha<span class="op">=</span><span class="fl">0.4</span>):</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Visualize CAM overlaid on original image"""</span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image for display</span></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image.shape[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">3</span>:</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>            display_image <span class="op">=</span> image[:<span class="dv">3</span>]  <span class="co"># Use first 3 channels</span></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>            display_image <span class="op">=</span> image</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>        display_image <span class="op">=</span> (display_image <span class="op">-</span> display_image.<span class="bu">min</span>()) <span class="op">/</span> (display_image.<span class="bu">max</span>() <span class="op">-</span> display_image.<span class="bu">min</span>())</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>        display_image <span class="op">=</span> display_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).detach().numpy()</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize CAM to match image size</span></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>        cam_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>            cam.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>display_image.shape[:<span class="dv">2</span>],</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>        ).squeeze().numpy()</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create visualization</span></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].imshow(display_image)</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Original Image'</span>)</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].imshow(cam_resized, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Grad-CAM'</span>)</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(display_image)</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(cam_resized, alpha<span class="op">=</span>alpha, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'Grad-CAM Overlay'</span>)</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Create GradCAM for conv3 layer</span></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>gradcam <span class="op">=</span> GradCAM(model, target_layer<span class="op">=</span><span class="st">'conv3'</span>)</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate CAM</span></span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>cam <span class="op">=</span> gradcam.generate_cam(sample_input)</span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generated CAM shape: </span><span class="sc">{</span>cam<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CAM range: [</span><span class="sc">{</span>cam<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>cam<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.3f}</span><span class="ss">]"</span>)</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize</span></span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>gradcam.visualize_cam(sample_input[<span class="dv">0</span>], cam)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="feature-analysis-and-dimensionality-reduction" class="level2">
<h2 class="anchored" data-anchor-id="feature-analysis-and-dimensionality-reduction">Feature Analysis and Dimensionality Reduction</h2>
<section id="pca-analysis-of-features" class="level3">
<h3 class="anchored" data-anchor-id="pca-analysis-of-features">PCA analysis of features</h3>
<div id="b97f3b5a" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_features_pca(features, n_components<span class="op">=</span><span class="dv">50</span>, visualize<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Analyze features using PCA"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten features if needed</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> features.shape</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features.view(features.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> features.shape</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to numpy</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    features_np <span class="op">=</span> features_flat.numpy()</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply PCA</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span>n_components)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    features_pca <span class="op">=</span> pca.fit_transform(features_np)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Analyze explained variance</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    explained_var_ratio <span class="op">=</span> pca.explained_variance_ratio_</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    cumulative_var <span class="op">=</span> np.cumsum(explained_var_ratio)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> visualize:</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">5</span>))</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Explained variance</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].bar(<span class="bu">range</span>(<span class="bu">len</span>(explained_var_ratio)), explained_var_ratio)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Explained Variance by Component'</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Principal Component'</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Explained Variance Ratio'</span>)</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cumulative explained variance</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].plot(cumulative_var, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Cumulative Explained Variance'</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Number of Components'</span>)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Cumulative Variance Ratio'</span>)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First two components</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].scatter(features_pca[:, <span class="dv">0</span>], features_pca[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'First Two Principal Components'</span>)</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_xlabel(<span class="st">'PC1'</span>)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_ylabel(<span class="st">'PC2'</span>)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>        <span class="st">'features_pca'</span>: features_pca,</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="st">'explained_variance_ratio'</span>: explained_var_ratio,</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cumulative_variance'</span>: cumulative_var,</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        <span class="st">'pca_model'</span>: pca</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample features for analysis</span></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>sample_features <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">256</span>)  <span class="co"># 100 samples, 256 features</span></span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>pca_results <span class="op">=</span> analyze_features_pca(sample_features, n_components<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PCA features shape: </span><span class="sc">{</span>pca_results[<span class="st">'features_pca'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 5 components explain </span><span class="sc">{</span>pca_results[<span class="st">'cumulative_variance'</span>][<span class="dv">4</span>]<span class="sc">:.1%}</span><span class="ss"> of variance"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="t-sne-visualization" class="level3">
<h3 class="anchored" data-anchor-id="t-sne-visualization">t-SNE visualization</h3>
<div id="11fe1943" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_features_tsne(features, labels<span class="op">=</span><span class="va">None</span>, perplexity<span class="op">=</span><span class="dv">30</span>, random_state<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Visualize features using t-SNE"""</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten features if needed</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features.view(features.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    features_np <span class="op">=</span> features_flat.numpy()</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply t-SNE</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Computing t-SNE embedding..."</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, perplexity<span class="op">=</span>perplexity, random_state<span class="op">=</span>random_state)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    features_tsne <span class="op">=</span> tsne.fit_transform(features_np)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create visualization</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Color by labels</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        unique_labels <span class="op">=</span> np.unique(labels)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> plt.cm.tab10(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="bu">len</span>(unique_labels)))</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(unique_labels):</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> labels <span class="op">==</span> label</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>            plt.scatter(features_tsne[mask, <span class="dv">0</span>], features_tsne[mask, <span class="dv">1</span>], </span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>                       c<span class="op">=</span>[colors[i]], label<span class="op">=</span><span class="ss">f'Class </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        plt.legend()</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        plt.scatter(features_tsne[:, <span class="dv">0</span>], features_tsne[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'t-SNE Visualization of Features'</span>)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'t-SNE 1'</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'t-SNE 2'</span>)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> features_tsne</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample data with labels</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>sample_features <span class="op">=</span> torch.randn(<span class="dv">200</span>, <span class="dv">256</span>)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>sample_labels <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">200</span>)  <span class="co"># 5 classes</span></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with t-SNE</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>tsne_features <span class="op">=</span> visualize_features_tsne(sample_features, sample_labels)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"t-SNE features shape: </span><span class="sc">{</span>tsne_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="inference-optimization" class="level2">
<h2 class="anchored" data-anchor-id="inference-optimization">Inference Optimization</h2>
<section id="model-quantization-for-faster-inference" class="level3">
<h3 class="anchored" data-anchor-id="model-quantization-for-faster-inference">Model quantization for faster inference</h3>
<div id="5e074a5c" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantize_model(model, calibration_data<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Quantize model for faster inference"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dynamic quantization (post-training)</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    quantized_model <span class="op">=</span> torch.quantization.quantize_dynamic(</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        {nn.Linear, nn.Conv2d},  <span class="co"># Layers to quantize</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        dtype<span class="op">=</span>torch.qint8</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Applied dynamic quantization"</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_model</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_inference_speed(original_model, quantized_model, test_input, num_runs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare inference speed between models"""</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> time</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warm up</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> original_model(test_input)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> quantized_model(test_input)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Time original model</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> original_model(test_input)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    original_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Time quantized model</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> quantized_model(test_input)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>    quantized_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    speedup <span class="op">=</span> original_time <span class="op">/</span> quantized_time</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Original model: </span><span class="sc">{</span>original_time<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Quantized model: </span><span class="sc">{</span>quantized_time<span class="sc">:.3f}</span><span class="ss">s"</span>) </span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Speedup: </span><span class="sc">{</span>speedup<span class="sc">:.2f}</span><span class="ss">x"</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> speedup</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Create quantized version</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()  <span class="co"># Important: set to eval mode</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>quantized_model <span class="op">=</span> quantize_model(model)</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare speeds</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>test_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>speedup <span class="op">=</span> compare_inference_speed(model, quantized_model, test_input, num_runs<span class="op">=</span><span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="batch-size-optimization" class="level3">
<h3 class="anchored" data-anchor-id="batch-size-optimization">Batch size optimization</h3>
<div id="688bb4aa" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_optimal_batch_size(model, input_shape, device<span class="op">=</span><span class="st">'cpu'</span>, max_batch_size<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Find optimal batch size for memory and speed"""</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    batch_sizes <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> max_batch_size <span class="op">&gt;</span> <span class="dv">64</span>:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        batch_sizes.extend([<span class="dv">128</span>, <span class="dv">256</span>])</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    batch_sizes <span class="op">=</span> [bs <span class="cf">for</span> bs <span class="kw">in</span> batch_sizes <span class="cf">if</span> bs <span class="op">&lt;=</span> max_batch_size]</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> {}</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_size <span class="kw">in</span> batch_sizes:</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create test batch</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>            test_batch <span class="op">=</span> torch.randn(batch_size, <span class="op">*</span>input_shape[<span class="dv">1</span>:]).to(device)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Measure memory and time</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device <span class="op">!=</span> <span class="st">'cpu'</span> <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>                torch.cuda.reset_peak_memory_stats()</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>                start_memory <span class="op">=</span> torch.cuda.memory_allocated()</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>            <span class="im">import</span> time</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>            start_time <span class="op">=</span> time.time()</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):  <span class="co"># Average over multiple runs</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>                    outputs <span class="op">=</span> model(test_batch)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>            elapsed_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>            throughput <span class="op">=</span> (batch_size <span class="op">*</span> <span class="dv">10</span>) <span class="op">/</span> elapsed_time  <span class="co"># samples per second</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device <span class="op">!=</span> <span class="st">'cpu'</span> <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>                peak_memory <span class="op">=</span> torch.cuda.max_memory_allocated()</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>                memory_per_sample <span class="op">=</span> (peak_memory <span class="op">-</span> start_memory) <span class="op">/</span> batch_size</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>                memory_per_sample <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a>            results[batch_size] <span class="op">=</span> {</span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a>                <span class="st">'throughput'</span>: throughput,</span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a>                <span class="st">'time_per_sample'</span>: elapsed_time <span class="op">/</span> (batch_size <span class="op">*</span> <span class="dv">10</span>),</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>                <span class="st">'memory_per_sample'</span>: memory_per_sample <span class="op">/</span> (<span class="dv">1024</span><span class="op">**</span><span class="dv">2</span>)  <span class="co"># MB</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Batch size </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>throughput<span class="sc">:.1f}</span><span class="ss"> samples/sec, "</span></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f"</span><span class="sc">{</span>memory_per_sample <span class="op">/</span> (<span class="dv">1024</span><span class="op">**</span><span class="dv">2</span>)<span class="sc">:.1f}</span><span class="ss"> MB/sample"</span>)</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">"out of memory"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Batch size </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">: Out of memory"</span>)</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> e</span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find optimal batch size (highest throughput)</span></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> results:</span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>        optimal_batch_size <span class="op">=</span> <span class="bu">max</span>(results.keys(), key<span class="op">=</span><span class="kw">lambda</span> k: results[k][<span class="st">'throughput'</span>])</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Optimal batch size: </span><span class="sc">{</span>optimal_batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> optimal_batch_size, results</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span>, results</span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Find optimal batch size</span></span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>optimal_bs, batch_results <span class="op">=</span> find_optimal_batch_size(</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>    input_shape<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span><span class="st">'cpu'</span>,</span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a>    max_batch_size<span class="op">=</span><span class="dv">64</span></span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Key inference and feature extraction techniques: - <strong>Basic inference</strong>: Single image and batch processing - <strong>Feature extraction</strong>: Layer-wise and multi-scale features<br>
- <strong>Attention visualization</strong>: Understanding model focus - <strong>Gradient explanations</strong>: Grad-CAM for interpretability - <strong>Dimensionality reduction</strong>: PCA and t-SNE analysis - <strong>Optimization</strong>: Quantization and batch size tuning - <strong>Performance monitoring</strong>: Speed and memory profiling</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/kcaylor\.github\.io\/GEOG-288KC-geospatial-foundation-models");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Model Inference &amp; Feature Extraction"</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Running inference and extracting features"</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> geoai</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction to Model Inference</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>Model inference and feature extraction are crucial for applying trained models to satellite imagery analysis and extracting meaningful representations.</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PyTorch version: </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CUDA available: </span><span class="sc">{</span>torch<span class="sc">.</span>cuda<span class="sc">.</span>is_available()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## Basic Inference Patterns</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a><span class="fu">### Single image inference</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GeospatialClassifier(nn.Module):</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Example geospatial classifier for demonstration"""</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_channels<span class="op">=</span><span class="dv">6</span>, num_classes<span class="op">=</span><span class="dv">10</span>, embed_dim<span class="op">=</span><span class="dv">256</span>):</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction layers</span></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(num_channels, <span class="dv">64</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global pooling and classification</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.global_pool <span class="op">=</span> nn.AdaptiveAvgPool2d(<span class="dv">1</span>)</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(<span class="dv">256</span>, num_classes)</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature embedding layer</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.feature_embed <span class="op">=</span> nn.Linear(<span class="dv">256</span>, embed_dim)</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, return_features<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature extraction</span></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> F.relu(<span class="va">self</span>.conv3(x))</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global pooling</span></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>        pooled <span class="op">=</span> <span class="va">self</span>.global_pool(features).flatten(<span class="dv">1</span>)</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classification</span></span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.classifier(pooled)</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> return_features:</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>            embeddings <span class="op">=</span> <span class="va">self</span>.feature_embed(pooled)</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> {</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>                <span class="st">'logits'</span>: logits,</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>                <span class="st">'features'</span>: embeddings,</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>                <span class="st">'spatial_features'</span>: features,</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>                <span class="st">'pooled_features'</span>: pooled</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Create model</span></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> GeospatialClassifier(num_channels<span class="op">=</span><span class="dv">6</span>, num_classes<span class="op">=</span><span class="dv">10</span>, embed_dim<span class="op">=</span><span class="dv">256</span>)</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Single image inference</span></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>sample_image <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)  <span class="co"># Batch of 1, 6 channels</span></span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Basic inference</span></span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> model(sample_image)</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inference with features</span></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(sample_image, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Input shape: </span><span class="sc">{</span>sample_image<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predictions shape: </span><span class="sc">{</span>predictions<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Logits shape: </span><span class="sc">{</span>outputs[<span class="st">'logits'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Features shape: </span><span class="sc">{</span>outputs[<span class="st">'features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Spatial features shape: </span><span class="sc">{</span>outputs[<span class="st">'spatial_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a><span class="fu">### Batch inference</span></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_inference(model, images, batch_size<span class="op">=</span><span class="dv">32</span>, device<span class="op">=</span><span class="st">'cpu'</span>):</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Perform batch inference on multiple images"""</span></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a>    all_predictions <span class="op">=</span> []</span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a>    all_features <span class="op">=</span> []</span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process in batches</span></span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>    n_images <span class="op">=</span> <span class="bu">len</span>(images)</span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a>    n_batches <span class="op">=</span> (n_images <span class="op">+</span> batch_size <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> batch_size</span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_batches):</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a>            start_idx <span class="op">=</span> i <span class="op">*</span> batch_size</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a>            end_idx <span class="op">=</span> <span class="bu">min</span>(start_idx <span class="op">+</span> batch_size, n_images)</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>            batch <span class="op">=</span> images[start_idx:end_idx].to(device)</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get predictions and features</span></span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(batch, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a>            all_predictions.append(outputs[<span class="st">'logits'</span>].cpu())</span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a>            all_features.append(outputs[<span class="st">'features'</span>].cpu())</span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Processed batch </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>n_batches<span class="sc">}</span><span class="ss">"</span>, end<span class="op">=</span><span class="st">'</span><span class="ch">\r</span><span class="st">'</span>)</span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Concatenate results</span></span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a>    final_predictions <span class="op">=</span> torch.cat(all_predictions, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a>    final_features <span class="op">=</span> torch.cat(all_features, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Completed inference on </span><span class="sc">{</span>n_images<span class="sc">}</span><span class="ss"> images"</span>)</span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> final_predictions, final_features</span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sample batch</span></span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a>batch_images <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a><span class="co"># Run batch inference</span></span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a>predictions, features <span class="op">=</span> batch_inference(model, batch_images, batch_size<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch predictions shape: </span><span class="sc">{</span>predictions<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Batch features shape: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Extraction Techniques</span></span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a><span class="fu">### Layer-wise feature extraction</span></span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeatureExtractor:</span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract features from specific layers of a model"""</span></span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, layer_names<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features <span class="op">=</span> {}</span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> layer_names <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract from all named modules</span></span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a>            layer_names <span class="op">=</span> [name <span class="cf">for</span> name, _ <span class="kw">in</span> model.named_modules() <span class="cf">if</span> name]</span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_hooks(layer_names)</span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_hooks(<span class="va">self</span>, layer_names):</span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register forward hooks for feature extraction"""</span></span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> make_hook(name):</span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> hook(module, <span class="bu">input</span>, output):</span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Store detached copy to avoid gradient tracking</span></span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">isinstance</span>(output, torch.Tensor):</span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> output.detach().cpu()</span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, (<span class="bu">list</span>, <span class="bu">tuple</span>)):</span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> [o.detach().cpu() <span class="cf">if</span> <span class="bu">isinstance</span>(o, torch.Tensor) <span class="cf">else</span> o <span class="cf">for</span> o <span class="kw">in</span> output]</span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, <span class="bu">dict</span>):</span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.features[name] <span class="op">=</span> {k: v.detach().cpu() <span class="cf">if</span> <span class="bu">isinstance</span>(v, torch.Tensor) <span class="cf">else</span> v </span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a>                                         <span class="cf">for</span> k, v <span class="kw">in</span> output.items()}</span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> hook</span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register hooks</span></span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.model.named_modules():</span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> name <span class="kw">in</span> layer_names:</span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a>                handle <span class="op">=</span> module.register_forward_hook(make_hook(name))</span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(handle)</span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Registered hook for layer: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract(<span class="va">self</span>, images):</span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract features from registered layers"""</span></span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.features.clear()</span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-196"><a href="#cb12-196" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-197"><a href="#cb12-197" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass triggers hooks</span></span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.features.copy()</span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_hooks(<span class="va">self</span>):</span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Remove all registered hooks"""</span></span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a>            hook.remove()</span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks.clear()</span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a><span class="co"># Create feature extractor</span></span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a>extractor <span class="op">=</span> FeatureExtractor(</span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a>    layer_names<span class="op">=</span>[<span class="st">'conv1'</span>, <span class="st">'conv2'</span>, <span class="st">'conv3'</span>, <span class="st">'global_pool'</span>]</span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract features</span></span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a>extracted_features <span class="op">=</span> extractor.extract(sample_input)</span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Extracted features:"</span>)</span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer_name, features <span class="kw">in</span> extracted_features.items():</span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(features, torch.Tensor):</span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>layer_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="bu">type</span>(features)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean up</span></span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a>extractor.remove_hooks()</span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-scale feature extraction</span></span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiScaleFeatureExtractor(nn.Module):</span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract features at multiple scales"""</span></span>
<span id="cb12-235"><a href="#cb12-235" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, backbone):</span>
<span id="cb12-237"><a href="#cb12-237" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-238"><a href="#cb12-238" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> backbone</span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Feature pyramid levels</span></span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scales <span class="op">=</span> [<span class="fl">1.0</span>, <span class="fl">0.75</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>]</span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a>        batch_size, channels, height, width <span class="op">=</span> x.shape</span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a>        multiscale_features <span class="op">=</span> {}</span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> scale <span class="kw">in</span> <span class="va">self</span>.scales:</span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resize input</span></span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> scale <span class="op">!=</span> <span class="fl">1.0</span>:</span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a>                new_size <span class="op">=</span> (<span class="bu">int</span>(height <span class="op">*</span> scale), <span class="bu">int</span>(width <span class="op">*</span> scale))</span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a>                scaled_input <span class="op">=</span> F.interpolate(x, size<span class="op">=</span>new_size, mode<span class="op">=</span><span class="st">'bilinear'</span>, align_corners<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb12-254"><a href="#cb12-254" aria-hidden="true" tabindex="-1"></a>                scaled_input <span class="op">=</span> x</span>
<span id="cb12-255"><a href="#cb12-255" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract features</span></span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> <span class="va">self</span>.backbone(scaled_input, return_features<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Store features with scale info</span></span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a>            scale_key <span class="op">=</span> <span class="ss">f"scale_</span><span class="sc">{</span>scale<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a>            multiscale_features[scale_key] <span class="op">=</span> {</span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a>                <span class="st">'features'</span>: outputs[<span class="st">'features'</span>],</span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a>                <span class="st">'spatial_features'</span>: outputs[<span class="st">'spatial_features'</span>],</span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a>                <span class="st">'input_size'</span>: scaled_input.shape[<span class="op">-</span><span class="dv">2</span>:]</span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> multiscale_features</span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a><span class="co"># Create multi-scale extractor</span></span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a>multiscale_extractor <span class="op">=</span> MultiScaleFeatureExtractor(model)</span>
<span id="cb12-272"><a href="#cb12-272" aria-hidden="true" tabindex="-1"></a>multiscale_extractor.<span class="bu">eval</span>()</span>
<span id="cb12-273"><a href="#cb12-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract multi-scale features</span></span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a>multiscale_features <span class="op">=</span> multiscale_extractor(sample_input)</span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Multi-scale features:"</span>)</span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> scale, features <span class="kw">in</span> multiscale_features.items():</span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>scale<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Features: </span><span class="sc">{</span>features[<span class="st">'features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Spatial: </span><span class="sc">{</span>features[<span class="st">'spatial_features'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Input size: </span><span class="sc">{</span>features[<span class="st">'input_size'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a><span class="fu">## Advanced Inference Techniques</span></span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a><span class="fu">### Attention map visualization</span></span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionExtractor:</span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Extract and visualize attention maps"""</span></span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model):</span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_maps <span class="op">=</span> {}</span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks <span class="op">=</span> []</span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_attention_hooks(<span class="va">self</span>):</span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register hooks for attention layers"""</span></span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> attention_hook(name):</span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> hook(module, <span class="bu">input</span>, output):</span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a>                <span class="co"># For attention mechanisms, we typically want the attention weights</span></span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a>                <span class="co"># This is a simplified example - actual implementation depends on model architecture</span></span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(module, <span class="st">'attention_weights'</span>):</span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> module.attention_weights.detach().cpu()</span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">isinstance</span>(output, <span class="bu">tuple</span>) <span class="kw">and</span> <span class="bu">len</span>(output) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Assume second output contains attention weights</span></span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> output[<span class="dv">1</span>].detach().cpu()</span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a>                <span class="cf">elif</span> <span class="bu">hasattr</span>(output, <span class="st">'attentions'</span>):</span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.attention_maps[name] <span class="op">=</span> output.attentions.detach().cpu()</span>
<span id="cb12-314"><a href="#cb12-314" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> hook</span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Look for attention-related modules</span></span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> name, module <span class="kw">in</span> <span class="va">self</span>.model.named_modules():</span>
<span id="cb12-318"><a href="#cb12-318" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'attention'</span> <span class="kw">in</span> name.lower() <span class="kw">or</span> <span class="st">'attn'</span> <span class="kw">in</span> name.lower():</span>
<span id="cb12-319"><a href="#cb12-319" aria-hidden="true" tabindex="-1"></a>                handle <span class="op">=</span> module.register_forward_hook(attention_hook(name))</span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.hooks.append(handle)</span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Registered attention hook: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> extract_attention(<span class="va">self</span>, images):</span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Extract attention maps"""</span></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_maps.clear()</span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.attention_maps.copy()</span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> visualize_attention(<span class="va">self</span>, image, attention_map, alpha<span class="op">=</span><span class="fl">0.6</span>):</span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Visualize attention map overlaid on image"""</span></span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image to RGB if needed</span></span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image.shape[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">3</span>:</span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use first 3 channels as RGB</span></span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a>            rgb_image <span class="op">=</span> image[:<span class="dv">3</span>]</span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a>            rgb_image <span class="op">=</span> image</span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize image for display</span></span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a>        rgb_image <span class="op">=</span> (rgb_image <span class="op">-</span> rgb_image.<span class="bu">min</span>()) <span class="op">/</span> (rgb_image.<span class="bu">max</span>() <span class="op">-</span> rgb_image.<span class="bu">min</span>())</span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a>        rgb_image <span class="op">=</span> rgb_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).numpy()</span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Process attention map</span></span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> attention_map.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a>            attention_map <span class="op">=</span> attention_map.mean(dim<span class="op">=</span><span class="dv">0</span>)  <span class="co"># Average over heads/channels</span></span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize attention map to match image size</span></span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a>        attention_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a>            attention_map.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>rgb_image.shape[:<span class="dv">2</span>],</span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a>        ).squeeze().numpy()</span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create visualization</span></span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Original image</span></span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].imshow(rgb_image)</span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Original Image'</span>)</span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Attention map</span></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].imshow(attention_resized, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Attention Map'</span>)</span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Overlay</span></span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(rgb_image)</span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(attention_resized, alpha<span class="op">=</span>alpha, cmap<span class="op">=</span><span class="st">'hot'</span>)</span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'Attention Overlay'</span>)</span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> remove_hooks(<span class="va">self</span>):</span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Remove attention hooks"""</span></span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> hook <span class="kw">in</span> <span class="va">self</span>.hooks:</span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a>            hook.remove()</span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hooks.clear()</span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a><span class="co"># Create attention extractor (mock example)</span></span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a>attention_extractor <span class="op">=</span> AttentionExtractor(model)</span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a><span class="co"># attention_extractor.register_attention_hooks()  # Would need actual attention layers</span></span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Attention extractor ready (requires model with attention layers)"</span>)</span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient-based explanations</span></span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GradCAM:</span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Gradient-weighted Class Activation Mapping"""</span></span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model, target_layer):</span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_layer <span class="op">=</span> target_layer</span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gradients <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activations <span class="op">=</span> <span class="va">None</span></span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Register hooks</span></span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_hooks()</span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> register_hooks(<span class="va">self</span>):</span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Register hooks for gradients and activations"""</span></span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> backward_hook(module, grad_input, grad_output):</span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.gradients <span class="op">=</span> grad_output[<span class="dv">0</span>].detach()</span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> forward_hook(module, <span class="bu">input</span>, output):</span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.activations <span class="op">=</span> output.detach()</span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find target layer</span></span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a>        target_module <span class="op">=</span> <span class="bu">dict</span>(<span class="va">self</span>.model.named_modules())[<span class="va">self</span>.target_layer]</span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a>        target_module.register_forward_hook(forward_hook)</span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a>        target_module.register_backward_hook(backward_hook)</span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate_cam(<span class="va">self</span>, images, class_idx<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Generate Class Activation Map"""</span></span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Enable gradients</span></span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a>        images.requires_grad_(<span class="va">True</span>)</span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-429"><a href="#cb12-429" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb12-430"><a href="#cb12-430" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.model(images)</span>
<span id="cb12-431"><a href="#cb12-431" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If class_idx not specified, use predicted class</span></span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> class_idx <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a>            class_idx <span class="op">=</span> outputs.argmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass for target class</span></span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model.zero_grad()</span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a>        class_loss <span class="op">=</span> outputs[<span class="dv">0</span>, class_idx[<span class="dv">0</span>]] <span class="cf">if</span> <span class="bu">isinstance</span>(class_idx, torch.Tensor) <span class="cf">else</span> outputs[<span class="dv">0</span>, class_idx]</span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a>        class_loss.backward()</span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute CAM</span></span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> <span class="va">self</span>.gradients[<span class="dv">0</span>]  <span class="co"># First image in batch</span></span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a>        activations <span class="op">=</span> <span class="va">self</span>.activations[<span class="dv">0</span>]  <span class="co"># First image in batch</span></span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global average pooling of gradients</span></span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> torch.mean(gradients, dim<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Weighted combination of activation maps</span></span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> torch.zeros(activations.shape[<span class="dv">1</span>], activations.shape[<span class="dv">2</span>])</span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, w <span class="kw">in</span> <span class="bu">enumerate</span>(weights):</span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a>            cam <span class="op">+=</span> w <span class="op">*</span> activations[i]</span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply ReLU and normalize</span></span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> F.relu(cam)</span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a>        cam <span class="op">=</span> cam <span class="op">/</span> torch.<span class="bu">max</span>(cam) <span class="cf">if</span> torch.<span class="bu">max</span>(cam) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> cam</span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> cam</span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> visualize_cam(<span class="va">self</span>, image, cam, alpha<span class="op">=</span><span class="fl">0.4</span>):</span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Visualize CAM overlaid on original image"""</span></span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert image for display</span></span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> image.shape[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">3</span>:</span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a>            display_image <span class="op">=</span> image[:<span class="dv">3</span>]  <span class="co"># Use first 3 channels</span></span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a>            display_image <span class="op">=</span> image</span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a>        display_image <span class="op">=</span> (display_image <span class="op">-</span> display_image.<span class="bu">min</span>()) <span class="op">/</span> (display_image.<span class="bu">max</span>() <span class="op">-</span> display_image.<span class="bu">min</span>())</span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a>        display_image <span class="op">=</span> display_image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).detach().numpy()</span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize CAM to match image size</span></span>
<span id="cb12-472"><a href="#cb12-472" aria-hidden="true" tabindex="-1"></a>        cam_resized <span class="op">=</span> F.interpolate(</span>
<span id="cb12-473"><a href="#cb12-473" aria-hidden="true" tabindex="-1"></a>            cam.unsqueeze(<span class="dv">0</span>).unsqueeze(<span class="dv">0</span>),</span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>display_image.shape[:<span class="dv">2</span>],</span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">'bilinear'</span>,</span>
<span id="cb12-476"><a href="#cb12-476" aria-hidden="true" tabindex="-1"></a>            align_corners<span class="op">=</span><span class="va">False</span></span>
<span id="cb12-477"><a href="#cb12-477" aria-hidden="true" tabindex="-1"></a>        ).squeeze().numpy()</span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create visualization</span></span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].imshow(display_image)</span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Original Image'</span>)</span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb12-485"><a href="#cb12-485" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-486"><a href="#cb12-486" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].imshow(cam_resized, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Grad-CAM'</span>)</span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(display_image)</span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].imshow(cam_resized, alpha<span class="op">=</span>alpha, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'Grad-CAM Overlay'</span>)</span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].axis(<span class="st">'off'</span>)</span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a><span class="co"># Create GradCAM for conv3 layer</span></span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a>gradcam <span class="op">=</span> GradCAM(model, target_layer<span class="op">=</span><span class="st">'conv3'</span>)</span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate CAM</span></span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a>sample_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a>cam <span class="op">=</span> gradcam.generate_cam(sample_input)</span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Generated CAM shape: </span><span class="sc">{</span>cam<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CAM range: [</span><span class="sc">{</span>cam<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.3f}</span><span class="ss">, </span><span class="sc">{</span>cam<span class="sc">.</span><span class="bu">max</span>()<span class="sc">:.3f}</span><span class="ss">]"</span>)</span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize</span></span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a>gradcam.visualize_cam(sample_input[<span class="dv">0</span>], cam)</span>
<span id="cb12-510"><a href="#cb12-510" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-511"><a href="#cb12-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Analysis and Dimensionality Reduction</span></span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a><span class="fu">### PCA analysis of features</span></span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> analyze_features_pca(features, n_components<span class="op">=</span><span class="dv">50</span>, visualize<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Analyze features using PCA"""</span></span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten features if needed</span></span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> features.shape</span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features.view(features.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features</span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> features.shape</span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert to numpy</span></span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a>    features_np <span class="op">=</span> features_flat.numpy()</span>
<span id="cb12-531"><a href="#cb12-531" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-532"><a href="#cb12-532" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply PCA</span></span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a>    pca <span class="op">=</span> PCA(n_components<span class="op">=</span>n_components)</span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a>    features_pca <span class="op">=</span> pca.fit_transform(features_np)</span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Analyze explained variance</span></span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a>    explained_var_ratio <span class="op">=</span> pca.explained_variance_ratio_</span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a>    cumulative_var <span class="op">=</span> np.cumsum(explained_var_ratio)</span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> visualize:</span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a>        fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">18</span>, <span class="dv">5</span>))</span>
<span id="cb12-542"><a href="#cb12-542" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-543"><a href="#cb12-543" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Explained variance</span></span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].bar(<span class="bu">range</span>(<span class="bu">len</span>(explained_var_ratio)), explained_var_ratio)</span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_title(<span class="st">'Explained Variance by Component'</span>)</span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_xlabel(<span class="st">'Principal Component'</span>)</span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">0</span>].set_ylabel(<span class="st">'Explained Variance Ratio'</span>)</span>
<span id="cb12-548"><a href="#cb12-548" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-549"><a href="#cb12-549" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cumulative explained variance</span></span>
<span id="cb12-550"><a href="#cb12-550" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].plot(cumulative_var, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb12-551"><a href="#cb12-551" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_title(<span class="st">'Cumulative Explained Variance'</span>)</span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_xlabel(<span class="st">'Number of Components'</span>)</span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].set_ylabel(<span class="st">'Cumulative Variance Ratio'</span>)</span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">1</span>].grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First two components</span></span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].scatter(features_pca[:, <span class="dv">0</span>], features_pca[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_title(<span class="st">'First Two Principal Components'</span>)</span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_xlabel(<span class="st">'PC1'</span>)</span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a>        axes[<span class="dv">2</span>].set_ylabel(<span class="st">'PC2'</span>)</span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a>        plt.tight_layout()</span>
<span id="cb12-563"><a href="#cb12-563" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb12-564"><a href="#cb12-564" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-565"><a href="#cb12-565" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {</span>
<span id="cb12-566"><a href="#cb12-566" aria-hidden="true" tabindex="-1"></a>        <span class="st">'features_pca'</span>: features_pca,</span>
<span id="cb12-567"><a href="#cb12-567" aria-hidden="true" tabindex="-1"></a>        <span class="st">'explained_variance_ratio'</span>: explained_var_ratio,</span>
<span id="cb12-568"><a href="#cb12-568" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cumulative_variance'</span>: cumulative_var,</span>
<span id="cb12-569"><a href="#cb12-569" aria-hidden="true" tabindex="-1"></a>        <span class="st">'pca_model'</span>: pca</span>
<span id="cb12-570"><a href="#cb12-570" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb12-571"><a href="#cb12-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-572"><a href="#cb12-572" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample features for analysis</span></span>
<span id="cb12-573"><a href="#cb12-573" aria-hidden="true" tabindex="-1"></a>sample_features <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">256</span>)  <span class="co"># 100 samples, 256 features</span></span>
<span id="cb12-574"><a href="#cb12-574" aria-hidden="true" tabindex="-1"></a>pca_results <span class="op">=</span> analyze_features_pca(sample_features, n_components<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb12-575"><a href="#cb12-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-576"><a href="#cb12-576" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"PCA features shape: </span><span class="sc">{</span>pca_results[<span class="st">'features_pca'</span>]<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-577"><a href="#cb12-577" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 5 components explain </span><span class="sc">{</span>pca_results[<span class="st">'cumulative_variance'</span>][<span class="dv">4</span>]<span class="sc">:.1%}</span><span class="ss"> of variance"</span>)</span>
<span id="cb12-578"><a href="#cb12-578" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-579"><a href="#cb12-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-580"><a href="#cb12-580" aria-hidden="true" tabindex="-1"></a><span class="fu">### t-SNE visualization</span></span>
<span id="cb12-583"><a href="#cb12-583" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-584"><a href="#cb12-584" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize_features_tsne(features, labels<span class="op">=</span><span class="va">None</span>, perplexity<span class="op">=</span><span class="dv">30</span>, random_state<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb12-585"><a href="#cb12-585" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Visualize features using t-SNE"""</span></span>
<span id="cb12-586"><a href="#cb12-586" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-587"><a href="#cb12-587" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten features if needed</span></span>
<span id="cb12-588"><a href="#cb12-588" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> features.dim() <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb12-589"><a href="#cb12-589" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features.view(features.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb12-590"><a href="#cb12-590" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-591"><a href="#cb12-591" aria-hidden="true" tabindex="-1"></a>        features_flat <span class="op">=</span> features</span>
<span id="cb12-592"><a href="#cb12-592" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-593"><a href="#cb12-593" aria-hidden="true" tabindex="-1"></a>    features_np <span class="op">=</span> features_flat.numpy()</span>
<span id="cb12-594"><a href="#cb12-594" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-595"><a href="#cb12-595" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply t-SNE</span></span>
<span id="cb12-596"><a href="#cb12-596" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Computing t-SNE embedding..."</span>)</span>
<span id="cb12-597"><a href="#cb12-597" aria-hidden="true" tabindex="-1"></a>    tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, perplexity<span class="op">=</span>perplexity, random_state<span class="op">=</span>random_state)</span>
<span id="cb12-598"><a href="#cb12-598" aria-hidden="true" tabindex="-1"></a>    features_tsne <span class="op">=</span> tsne.fit_transform(features_np)</span>
<span id="cb12-599"><a href="#cb12-599" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-600"><a href="#cb12-600" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create visualization</span></span>
<span id="cb12-601"><a href="#cb12-601" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb12-602"><a href="#cb12-602" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-603"><a href="#cb12-603" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb12-604"><a href="#cb12-604" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Color by labels</span></span>
<span id="cb12-605"><a href="#cb12-605" aria-hidden="true" tabindex="-1"></a>        unique_labels <span class="op">=</span> np.unique(labels)</span>
<span id="cb12-606"><a href="#cb12-606" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> plt.cm.tab10(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="bu">len</span>(unique_labels)))</span>
<span id="cb12-607"><a href="#cb12-607" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-608"><a href="#cb12-608" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(unique_labels):</span>
<span id="cb12-609"><a href="#cb12-609" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> labels <span class="op">==</span> label</span>
<span id="cb12-610"><a href="#cb12-610" aria-hidden="true" tabindex="-1"></a>            plt.scatter(features_tsne[mask, <span class="dv">0</span>], features_tsne[mask, <span class="dv">1</span>], </span>
<span id="cb12-611"><a href="#cb12-611" aria-hidden="true" tabindex="-1"></a>                       c<span class="op">=</span>[colors[i]], label<span class="op">=</span><span class="ss">f'Class </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb12-612"><a href="#cb12-612" aria-hidden="true" tabindex="-1"></a>        plt.legend()</span>
<span id="cb12-613"><a href="#cb12-613" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb12-614"><a href="#cb12-614" aria-hidden="true" tabindex="-1"></a>        plt.scatter(features_tsne[:, <span class="dv">0</span>], features_tsne[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb12-615"><a href="#cb12-615" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-616"><a href="#cb12-616" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'t-SNE Visualization of Features'</span>)</span>
<span id="cb12-617"><a href="#cb12-617" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'t-SNE 1'</span>)</span>
<span id="cb12-618"><a href="#cb12-618" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'t-SNE 2'</span>)</span>
<span id="cb12-619"><a href="#cb12-619" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb12-620"><a href="#cb12-620" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb12-621"><a href="#cb12-621" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-622"><a href="#cb12-622" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> features_tsne</span>
<span id="cb12-623"><a href="#cb12-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-624"><a href="#cb12-624" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample data with labels</span></span>
<span id="cb12-625"><a href="#cb12-625" aria-hidden="true" tabindex="-1"></a>sample_features <span class="op">=</span> torch.randn(<span class="dv">200</span>, <span class="dv">256</span>)</span>
<span id="cb12-626"><a href="#cb12-626" aria-hidden="true" tabindex="-1"></a>sample_labels <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">200</span>)  <span class="co"># 5 classes</span></span>
<span id="cb12-627"><a href="#cb12-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-628"><a href="#cb12-628" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize with t-SNE</span></span>
<span id="cb12-629"><a href="#cb12-629" aria-hidden="true" tabindex="-1"></a>tsne_features <span class="op">=</span> visualize_features_tsne(sample_features, sample_labels)</span>
<span id="cb12-630"><a href="#cb12-630" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"t-SNE features shape: </span><span class="sc">{</span>tsne_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-631"><a href="#cb12-631" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-632"><a href="#cb12-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-633"><a href="#cb12-633" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference Optimization</span></span>
<span id="cb12-634"><a href="#cb12-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-635"><a href="#cb12-635" aria-hidden="true" tabindex="-1"></a><span class="fu">### Model quantization for faster inference</span></span>
<span id="cb12-638"><a href="#cb12-638" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-639"><a href="#cb12-639" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quantize_model(model, calibration_data<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb12-640"><a href="#cb12-640" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Quantize model for faster inference"""</span></span>
<span id="cb12-641"><a href="#cb12-641" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-642"><a href="#cb12-642" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dynamic quantization (post-training)</span></span>
<span id="cb12-643"><a href="#cb12-643" aria-hidden="true" tabindex="-1"></a>    quantized_model <span class="op">=</span> torch.quantization.quantize_dynamic(</span>
<span id="cb12-644"><a href="#cb12-644" aria-hidden="true" tabindex="-1"></a>        model,</span>
<span id="cb12-645"><a href="#cb12-645" aria-hidden="true" tabindex="-1"></a>        {nn.Linear, nn.Conv2d},  <span class="co"># Layers to quantize</span></span>
<span id="cb12-646"><a href="#cb12-646" aria-hidden="true" tabindex="-1"></a>        dtype<span class="op">=</span>torch.qint8</span>
<span id="cb12-647"><a href="#cb12-647" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-648"><a href="#cb12-648" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-649"><a href="#cb12-649" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Applied dynamic quantization"</span>)</span>
<span id="cb12-650"><a href="#cb12-650" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-651"><a href="#cb12-651" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> quantized_model</span>
<span id="cb12-652"><a href="#cb12-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-653"><a href="#cb12-653" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_inference_speed(original_model, quantized_model, test_input, num_runs<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb12-654"><a href="#cb12-654" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compare inference speed between models"""</span></span>
<span id="cb12-655"><a href="#cb12-655" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-656"><a href="#cb12-656" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> time</span>
<span id="cb12-657"><a href="#cb12-657" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-658"><a href="#cb12-658" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warm up</span></span>
<span id="cb12-659"><a href="#cb12-659" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb12-660"><a href="#cb12-660" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-661"><a href="#cb12-661" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> original_model(test_input)</span>
<span id="cb12-662"><a href="#cb12-662" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> quantized_model(test_input)</span>
<span id="cb12-663"><a href="#cb12-663" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-664"><a href="#cb12-664" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Time original model</span></span>
<span id="cb12-665"><a href="#cb12-665" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb12-666"><a href="#cb12-666" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb12-667"><a href="#cb12-667" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-668"><a href="#cb12-668" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> original_model(test_input)</span>
<span id="cb12-669"><a href="#cb12-669" aria-hidden="true" tabindex="-1"></a>    original_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb12-670"><a href="#cb12-670" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-671"><a href="#cb12-671" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Time quantized model</span></span>
<span id="cb12-672"><a href="#cb12-672" aria-hidden="true" tabindex="-1"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb12-673"><a href="#cb12-673" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb12-674"><a href="#cb12-674" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-675"><a href="#cb12-675" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> quantized_model(test_input)</span>
<span id="cb12-676"><a href="#cb12-676" aria-hidden="true" tabindex="-1"></a>    quantized_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb12-677"><a href="#cb12-677" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-678"><a href="#cb12-678" aria-hidden="true" tabindex="-1"></a>    speedup <span class="op">=</span> original_time <span class="op">/</span> quantized_time</span>
<span id="cb12-679"><a href="#cb12-679" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-680"><a href="#cb12-680" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Original model: </span><span class="sc">{</span>original_time<span class="sc">:.3f}</span><span class="ss">s"</span>)</span>
<span id="cb12-681"><a href="#cb12-681" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Quantized model: </span><span class="sc">{</span>quantized_time<span class="sc">:.3f}</span><span class="ss">s"</span>) </span>
<span id="cb12-682"><a href="#cb12-682" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Speedup: </span><span class="sc">{</span>speedup<span class="sc">:.2f}</span><span class="ss">x"</span>)</span>
<span id="cb12-683"><a href="#cb12-683" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-684"><a href="#cb12-684" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> speedup</span>
<span id="cb12-685"><a href="#cb12-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-686"><a href="#cb12-686" aria-hidden="true" tabindex="-1"></a><span class="co"># Create quantized version</span></span>
<span id="cb12-687"><a href="#cb12-687" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()  <span class="co"># Important: set to eval mode</span></span>
<span id="cb12-688"><a href="#cb12-688" aria-hidden="true" tabindex="-1"></a>quantized_model <span class="op">=</span> quantize_model(model)</span>
<span id="cb12-689"><a href="#cb12-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-690"><a href="#cb12-690" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare speeds</span></span>
<span id="cb12-691"><a href="#cb12-691" aria-hidden="true" tabindex="-1"></a>test_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb12-692"><a href="#cb12-692" aria-hidden="true" tabindex="-1"></a>speedup <span class="op">=</span> compare_inference_speed(model, quantized_model, test_input, num_runs<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb12-693"><a href="#cb12-693" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-694"><a href="#cb12-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-695"><a href="#cb12-695" aria-hidden="true" tabindex="-1"></a><span class="fu">### Batch size optimization</span></span>
<span id="cb12-698"><a href="#cb12-698" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-699"><a href="#cb12-699" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_optimal_batch_size(model, input_shape, device<span class="op">=</span><span class="st">'cpu'</span>, max_batch_size<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb12-700"><a href="#cb12-700" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Find optimal batch size for memory and speed"""</span></span>
<span id="cb12-701"><a href="#cb12-701" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-702"><a href="#cb12-702" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb12-703"><a href="#cb12-703" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb12-704"><a href="#cb12-704" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-705"><a href="#cb12-705" aria-hidden="true" tabindex="-1"></a>    batch_sizes <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>]</span>
<span id="cb12-706"><a href="#cb12-706" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> max_batch_size <span class="op">&gt;</span> <span class="dv">64</span>:</span>
<span id="cb12-707"><a href="#cb12-707" aria-hidden="true" tabindex="-1"></a>        batch_sizes.extend([<span class="dv">128</span>, <span class="dv">256</span>])</span>
<span id="cb12-708"><a href="#cb12-708" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-709"><a href="#cb12-709" aria-hidden="true" tabindex="-1"></a>    batch_sizes <span class="op">=</span> [bs <span class="cf">for</span> bs <span class="kw">in</span> batch_sizes <span class="cf">if</span> bs <span class="op">&lt;=</span> max_batch_size]</span>
<span id="cb12-710"><a href="#cb12-710" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-711"><a href="#cb12-711" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> {}</span>
<span id="cb12-712"><a href="#cb12-712" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-713"><a href="#cb12-713" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_size <span class="kw">in</span> batch_sizes:</span>
<span id="cb12-714"><a href="#cb12-714" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb12-715"><a href="#cb12-715" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create test batch</span></span>
<span id="cb12-716"><a href="#cb12-716" aria-hidden="true" tabindex="-1"></a>            test_batch <span class="op">=</span> torch.randn(batch_size, <span class="op">*</span>input_shape[<span class="dv">1</span>:]).to(device)</span>
<span id="cb12-717"><a href="#cb12-717" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-718"><a href="#cb12-718" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Measure memory and time</span></span>
<span id="cb12-719"><a href="#cb12-719" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device <span class="op">!=</span> <span class="st">'cpu'</span> <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb12-720"><a href="#cb12-720" aria-hidden="true" tabindex="-1"></a>                torch.cuda.reset_peak_memory_stats()</span>
<span id="cb12-721"><a href="#cb12-721" aria-hidden="true" tabindex="-1"></a>                start_memory <span class="op">=</span> torch.cuda.memory_allocated()</span>
<span id="cb12-722"><a href="#cb12-722" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-723"><a href="#cb12-723" aria-hidden="true" tabindex="-1"></a>            <span class="im">import</span> time</span>
<span id="cb12-724"><a href="#cb12-724" aria-hidden="true" tabindex="-1"></a>            start_time <span class="op">=</span> time.time()</span>
<span id="cb12-725"><a href="#cb12-725" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-726"><a href="#cb12-726" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-727"><a href="#cb12-727" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):  <span class="co"># Average over multiple runs</span></span>
<span id="cb12-728"><a href="#cb12-728" aria-hidden="true" tabindex="-1"></a>                    outputs <span class="op">=</span> model(test_batch)</span>
<span id="cb12-729"><a href="#cb12-729" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-730"><a href="#cb12-730" aria-hidden="true" tabindex="-1"></a>            elapsed_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb12-731"><a href="#cb12-731" aria-hidden="true" tabindex="-1"></a>            throughput <span class="op">=</span> (batch_size <span class="op">*</span> <span class="dv">10</span>) <span class="op">/</span> elapsed_time  <span class="co"># samples per second</span></span>
<span id="cb12-732"><a href="#cb12-732" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-733"><a href="#cb12-733" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device <span class="op">!=</span> <span class="st">'cpu'</span> <span class="kw">and</span> torch.cuda.is_available():</span>
<span id="cb12-734"><a href="#cb12-734" aria-hidden="true" tabindex="-1"></a>                peak_memory <span class="op">=</span> torch.cuda.max_memory_allocated()</span>
<span id="cb12-735"><a href="#cb12-735" aria-hidden="true" tabindex="-1"></a>                memory_per_sample <span class="op">=</span> (peak_memory <span class="op">-</span> start_memory) <span class="op">/</span> batch_size</span>
<span id="cb12-736"><a href="#cb12-736" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb12-737"><a href="#cb12-737" aria-hidden="true" tabindex="-1"></a>                memory_per_sample <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb12-738"><a href="#cb12-738" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-739"><a href="#cb12-739" aria-hidden="true" tabindex="-1"></a>            results[batch_size] <span class="op">=</span> {</span>
<span id="cb12-740"><a href="#cb12-740" aria-hidden="true" tabindex="-1"></a>                <span class="st">'throughput'</span>: throughput,</span>
<span id="cb12-741"><a href="#cb12-741" aria-hidden="true" tabindex="-1"></a>                <span class="st">'time_per_sample'</span>: elapsed_time <span class="op">/</span> (batch_size <span class="op">*</span> <span class="dv">10</span>),</span>
<span id="cb12-742"><a href="#cb12-742" aria-hidden="true" tabindex="-1"></a>                <span class="st">'memory_per_sample'</span>: memory_per_sample <span class="op">/</span> (<span class="dv">1024</span><span class="op">**</span><span class="dv">2</span>)  <span class="co"># MB</span></span>
<span id="cb12-743"><a href="#cb12-743" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb12-744"><a href="#cb12-744" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-745"><a href="#cb12-745" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Batch size </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>throughput<span class="sc">:.1f}</span><span class="ss"> samples/sec, "</span></span>
<span id="cb12-746"><a href="#cb12-746" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f"</span><span class="sc">{</span>memory_per_sample <span class="op">/</span> (<span class="dv">1024</span><span class="op">**</span><span class="dv">2</span>)<span class="sc">:.1f}</span><span class="ss"> MB/sample"</span>)</span>
<span id="cb12-747"><a href="#cb12-747" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-748"><a href="#cb12-748" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> e:</span>
<span id="cb12-749"><a href="#cb12-749" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">"out of memory"</span> <span class="kw">in</span> <span class="bu">str</span>(e):</span>
<span id="cb12-750"><a href="#cb12-750" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Batch size </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">: Out of memory"</span>)</span>
<span id="cb12-751"><a href="#cb12-751" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb12-752"><a href="#cb12-752" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb12-753"><a href="#cb12-753" aria-hidden="true" tabindex="-1"></a>                <span class="cf">raise</span> e</span>
<span id="cb12-754"><a href="#cb12-754" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-755"><a href="#cb12-755" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find optimal batch size (highest throughput)</span></span>
<span id="cb12-756"><a href="#cb12-756" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> results:</span>
<span id="cb12-757"><a href="#cb12-757" aria-hidden="true" tabindex="-1"></a>        optimal_batch_size <span class="op">=</span> <span class="bu">max</span>(results.keys(), key<span class="op">=</span><span class="kw">lambda</span> k: results[k][<span class="st">'throughput'</span>])</span>
<span id="cb12-758"><a href="#cb12-758" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Optimal batch size: </span><span class="sc">{</span>optimal_batch_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-759"><a href="#cb12-759" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> optimal_batch_size, results</span>
<span id="cb12-760"><a href="#cb12-760" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-761"><a href="#cb12-761" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span>, results</span>
<span id="cb12-762"><a href="#cb12-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-763"><a href="#cb12-763" aria-hidden="true" tabindex="-1"></a><span class="co"># Find optimal batch size</span></span>
<span id="cb12-764"><a href="#cb12-764" aria-hidden="true" tabindex="-1"></a>optimal_bs, batch_results <span class="op">=</span> find_optimal_batch_size(</span>
<span id="cb12-765"><a href="#cb12-765" aria-hidden="true" tabindex="-1"></a>    model, </span>
<span id="cb12-766"><a href="#cb12-766" aria-hidden="true" tabindex="-1"></a>    input_shape<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">224</span>, <span class="dv">224</span>),</span>
<span id="cb12-767"><a href="#cb12-767" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span><span class="st">'cpu'</span>,</span>
<span id="cb12-768"><a href="#cb12-768" aria-hidden="true" tabindex="-1"></a>    max_batch_size<span class="op">=</span><span class="dv">64</span></span>
<span id="cb12-769"><a href="#cb12-769" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-770"><a href="#cb12-770" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-771"><a href="#cb12-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-772"><a href="#cb12-772" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary</span></span>
<span id="cb12-773"><a href="#cb12-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-774"><a href="#cb12-774" aria-hidden="true" tabindex="-1"></a>Key inference and feature extraction techniques:</span>
<span id="cb12-775"><a href="#cb12-775" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Basic inference**: Single image and batch processing</span>
<span id="cb12-776"><a href="#cb12-776" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Feature extraction**: Layer-wise and multi-scale features  </span>
<span id="cb12-777"><a href="#cb12-777" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Attention visualization**: Understanding model focus</span>
<span id="cb12-778"><a href="#cb12-778" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gradient explanations**: Grad-CAM for interpretability</span>
<span id="cb12-779"><a href="#cb12-779" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dimensionality reduction**: PCA and t-SNE analysis</span>
<span id="cb12-780"><a href="#cb12-780" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Optimization**: Quantization and batch size tuning</span>
<span id="cb12-781"><a href="#cb12-781" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Performance monitoring**: Speed and memory profiling</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../images/geog-logo.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Department of Geography logo</figcaption>
</figure>
</div>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This website is built with <a href="https://github.com/kcaylor/GEOG-288KC-geospatial-foundation-models"><i class="fa-brands fa-github" title="the github octocat logo" aria-label="github"></i></a> and <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>